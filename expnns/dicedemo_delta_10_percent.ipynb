{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Usage of the robustness check on DiCE\n",
    "\n",
    "NOTE IN THIS DATASET, CLASS \"BAD\" IS TOGGLED. IN THE PROCESSED DATASET, \"BAD\"==1 MEANS GOOD."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os, sys, pickle, json, math, time, multiprocessing, warnings, itertools, random, warnings, gc, ast, subprocess\n",
    "import copy\n",
    "from collections import defaultdict, Counter, namedtuple\n",
    "from math import log\n",
    "from itertools import product, combinations\n",
    "from random import choice, choices, sample, seed\n",
    "from datetime import datetime\n",
    "\n",
    "import gurobipy\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from importlib import reload\n",
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Multi-processing\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# sci-kit learn\n",
    "import sklearn\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, make_scorer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, precision_recall_curve\n",
    "from utilexp import *\n",
    "\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 150\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "from expnns.preprocessor import Preprocessor, min_max_scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "       LOAN   MORTDUE     VALUE   YOJ  DEROG  DELINQ       CLAGE  NINQ  CLNO  \\\n5      1700   30548.0   40320.0   9.0    0.0     0.0  101.466002   1.0   8.0   \n7      1800   28502.0   43034.0  11.0    0.0     0.0   88.766030   0.0   8.0   \n19     2300  102370.0  120953.0   2.0    0.0     0.0   90.992533   0.0  13.0   \n25     2400   34863.0   47471.0  12.0    0.0     0.0   70.491080   1.0  21.0   \n26     2400   98449.0  117195.0   4.0    0.0     0.0   93.811775   0.0  13.0   \n...     ...       ...       ...   ...    ...     ...         ...   ...   ...   \n5955  88900   57264.0   90185.0  16.0    0.0     0.0  221.808718   0.0  16.0   \n5956  89000   54576.0   92937.0  16.0    0.0     0.0  208.692070   0.0  15.0   \n5957  89200   54045.0   92924.0  15.0    0.0     0.0  212.279697   0.0  15.0   \n5958  89800   50370.0   91861.0  14.0    0.0     0.0  213.892709   0.0  16.0   \n5959  89900   48811.0   88934.0  15.0    0.0     0.0  219.601002   0.0  16.0   \n\n        DEBTINC  BAD  \n5     37.113614    1  \n7     36.884894    1  \n19    31.588503    0  \n25    38.263601    1  \n26    29.681827    0  \n...         ...  ...  \n5955  36.112347    0  \n5956  35.859971    0  \n5957  35.556590    0  \n5958  34.340882    0  \n5959  34.571519    0  \n\n[3515 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LOAN</th>\n      <th>MORTDUE</th>\n      <th>VALUE</th>\n      <th>YOJ</th>\n      <th>DEROG</th>\n      <th>DELINQ</th>\n      <th>CLAGE</th>\n      <th>NINQ</th>\n      <th>CLNO</th>\n      <th>DEBTINC</th>\n      <th>BAD</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>1700</td>\n      <td>30548.0</td>\n      <td>40320.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>101.466002</td>\n      <td>1.0</td>\n      <td>8.0</td>\n      <td>37.113614</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1800</td>\n      <td>28502.0</td>\n      <td>43034.0</td>\n      <td>11.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>88.766030</td>\n      <td>0.0</td>\n      <td>8.0</td>\n      <td>36.884894</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2300</td>\n      <td>102370.0</td>\n      <td>120953.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>90.992533</td>\n      <td>0.0</td>\n      <td>13.0</td>\n      <td>31.588503</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>2400</td>\n      <td>34863.0</td>\n      <td>47471.0</td>\n      <td>12.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>70.491080</td>\n      <td>1.0</td>\n      <td>21.0</td>\n      <td>38.263601</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>2400</td>\n      <td>98449.0</td>\n      <td>117195.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>93.811775</td>\n      <td>0.0</td>\n      <td>13.0</td>\n      <td>29.681827</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5955</th>\n      <td>88900</td>\n      <td>57264.0</td>\n      <td>90185.0</td>\n      <td>16.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>221.808718</td>\n      <td>0.0</td>\n      <td>16.0</td>\n      <td>36.112347</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5956</th>\n      <td>89000</td>\n      <td>54576.0</td>\n      <td>92937.0</td>\n      <td>16.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>208.692070</td>\n      <td>0.0</td>\n      <td>15.0</td>\n      <td>35.859971</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5957</th>\n      <td>89200</td>\n      <td>54045.0</td>\n      <td>92924.0</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>212.279697</td>\n      <td>0.0</td>\n      <td>15.0</td>\n      <td>35.556590</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5958</th>\n      <td>89800</td>\n      <td>50370.0</td>\n      <td>91861.0</td>\n      <td>14.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>213.892709</td>\n      <td>0.0</td>\n      <td>16.0</td>\n      <td>34.340882</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5959</th>\n      <td>89900</td>\n      <td>48811.0</td>\n      <td>88934.0</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>219.601002</td>\n      <td>0.0</td>\n      <td>16.0</td>\n      <td>34.571519</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3515 rows Ã— 11 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv(\"../datasets/heloc/hmeq.csv\", delimiter=',')\n",
    "continuous_features = [\"LOAN\", \"MORTDUE\", \"VALUE\", \"YOJ\", \"DEROG\", \"DELINQ\", \"CLAGE\", \"NINQ\", \"CLNO\",\n",
    "                       \"DEBTINC\"]\n",
    "df = df.drop(columns=[\"REASON\", \"JOB\"])\n",
    "df = df.dropna()\n",
    "\n",
    "dfx, dfy = df.drop(columns=['BAD']), pd.DataFrame(df['BAD'])\n",
    "df = pd.concat([dfx, dfy], axis=1)\n",
    "display(df)\n",
    "# min max scale\n",
    "min_vals = np.min(df[continuous_features], axis=0)\n",
    "max_vals = np.max(df[continuous_features], axis=0)\n",
    "df_mm = min_max_scale(df, continuous_features, min_vals, max_vals)\n",
    "\n",
    "# get X, y\n",
    "X, y = df_mm.drop(columns=['BAD']), pd.DataFrame(1 - df_mm['BAD'])\n",
    "df1 = df_mm[:1757]\n",
    "SPLIT = .2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=SPLIT, shuffle=True,\n",
    "                                                    random_state=0)\n",
    "\n",
    "ordinal_features = {}\n",
    "discrete_features = {}\n",
    "columns = list(df_mm.columns)\n",
    "feat_var_map = {}\n",
    "for i in range(len(X.columns)):\n",
    "    feat_var_map[i] = [i]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "min_vals = np.array(min_vals)\n",
    "max_vals = np.array(max_vals)\n",
    "\n",
    "\n",
    "def inverse_min_max_scale(xm):\n",
    "    return (max_vals - min_vals) * xm + min_vals"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "X1, X2 = X[:1757], X[1757:]\n",
    "y1, y2 = y[:1757], y[1757:]\n",
    "SPLIT = .2\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, stratify=y1, test_size=SPLIT, shuffle=True,\n",
    "                                                        random_state=0)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, stratify=y2, test_size=SPLIT, shuffle=True,\n",
    "                                                        random_state=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "from utilexp import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class DemoDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        try:\n",
    "            self.X = X.values\n",
    "            self.y = y.values\n",
    "        except:\n",
    "            self.X = X\n",
    "            self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx]).float(), torch.from_numpy(self.y[idx]), float()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "trainds = DemoDataset(X1_train, y1_train)\n",
    "testds = DemoDataset(X1_test, y1_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "params = {'batch_size': 32,\n",
    "          'shuffle': True}\n",
    "traindl = DataLoader(trainds, **params)\n",
    "testdl = DataLoader(testds, **params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 0.7561066150665283\n",
      "Epoch 0: train loss: 0.7125176191329956\n",
      "Epoch 0: train loss: 0.6799675226211548\n",
      "Epoch 0: train loss: 0.6465974450111389\n",
      "Epoch 0: train loss: 0.6300753951072693\n",
      "Epoch 0: train loss: 0.5903058052062988\n",
      "Epoch 0: train loss: 0.5498446822166443\n",
      "Epoch 0: train loss: 0.5088981986045837\n",
      "Epoch 0: train loss: 0.5646379590034485\n",
      "Epoch 0: train loss: 0.4966583251953125\n",
      "Epoch 0: train loss: 0.4678831696510315\n",
      "Epoch 0: train loss: 0.48069459199905396\n",
      "Epoch 0: train loss: 0.48219943046569824\n",
      "Epoch 0: train loss: 0.5006915926933289\n",
      "Epoch 0: train loss: 0.49810290336608887\n",
      "Epoch 0: train loss: 0.39517197012901306\n",
      "Epoch 0: train loss: 0.3428173363208771\n",
      "Epoch 0: train loss: 0.3995702266693115\n",
      "Epoch 0: train loss: 0.38834312558174133\n",
      "Epoch 0: train loss: 0.4172157645225525\n",
      "Epoch 0: train loss: 0.3714316189289093\n",
      "Epoch 0: train loss: 0.34066081047058105\n",
      "Epoch 0: train loss: 0.26017943024635315\n",
      "Epoch 0: train loss: 0.37134549021720886\n",
      "Epoch 0: train loss: 0.448483407497406\n",
      "Epoch 0: train loss: 0.17939993739128113\n",
      "Epoch 0: train loss: 0.1996452808380127\n",
      "Epoch 0: train loss: 0.10415883362293243\n",
      "Epoch 0: train loss: 0.3038252294063568\n",
      "Epoch 0: train loss: 0.3104920983314514\n",
      "Epoch 0: train loss: 0.15500810742378235\n",
      "Epoch 0: train loss: 0.232462540268898\n",
      "Epoch 0: train loss: 0.33081045746803284\n",
      "Epoch 0: train loss: 0.1719001680612564\n",
      "Epoch 0: train loss: 0.21265827119350433\n",
      "Epoch 0: train loss: 0.4234054684638977\n",
      "Epoch 0: train loss: 0.33343976736068726\n",
      "Epoch 0: train loss: 0.30550089478492737\n",
      "Epoch 0: train loss: 0.40126165747642517\n",
      "Epoch 0: train loss: 0.3856850266456604\n",
      "Epoch 0: train loss: 0.37281912565231323\n",
      "Epoch 0: train loss: 0.05730840936303139\n",
      "Epoch 0: train loss: 0.33499452471733093\n",
      "Epoch 0: train loss: 0.16493378579616547\n",
      "Epoch 1: train loss: 0.15117435157299042\n",
      "Epoch 1: train loss: 0.3235839605331421\n",
      "Epoch 1: train loss: 0.47419846057891846\n",
      "Epoch 1: train loss: 0.5828185677528381\n",
      "Epoch 1: train loss: 0.3156416714191437\n",
      "Epoch 1: train loss: 0.37365472316741943\n",
      "Epoch 1: train loss: 0.22967930138111115\n",
      "Epoch 1: train loss: 0.46161821484565735\n",
      "Epoch 1: train loss: 0.2261500209569931\n",
      "Epoch 1: train loss: 0.15803731977939606\n",
      "Epoch 1: train loss: 0.2658293545246124\n",
      "Epoch 1: train loss: 0.24024122953414917\n",
      "Epoch 1: train loss: 0.15688610076904297\n",
      "Epoch 1: train loss: 0.4391501843929291\n",
      "Epoch 1: train loss: 0.31769058108329773\n",
      "Epoch 1: train loss: 0.23152947425842285\n",
      "Epoch 1: train loss: 0.2857096195220947\n",
      "Epoch 1: train loss: 0.21227729320526123\n",
      "Epoch 1: train loss: 0.36685019731521606\n",
      "Epoch 1: train loss: 0.42889952659606934\n",
      "Epoch 1: train loss: 0.5534692406654358\n",
      "Epoch 1: train loss: 0.40811124444007874\n",
      "Epoch 1: train loss: 0.16958971321582794\n",
      "Epoch 1: train loss: 0.38547348976135254\n",
      "Epoch 1: train loss: 0.3075281083583832\n",
      "Epoch 1: train loss: 0.3689913749694824\n",
      "Epoch 1: train loss: 0.17522786557674408\n",
      "Epoch 1: train loss: 0.22189383208751678\n",
      "Epoch 1: train loss: 0.23273862898349762\n",
      "Epoch 1: train loss: 0.3755817413330078\n",
      "Epoch 1: train loss: 0.17914091050624847\n",
      "Epoch 1: train loss: 0.4114265441894531\n",
      "Epoch 1: train loss: 0.22856324911117554\n",
      "Epoch 1: train loss: 0.4240814447402954\n",
      "Epoch 1: train loss: 0.3239096999168396\n",
      "Epoch 1: train loss: 0.23372618854045868\n",
      "Epoch 1: train loss: 0.21428976953029633\n",
      "Epoch 1: train loss: 0.38222643733024597\n",
      "Epoch 1: train loss: 0.0943443551659584\n",
      "Epoch 1: train loss: 0.41130542755126953\n",
      "Epoch 1: train loss: 0.23327504098415375\n",
      "Epoch 1: train loss: 0.35598623752593994\n",
      "Epoch 1: train loss: 0.38121268153190613\n",
      "Epoch 1: train loss: 0.22492238879203796\n",
      "Epoch 2: train loss: 0.3006616532802582\n",
      "Epoch 2: train loss: 0.233199343085289\n",
      "Epoch 2: train loss: 0.4473186731338501\n",
      "Epoch 2: train loss: 0.24051812291145325\n",
      "Epoch 2: train loss: 0.2813049554824829\n",
      "Epoch 2: train loss: 0.15351304411888123\n",
      "Epoch 2: train loss: 0.5047293305397034\n",
      "Epoch 2: train loss: 0.2765783965587616\n",
      "Epoch 2: train loss: 0.0918620303273201\n",
      "Epoch 2: train loss: 0.4004482626914978\n",
      "Epoch 2: train loss: 0.3036707043647766\n",
      "Epoch 2: train loss: 0.4570371210575104\n",
      "Epoch 2: train loss: 0.25599363446235657\n",
      "Epoch 2: train loss: 0.2819197475910187\n",
      "Epoch 2: train loss: 0.23290784657001495\n",
      "Epoch 2: train loss: 0.23319247364997864\n",
      "Epoch 2: train loss: 0.39517754316329956\n",
      "Epoch 2: train loss: 0.5858925580978394\n",
      "Epoch 2: train loss: 0.3265319764614105\n",
      "Epoch 2: train loss: 0.24250665307044983\n",
      "Epoch 2: train loss: 0.3003060221672058\n",
      "Epoch 2: train loss: 0.2272438108921051\n",
      "Epoch 2: train loss: 0.5088722705841064\n",
      "Epoch 2: train loss: 0.3255001902580261\n",
      "Epoch 2: train loss: 0.27956345677375793\n",
      "Epoch 2: train loss: 0.22195889055728912\n",
      "Epoch 2: train loss: 0.10848863422870636\n",
      "Epoch 2: train loss: 0.1690853387117386\n",
      "Epoch 2: train loss: 0.305952787399292\n",
      "Epoch 2: train loss: 0.4526998996734619\n",
      "Epoch 2: train loss: 0.3579529821872711\n",
      "Epoch 2: train loss: 0.31716182827949524\n",
      "Epoch 2: train loss: 0.15031103789806366\n",
      "Epoch 2: train loss: 0.5198524594306946\n",
      "Epoch 2: train loss: 0.2357427328824997\n",
      "Epoch 2: train loss: 0.27095189690589905\n",
      "Epoch 2: train loss: 0.3284253776073456\n",
      "Epoch 2: train loss: 0.21530009806156158\n",
      "Epoch 2: train loss: 0.3476293981075287\n",
      "Epoch 2: train loss: 0.17312230169773102\n",
      "Epoch 2: train loss: 0.15065565705299377\n",
      "Epoch 2: train loss: 0.30475395917892456\n",
      "Epoch 2: train loss: 0.2337937206029892\n",
      "Epoch 2: train loss: 0.30886924266815186\n",
      "Epoch 3: train loss: 0.30150502920150757\n",
      "Epoch 3: train loss: 0.2440224438905716\n",
      "Epoch 3: train loss: 0.2111651599407196\n",
      "Epoch 3: train loss: 0.14784753322601318\n",
      "Epoch 3: train loss: 0.23613399267196655\n",
      "Epoch 3: train loss: 0.44634687900543213\n",
      "Epoch 3: train loss: 0.3135632276535034\n",
      "Epoch 3: train loss: 0.17298655211925507\n",
      "Epoch 3: train loss: 0.38308948278427124\n",
      "Epoch 3: train loss: 0.18090645968914032\n",
      "Epoch 3: train loss: 0.27762943506240845\n",
      "Epoch 3: train loss: 0.3252798020839691\n",
      "Epoch 3: train loss: 0.35580408573150635\n",
      "Epoch 3: train loss: 0.4450046122074127\n",
      "Epoch 3: train loss: 0.5013754367828369\n",
      "Epoch 3: train loss: 0.29401227831840515\n",
      "Epoch 3: train loss: 0.21410273015499115\n",
      "Epoch 3: train loss: 0.3080899715423584\n",
      "Epoch 3: train loss: 0.29055678844451904\n",
      "Epoch 3: train loss: 0.16628970205783844\n",
      "Epoch 3: train loss: 0.3073398470878601\n",
      "Epoch 3: train loss: 0.31725308299064636\n",
      "Epoch 3: train loss: 0.18843229115009308\n",
      "Epoch 3: train loss: 0.48339125514030457\n",
      "Epoch 3: train loss: 0.188336580991745\n",
      "Epoch 3: train loss: 0.4411422312259674\n",
      "Epoch 3: train loss: 0.2421180009841919\n",
      "Epoch 3: train loss: 0.26554882526397705\n",
      "Epoch 3: train loss: 0.4327605366706848\n",
      "Epoch 3: train loss: 0.3253445029258728\n",
      "Epoch 3: train loss: 0.2792184054851532\n",
      "Epoch 3: train loss: 0.17619891464710236\n",
      "Epoch 3: train loss: 0.2513074278831482\n",
      "Epoch 3: train loss: 0.29792284965515137\n",
      "Epoch 3: train loss: 0.23998120427131653\n",
      "Epoch 3: train loss: 0.20760706067085266\n",
      "Epoch 3: train loss: 0.261706680059433\n",
      "Epoch 3: train loss: 0.31540152430534363\n",
      "Epoch 3: train loss: 0.219583198428154\n",
      "Epoch 3: train loss: 0.2838086485862732\n",
      "Epoch 3: train loss: 0.5316582918167114\n",
      "Epoch 3: train loss: 0.1571643352508545\n",
      "Epoch 3: train loss: 0.21463510394096375\n",
      "Epoch 3: train loss: 0.25852862000465393\n",
      "Epoch 4: train loss: 0.29001086950302124\n",
      "Epoch 4: train loss: 0.35200613737106323\n",
      "Epoch 4: train loss: 0.30148178339004517\n",
      "Epoch 4: train loss: 0.1479751169681549\n",
      "Epoch 4: train loss: 0.1557110697031021\n",
      "Epoch 4: train loss: 0.3110688626766205\n",
      "Epoch 4: train loss: 0.14908310770988464\n",
      "Epoch 4: train loss: 0.23078611493110657\n",
      "Epoch 4: train loss: 0.0813106819987297\n",
      "Epoch 4: train loss: 0.2688640356063843\n",
      "Epoch 4: train loss: 0.4365893304347992\n",
      "Epoch 4: train loss: 0.22461549937725067\n",
      "Epoch 4: train loss: 0.47465968132019043\n",
      "Epoch 4: train loss: 0.20149575173854828\n",
      "Epoch 4: train loss: 0.3084174394607544\n",
      "Epoch 4: train loss: 0.24909767508506775\n",
      "Epoch 4: train loss: 0.37202727794647217\n",
      "Epoch 4: train loss: 0.2493010312318802\n",
      "Epoch 4: train loss: 0.1916753500699997\n",
      "Epoch 4: train loss: 0.30825504660606384\n",
      "Epoch 4: train loss: 0.3261219263076782\n",
      "Epoch 4: train loss: 0.20986586809158325\n",
      "Epoch 4: train loss: 0.3993828594684601\n",
      "Epoch 4: train loss: 0.2538907527923584\n",
      "Epoch 4: train loss: 0.35762202739715576\n",
      "Epoch 4: train loss: 0.2540120780467987\n",
      "Epoch 4: train loss: 0.11149989068508148\n",
      "Epoch 4: train loss: 0.35537540912628174\n",
      "Epoch 4: train loss: 0.48342129588127136\n",
      "Epoch 4: train loss: 0.28435656428337097\n",
      "Epoch 4: train loss: 0.3558938205242157\n",
      "Epoch 4: train loss: 0.3064504563808441\n",
      "Epoch 4: train loss: 0.14434686303138733\n",
      "Epoch 4: train loss: 0.10512865334749222\n",
      "Epoch 4: train loss: 0.28627443313598633\n",
      "Epoch 4: train loss: 0.30347123742103577\n",
      "Epoch 4: train loss: 0.5031812787055969\n",
      "Epoch 4: train loss: 0.2341170310974121\n",
      "Epoch 4: train loss: 0.5925866365432739\n",
      "Epoch 4: train loss: 0.23662026226520538\n",
      "Epoch 4: train loss: 0.13514173030853271\n",
      "Epoch 4: train loss: 0.14381350576877594\n",
      "Epoch 4: train loss: 0.4889426529407501\n",
      "Epoch 4: train loss: 0.28756237030029297\n",
      "Epoch 5: train loss: 0.4963916540145874\n",
      "Epoch 5: train loss: 0.2595599591732025\n",
      "Epoch 5: train loss: 0.18270641565322876\n",
      "Epoch 5: train loss: 0.3281555473804474\n",
      "Epoch 5: train loss: 0.2549801170825958\n",
      "Epoch 5: train loss: 0.2917878031730652\n",
      "Epoch 5: train loss: 0.35759079456329346\n",
      "Epoch 5: train loss: 0.29357653856277466\n",
      "Epoch 5: train loss: 0.20391163229942322\n",
      "Epoch 5: train loss: 0.27079105377197266\n",
      "Epoch 5: train loss: 0.2861233055591583\n",
      "Epoch 5: train loss: 0.28836488723754883\n",
      "Epoch 5: train loss: 0.2637569010257721\n",
      "Epoch 5: train loss: 0.4614225924015045\n",
      "Epoch 5: train loss: 0.3724443018436432\n",
      "Epoch 5: train loss: 0.5475738644599915\n",
      "Epoch 5: train loss: 0.11025647073984146\n",
      "Epoch 5: train loss: 0.2307712435722351\n",
      "Epoch 5: train loss: 0.21891522407531738\n",
      "Epoch 5: train loss: 0.1940099447965622\n",
      "Epoch 5: train loss: 0.2369927316904068\n",
      "Epoch 5: train loss: 0.2696647644042969\n",
      "Epoch 5: train loss: 0.3565617799758911\n",
      "Epoch 5: train loss: 0.2834908962249756\n",
      "Epoch 5: train loss: 0.15518218278884888\n",
      "Epoch 5: train loss: 0.3143523335456848\n",
      "Epoch 5: train loss: 0.25167080760002136\n",
      "Epoch 5: train loss: 0.4820764660835266\n",
      "Epoch 5: train loss: 0.25303807854652405\n",
      "Epoch 5: train loss: 0.3253413140773773\n",
      "Epoch 5: train loss: 0.21520094573497772\n",
      "Epoch 5: train loss: 0.30255362391471863\n",
      "Epoch 5: train loss: 0.13673298060894012\n",
      "Epoch 5: train loss: 0.16763122379779816\n",
      "Epoch 5: train loss: 0.08202968537807465\n",
      "Epoch 5: train loss: 0.5025425553321838\n",
      "Epoch 5: train loss: 0.2141127586364746\n",
      "Epoch 5: train loss: 0.17419643700122833\n",
      "Epoch 5: train loss: 0.28446483612060547\n",
      "Epoch 5: train loss: 0.1660676747560501\n",
      "Epoch 5: train loss: 0.43431413173675537\n",
      "Epoch 5: train loss: 0.2880444824695587\n",
      "Epoch 5: train loss: 0.28731852769851685\n",
      "Epoch 5: train loss: 0.15523004531860352\n",
      "Epoch 6: train loss: 0.36791202425956726\n",
      "Epoch 6: train loss: 0.3249806761741638\n",
      "Epoch 6: train loss: 0.29784417152404785\n",
      "Epoch 6: train loss: 0.3421495258808136\n",
      "Epoch 6: train loss: 0.3394685685634613\n",
      "Epoch 6: train loss: 0.366407185792923\n",
      "Epoch 6: train loss: 0.31409451365470886\n",
      "Epoch 6: train loss: 0.4751676023006439\n",
      "Epoch 6: train loss: 0.19385334849357605\n",
      "Epoch 6: train loss: 0.2850867509841919\n",
      "Epoch 6: train loss: 0.2314813733100891\n",
      "Epoch 6: train loss: 0.27120962738990784\n",
      "Epoch 6: train loss: 0.17744682729244232\n",
      "Epoch 6: train loss: 0.2276831865310669\n",
      "Epoch 6: train loss: 0.337249219417572\n",
      "Epoch 6: train loss: 0.24646158516407013\n",
      "Epoch 6: train loss: 0.4096513092517853\n",
      "Epoch 6: train loss: 0.19223159551620483\n",
      "Epoch 6: train loss: 0.1841105967760086\n",
      "Epoch 6: train loss: 0.23276211321353912\n",
      "Epoch 6: train loss: 0.2328096330165863\n",
      "Epoch 6: train loss: 0.4927167296409607\n",
      "Epoch 6: train loss: 0.34501907229423523\n",
      "Epoch 6: train loss: 0.26601672172546387\n",
      "Epoch 6: train loss: 0.31981462240219116\n",
      "Epoch 6: train loss: 0.3138081729412079\n",
      "Epoch 6: train loss: 0.26785188913345337\n",
      "Epoch 6: train loss: 0.26454126834869385\n",
      "Epoch 6: train loss: 0.3364274501800537\n",
      "Epoch 6: train loss: 0.22723089158535004\n",
      "Epoch 6: train loss: 0.21549227833747864\n",
      "Epoch 6: train loss: 0.23632560670375824\n",
      "Epoch 6: train loss: 0.19893497228622437\n",
      "Epoch 6: train loss: 0.35091960430145264\n",
      "Epoch 6: train loss: 0.31067270040512085\n",
      "Epoch 6: train loss: 0.24017810821533203\n",
      "Epoch 6: train loss: 0.13866589963436127\n",
      "Epoch 6: train loss: 0.14458319544792175\n",
      "Epoch 6: train loss: 0.24697136878967285\n",
      "Epoch 6: train loss: 0.29290372133255005\n",
      "Epoch 6: train loss: 0.19588947296142578\n",
      "Epoch 6: train loss: 0.13600079715251923\n",
      "Epoch 6: train loss: 0.1589098870754242\n",
      "Epoch 6: train loss: 0.43724769353866577\n",
      "Epoch 7: train loss: 0.31741294264793396\n",
      "Epoch 7: train loss: 0.26455333828926086\n",
      "Epoch 7: train loss: 0.410243958234787\n",
      "Epoch 7: train loss: 0.43137264251708984\n",
      "Epoch 7: train loss: 0.15577857196331024\n",
      "Epoch 7: train loss: 0.39921894669532776\n",
      "Epoch 7: train loss: 0.1881239265203476\n",
      "Epoch 7: train loss: 0.3177796006202698\n",
      "Epoch 7: train loss: 0.2644522488117218\n",
      "Epoch 7: train loss: 0.27434319257736206\n",
      "Epoch 7: train loss: 0.15953287482261658\n",
      "Epoch 7: train loss: 0.23473618924617767\n",
      "Epoch 7: train loss: 0.1600300520658493\n",
      "Epoch 7: train loss: 0.3175216317176819\n",
      "Epoch 7: train loss: 0.1231120228767395\n",
      "Epoch 7: train loss: 0.33061307668685913\n",
      "Epoch 7: train loss: 0.3563252389431\n",
      "Epoch 7: train loss: 0.25663989782333374\n",
      "Epoch 7: train loss: 0.34865450859069824\n",
      "Epoch 7: train loss: 0.37522566318511963\n",
      "Epoch 7: train loss: 0.24632877111434937\n",
      "Epoch 7: train loss: 0.09409064054489136\n",
      "Epoch 7: train loss: 0.1777736246585846\n",
      "Epoch 7: train loss: 0.4835573732852936\n",
      "Epoch 7: train loss: 0.08766942471265793\n",
      "Epoch 7: train loss: 0.26826903223991394\n",
      "Epoch 7: train loss: 0.4514915645122528\n",
      "Epoch 7: train loss: 0.41637828946113586\n",
      "Epoch 7: train loss: 0.27354696393013\n",
      "Epoch 7: train loss: 0.281785786151886\n",
      "Epoch 7: train loss: 0.10057345777750015\n",
      "Epoch 7: train loss: 0.1425982564687729\n",
      "Epoch 7: train loss: 0.3026372492313385\n",
      "Epoch 7: train loss: 0.284965455532074\n",
      "Epoch 7: train loss: 0.23618482053279877\n",
      "Epoch 7: train loss: 0.25086477398872375\n",
      "Epoch 7: train loss: 0.15263156592845917\n",
      "Epoch 7: train loss: 0.15123680233955383\n",
      "Epoch 7: train loss: 0.35906359553337097\n",
      "Epoch 7: train loss: 0.2361949235200882\n",
      "Epoch 7: train loss: 0.34590521454811096\n",
      "Epoch 7: train loss: 0.31370988488197327\n",
      "Epoch 7: train loss: 0.3941294252872467\n",
      "Epoch 7: train loss: 0.3453303873538971\n",
      "Epoch 8: train loss: 0.39486145973205566\n",
      "Epoch 8: train loss: 0.1680322140455246\n",
      "Epoch 8: train loss: 0.29876697063446045\n",
      "Epoch 8: train loss: 0.20489323139190674\n",
      "Epoch 8: train loss: 0.1709301471710205\n",
      "Epoch 8: train loss: 0.1747259646654129\n",
      "Epoch 8: train loss: 0.24071143567562103\n",
      "Epoch 8: train loss: 0.24465690553188324\n",
      "Epoch 8: train loss: 0.4850243926048279\n",
      "Epoch 8: train loss: 0.22286751866340637\n",
      "Epoch 8: train loss: 0.24355775117874146\n",
      "Epoch 8: train loss: 0.22878769040107727\n",
      "Epoch 8: train loss: 0.07185911387205124\n",
      "Epoch 8: train loss: 0.5044950842857361\n",
      "Epoch 8: train loss: 0.20015190541744232\n",
      "Epoch 8: train loss: 0.32127511501312256\n",
      "Epoch 8: train loss: 0.20954415202140808\n",
      "Epoch 8: train loss: 0.4062713384628296\n",
      "Epoch 8: train loss: 0.29808932542800903\n",
      "Epoch 8: train loss: 0.14691162109375\n",
      "Epoch 8: train loss: 0.22504772245883942\n",
      "Epoch 8: train loss: 0.37227314710617065\n",
      "Epoch 8: train loss: 0.10262051969766617\n",
      "Epoch 8: train loss: 0.21624881029129028\n",
      "Epoch 8: train loss: 0.26563358306884766\n",
      "Epoch 8: train loss: 0.1878555417060852\n",
      "Epoch 8: train loss: 0.20250795781612396\n",
      "Epoch 8: train loss: 0.28489741683006287\n",
      "Epoch 8: train loss: 0.25898048281669617\n",
      "Epoch 8: train loss: 0.25076594948768616\n",
      "Epoch 8: train loss: 0.4046160876750946\n",
      "Epoch 8: train loss: 0.39106646180152893\n",
      "Epoch 8: train loss: 0.5044299960136414\n",
      "Epoch 8: train loss: 0.3138543665409088\n",
      "Epoch 8: train loss: 0.28238120675086975\n",
      "Epoch 8: train loss: 0.1630823016166687\n",
      "Epoch 8: train loss: 0.15935640037059784\n",
      "Epoch 8: train loss: 0.2258034199476242\n",
      "Epoch 8: train loss: 0.5207258462905884\n",
      "Epoch 8: train loss: 0.24248827993869781\n",
      "Epoch 8: train loss: 0.3679702877998352\n",
      "Epoch 8: train loss: 0.37904414534568787\n",
      "Epoch 8: train loss: 0.15830190479755402\n",
      "Epoch 8: train loss: 0.19014383852481842\n",
      "Epoch 9: train loss: 0.23294954001903534\n",
      "Epoch 9: train loss: 0.12600596249103546\n",
      "Epoch 9: train loss: 0.16509707272052765\n",
      "Epoch 9: train loss: 0.38963040709495544\n",
      "Epoch 9: train loss: 0.22737255692481995\n",
      "Epoch 9: train loss: 0.2721008360385895\n",
      "Epoch 9: train loss: 0.3987947702407837\n",
      "Epoch 9: train loss: 0.14766375720500946\n",
      "Epoch 9: train loss: 0.27401819825172424\n",
      "Epoch 9: train loss: 0.18460367619991302\n",
      "Epoch 9: train loss: 0.44261568784713745\n",
      "Epoch 9: train loss: 0.10030105710029602\n",
      "Epoch 9: train loss: 0.2269371896982193\n",
      "Epoch 9: train loss: 0.2760920524597168\n",
      "Epoch 9: train loss: 0.28054389357566833\n",
      "Epoch 9: train loss: 0.4882378876209259\n",
      "Epoch 9: train loss: 0.1416906863451004\n",
      "Epoch 9: train loss: 0.23839960992336273\n",
      "Epoch 9: train loss: 0.44904178380966187\n",
      "Epoch 9: train loss: 0.07876718789339066\n",
      "Epoch 9: train loss: 0.10816147923469543\n",
      "Epoch 9: train loss: 0.2290745973587036\n",
      "Epoch 9: train loss: 0.17096279561519623\n",
      "Epoch 9: train loss: 0.3202340006828308\n",
      "Epoch 9: train loss: 0.15399569272994995\n",
      "Epoch 9: train loss: 0.18528857827186584\n",
      "Epoch 9: train loss: 0.124306820333004\n",
      "Epoch 9: train loss: 0.4131462275981903\n",
      "Epoch 9: train loss: 0.2811283469200134\n",
      "Epoch 9: train loss: 0.5331804752349854\n",
      "Epoch 9: train loss: 0.2466786950826645\n",
      "Epoch 9: train loss: 0.4349590241909027\n",
      "Epoch 9: train loss: 0.4696338176727295\n",
      "Epoch 9: train loss: 0.323451966047287\n",
      "Epoch 9: train loss: 0.28771087527275085\n",
      "Epoch 9: train loss: 0.19575141370296478\n",
      "Epoch 9: train loss: 0.3418761193752289\n",
      "Epoch 9: train loss: 0.15174180269241333\n",
      "Epoch 9: train loss: 0.3052819073200226\n",
      "Epoch 9: train loss: 0.38879939913749695\n",
      "Epoch 9: train loss: 0.264481782913208\n",
      "Epoch 9: train loss: 0.23988927900791168\n",
      "Epoch 9: train loss: 0.22191517055034637\n",
      "Epoch 9: train loss: 0.33345839381217957\n",
      "Epoch 10: train loss: 0.3291865289211273\n",
      "Epoch 10: train loss: 0.23935571312904358\n",
      "Epoch 10: train loss: 0.28241339325904846\n",
      "Epoch 10: train loss: 0.20297226309776306\n",
      "Epoch 10: train loss: 0.2893095314502716\n",
      "Epoch 10: train loss: 0.3567226231098175\n",
      "Epoch 10: train loss: 0.1798475980758667\n",
      "Epoch 10: train loss: 0.3213079571723938\n",
      "Epoch 10: train loss: 0.47971630096435547\n",
      "Epoch 10: train loss: 0.3664530813694\n",
      "Epoch 10: train loss: 0.15326662361621857\n",
      "Epoch 10: train loss: 0.384498655796051\n",
      "Epoch 10: train loss: 0.2920260429382324\n",
      "Epoch 10: train loss: 0.22353412210941315\n",
      "Epoch 10: train loss: 0.33026108145713806\n",
      "Epoch 10: train loss: 0.2504550814628601\n",
      "Epoch 10: train loss: 0.19908101856708527\n",
      "Epoch 10: train loss: 0.14081722497940063\n",
      "Epoch 10: train loss: 0.3257916271686554\n",
      "Epoch 10: train loss: 0.42937737703323364\n",
      "Epoch 10: train loss: 0.2578094005584717\n",
      "Epoch 10: train loss: 0.340278685092926\n",
      "Epoch 10: train loss: 0.1505853235721588\n",
      "Epoch 10: train loss: 0.3818219304084778\n",
      "Epoch 10: train loss: 0.15295296907424927\n",
      "Epoch 10: train loss: 0.1323496550321579\n",
      "Epoch 10: train loss: 0.4138619303703308\n",
      "Epoch 10: train loss: 0.1156097948551178\n",
      "Epoch 10: train loss: 0.19247359037399292\n",
      "Epoch 10: train loss: 0.33723345398902893\n",
      "Epoch 10: train loss: 0.3494575619697571\n",
      "Epoch 10: train loss: 0.3043226897716522\n",
      "Epoch 10: train loss: 0.25203588604927063\n",
      "Epoch 10: train loss: 0.28498515486717224\n",
      "Epoch 10: train loss: 0.26534178853034973\n",
      "Epoch 10: train loss: 0.18510332703590393\n",
      "Epoch 10: train loss: 0.26743969321250916\n",
      "Epoch 10: train loss: 0.2919510006904602\n",
      "Epoch 10: train loss: 0.21586447954177856\n",
      "Epoch 10: train loss: 0.2495303601026535\n",
      "Epoch 10: train loss: 0.20113356411457062\n",
      "Epoch 10: train loss: 0.14919482171535492\n",
      "Epoch 10: train loss: 0.2197585552930832\n",
      "Epoch 10: train loss: 0.166350856423378\n",
      "Epoch 11: train loss: 0.3948044776916504\n",
      "Epoch 11: train loss: 0.1582549661397934\n",
      "Epoch 11: train loss: 0.2300511598587036\n",
      "Epoch 11: train loss: 0.24032297730445862\n",
      "Epoch 11: train loss: 0.33050301671028137\n",
      "Epoch 11: train loss: 0.19706577062606812\n",
      "Epoch 11: train loss: 0.3949044644832611\n",
      "Epoch 11: train loss: 0.4828856885433197\n",
      "Epoch 11: train loss: 0.3029235303401947\n",
      "Epoch 11: train loss: 0.1815193146467209\n",
      "Epoch 11: train loss: 0.2006569802761078\n",
      "Epoch 11: train loss: 0.40800940990448\n",
      "Epoch 11: train loss: 0.37768685817718506\n",
      "Epoch 11: train loss: 0.3714052736759186\n",
      "Epoch 11: train loss: 0.1931666135787964\n",
      "Epoch 11: train loss: 0.31360265612602234\n",
      "Epoch 11: train loss: 0.3323143422603607\n",
      "Epoch 11: train loss: 0.3436920940876007\n",
      "Epoch 11: train loss: 0.24316613376140594\n",
      "Epoch 11: train loss: 0.18057608604431152\n",
      "Epoch 11: train loss: 0.33981820940971375\n",
      "Epoch 11: train loss: 0.20207378268241882\n",
      "Epoch 11: train loss: 0.22135776281356812\n",
      "Epoch 11: train loss: 0.3351353406906128\n",
      "Epoch 11: train loss: 0.21313443779945374\n",
      "Epoch 11: train loss: 0.11862824857234955\n",
      "Epoch 11: train loss: 0.25270840525627136\n",
      "Epoch 11: train loss: 0.14292290806770325\n",
      "Epoch 11: train loss: 0.3095250129699707\n",
      "Epoch 11: train loss: 0.19689404964447021\n",
      "Epoch 11: train loss: 0.2820110023021698\n",
      "Epoch 11: train loss: 0.28949669003486633\n",
      "Epoch 11: train loss: 0.1624271720647812\n",
      "Epoch 11: train loss: 0.2167641520500183\n",
      "Epoch 11: train loss: 0.22297132015228271\n",
      "Epoch 11: train loss: 0.2814459800720215\n",
      "Epoch 11: train loss: 0.23279796540737152\n",
      "Epoch 11: train loss: 0.2287934124469757\n",
      "Epoch 11: train loss: 0.2345796525478363\n",
      "Epoch 11: train loss: 0.40857329964637756\n",
      "Epoch 11: train loss: 0.29464486241340637\n",
      "Epoch 11: train loss: 0.1179322823882103\n",
      "Epoch 11: train loss: 0.25028660893440247\n",
      "Epoch 11: train loss: 0.27245089411735535\n",
      "Epoch 12: train loss: 0.29971492290496826\n",
      "Epoch 12: train loss: 0.18253445625305176\n",
      "Epoch 12: train loss: 0.3877212405204773\n",
      "Epoch 12: train loss: 0.09252616763114929\n",
      "Epoch 12: train loss: 0.23271243274211884\n",
      "Epoch 12: train loss: 0.3133228123188019\n",
      "Epoch 12: train loss: 0.12150122225284576\n",
      "Epoch 12: train loss: 0.38191285729408264\n",
      "Epoch 12: train loss: 0.1994285136461258\n",
      "Epoch 12: train loss: 0.36041682958602905\n",
      "Epoch 12: train loss: 0.2501697540283203\n",
      "Epoch 12: train loss: 0.445218563079834\n",
      "Epoch 12: train loss: 0.21320118010044098\n",
      "Epoch 12: train loss: 0.11714735627174377\n",
      "Epoch 12: train loss: 0.23522916436195374\n",
      "Epoch 12: train loss: 0.5467793345451355\n",
      "Epoch 12: train loss: 0.20875833928585052\n",
      "Epoch 12: train loss: 0.18425193428993225\n",
      "Epoch 12: train loss: 0.2892952561378479\n",
      "Epoch 12: train loss: 0.2999056279659271\n",
      "Epoch 12: train loss: 0.24374572932720184\n",
      "Epoch 12: train loss: 0.34429800510406494\n",
      "Epoch 12: train loss: 0.2562062740325928\n",
      "Epoch 12: train loss: 0.31030845642089844\n",
      "Epoch 12: train loss: 0.26383480429649353\n",
      "Epoch 12: train loss: 0.29376909136772156\n",
      "Epoch 12: train loss: 0.2624111473560333\n",
      "Epoch 12: train loss: 0.18868611752986908\n",
      "Epoch 12: train loss: 0.20770928263664246\n",
      "Epoch 12: train loss: 0.2213667631149292\n",
      "Epoch 12: train loss: 0.18328852951526642\n",
      "Epoch 12: train loss: 0.252101331949234\n",
      "Epoch 12: train loss: 0.3300538659095764\n",
      "Epoch 12: train loss: 0.25931641459465027\n",
      "Epoch 12: train loss: 0.28227850794792175\n",
      "Epoch 12: train loss: 0.326895147562027\n",
      "Epoch 12: train loss: 0.22653335332870483\n",
      "Epoch 12: train loss: 0.19568048417568207\n",
      "Epoch 12: train loss: 0.2521863281726837\n",
      "Epoch 12: train loss: 0.26076093316078186\n",
      "Epoch 12: train loss: 0.18672099709510803\n",
      "Epoch 12: train loss: 0.25840044021606445\n",
      "Epoch 12: train loss: 0.42767786979675293\n",
      "Epoch 12: train loss: 0.17887666821479797\n",
      "Epoch 13: train loss: 0.15021251142024994\n",
      "Epoch 13: train loss: 0.2600420415401459\n",
      "Epoch 13: train loss: 0.24497762322425842\n",
      "Epoch 13: train loss: 0.28138142824172974\n",
      "Epoch 13: train loss: 0.16030338406562805\n",
      "Epoch 13: train loss: 0.1454765945672989\n",
      "Epoch 13: train loss: 0.3271728754043579\n",
      "Epoch 13: train loss: 0.11773347109556198\n",
      "Epoch 13: train loss: 0.10218899697065353\n",
      "Epoch 13: train loss: 0.40810880064964294\n",
      "Epoch 13: train loss: 0.09373413771390915\n",
      "Epoch 13: train loss: 0.19818292558193207\n",
      "Epoch 13: train loss: 0.40875697135925293\n",
      "Epoch 13: train loss: 0.5807396769523621\n",
      "Epoch 13: train loss: 0.09426382929086685\n",
      "Epoch 13: train loss: 0.38139480352401733\n",
      "Epoch 13: train loss: 0.2716829180717468\n",
      "Epoch 13: train loss: 0.37557512521743774\n",
      "Epoch 13: train loss: 0.19526448845863342\n",
      "Epoch 13: train loss: 0.19905562698841095\n",
      "Epoch 13: train loss: 0.13773351907730103\n",
      "Epoch 13: train loss: 0.1672624796628952\n",
      "Epoch 13: train loss: 0.3725636601448059\n",
      "Epoch 13: train loss: 0.1926848441362381\n",
      "Epoch 13: train loss: 0.35727205872535706\n",
      "Epoch 13: train loss: 0.11475580185651779\n",
      "Epoch 13: train loss: 0.20196649432182312\n",
      "Epoch 13: train loss: 0.13721026480197906\n",
      "Epoch 13: train loss: 0.24758164584636688\n",
      "Epoch 13: train loss: 0.46005868911743164\n",
      "Epoch 13: train loss: 0.1902826577425003\n",
      "Epoch 13: train loss: 0.43736520409584045\n",
      "Epoch 13: train loss: 0.09363214671611786\n",
      "Epoch 13: train loss: 0.2047109454870224\n",
      "Epoch 13: train loss: 0.2557090222835541\n",
      "Epoch 13: train loss: 0.4252944886684418\n",
      "Epoch 13: train loss: 0.3475204408168793\n",
      "Epoch 13: train loss: 0.31533071398735046\n",
      "Epoch 13: train loss: 0.34169936180114746\n",
      "Epoch 13: train loss: 0.21714186668395996\n",
      "Epoch 13: train loss: 0.2907043397426605\n",
      "Epoch 13: train loss: 0.3053334057331085\n",
      "Epoch 13: train loss: 0.42835310101509094\n",
      "Epoch 13: train loss: 0.24469469487667084\n",
      "Epoch 14: train loss: 0.36073821783065796\n",
      "Epoch 14: train loss: 0.23641300201416016\n",
      "Epoch 14: train loss: 0.20166662335395813\n",
      "Epoch 14: train loss: 0.31520983576774597\n",
      "Epoch 14: train loss: 0.15138934552669525\n",
      "Epoch 14: train loss: 0.2725677490234375\n",
      "Epoch 14: train loss: 0.33166617155075073\n",
      "Epoch 14: train loss: 0.1127089187502861\n",
      "Epoch 14: train loss: 0.33593177795410156\n",
      "Epoch 14: train loss: 0.4186260402202606\n",
      "Epoch 14: train loss: 0.28186920285224915\n",
      "Epoch 14: train loss: 0.34639841318130493\n",
      "Epoch 14: train loss: 0.12337816506624222\n",
      "Epoch 14: train loss: 0.26319724321365356\n",
      "Epoch 14: train loss: 0.09683413803577423\n",
      "Epoch 14: train loss: 0.33786454796791077\n",
      "Epoch 14: train loss: 0.25864237546920776\n",
      "Epoch 14: train loss: 0.26981592178344727\n",
      "Epoch 14: train loss: 0.1331002116203308\n",
      "Epoch 14: train loss: 0.17434100806713104\n",
      "Epoch 14: train loss: 0.236282080411911\n",
      "Epoch 14: train loss: 0.2943727374076843\n",
      "Epoch 14: train loss: 0.4286450445652008\n",
      "Epoch 14: train loss: 0.15448246896266937\n",
      "Epoch 14: train loss: 0.26139211654663086\n",
      "Epoch 14: train loss: 0.22089339792728424\n",
      "Epoch 14: train loss: 0.20911963284015656\n",
      "Epoch 14: train loss: 0.3053935468196869\n",
      "Epoch 14: train loss: 0.4065456688404083\n",
      "Epoch 14: train loss: 0.2363809496164322\n",
      "Epoch 14: train loss: 0.18414877355098724\n",
      "Epoch 14: train loss: 0.24034914374351501\n",
      "Epoch 14: train loss: 0.21566465497016907\n",
      "Epoch 14: train loss: 0.2805514335632324\n",
      "Epoch 14: train loss: 0.358504056930542\n",
      "Epoch 14: train loss: 0.22025884687900543\n",
      "Epoch 14: train loss: 0.2010071575641632\n",
      "Epoch 14: train loss: 0.40092146396636963\n",
      "Epoch 14: train loss: 0.13994719088077545\n",
      "Epoch 14: train loss: 0.32197368144989014\n",
      "Epoch 14: train loss: 0.5010844469070435\n",
      "Epoch 14: train loss: 0.13291269540786743\n",
      "Epoch 14: train loss: 0.2693863809108734\n",
      "Epoch 14: train loss: 0.2909696400165558\n",
      "Epoch 15: train loss: 0.2554647624492645\n",
      "Epoch 15: train loss: 0.09990648925304413\n",
      "Epoch 15: train loss: 0.0687541663646698\n",
      "Epoch 15: train loss: 0.2166632115840912\n",
      "Epoch 15: train loss: 0.19859397411346436\n",
      "Epoch 15: train loss: 0.21905241906642914\n",
      "Epoch 15: train loss: 0.15224148333072662\n",
      "Epoch 15: train loss: 0.2574383020401001\n",
      "Epoch 15: train loss: 0.24784302711486816\n",
      "Epoch 15: train loss: 0.3510128855705261\n",
      "Epoch 15: train loss: 0.3082174062728882\n",
      "Epoch 15: train loss: 0.5238193273544312\n",
      "Epoch 15: train loss: 0.5725641846656799\n",
      "Epoch 15: train loss: 0.3670414090156555\n",
      "Epoch 15: train loss: 0.3458190858364105\n",
      "Epoch 15: train loss: 0.38747909665107727\n",
      "Epoch 15: train loss: 0.15350289642810822\n",
      "Epoch 15: train loss: 0.1464332938194275\n",
      "Epoch 15: train loss: 0.20642226934432983\n",
      "Epoch 15: train loss: 0.2826599180698395\n",
      "Epoch 15: train loss: 0.22325584292411804\n",
      "Epoch 15: train loss: 0.3609551191329956\n",
      "Epoch 15: train loss: 0.3552364408969879\n",
      "Epoch 15: train loss: 0.2194364070892334\n",
      "Epoch 15: train loss: 0.21964502334594727\n",
      "Epoch 15: train loss: 0.28659874200820923\n",
      "Epoch 15: train loss: 0.1805965155363083\n",
      "Epoch 15: train loss: 0.15303397178649902\n",
      "Epoch 15: train loss: 0.38186612725257874\n",
      "Epoch 15: train loss: 0.15853063762187958\n",
      "Epoch 15: train loss: 0.09415452182292938\n",
      "Epoch 15: train loss: 0.42549920082092285\n",
      "Epoch 15: train loss: 0.24116550385951996\n",
      "Epoch 15: train loss: 0.5033175349235535\n",
      "Epoch 15: train loss: 0.18888799846172333\n",
      "Epoch 15: train loss: 0.08326029777526855\n",
      "Epoch 15: train loss: 0.19137436151504517\n",
      "Epoch 15: train loss: 0.2872104048728943\n",
      "Epoch 15: train loss: 0.2300775796175003\n",
      "Epoch 15: train loss: 0.2732835114002228\n",
      "Epoch 15: train loss: 0.31645700335502625\n",
      "Epoch 15: train loss: 0.3970111310482025\n",
      "Epoch 15: train loss: 0.27089768648147583\n",
      "Epoch 15: train loss: 0.23399145901203156\n",
      "Epoch 16: train loss: 0.2530795931816101\n",
      "Epoch 16: train loss: 0.07044868916273117\n",
      "Epoch 16: train loss: 0.29711729288101196\n",
      "Epoch 16: train loss: 0.21306158602237701\n",
      "Epoch 16: train loss: 0.48772233724594116\n",
      "Epoch 16: train loss: 0.18534420430660248\n",
      "Epoch 16: train loss: 0.21489264070987701\n",
      "Epoch 16: train loss: 0.2654058337211609\n",
      "Epoch 16: train loss: 0.4921630620956421\n",
      "Epoch 16: train loss: 0.13020655512809753\n",
      "Epoch 16: train loss: 0.3454362154006958\n",
      "Epoch 16: train loss: 0.17646749317646027\n",
      "Epoch 16: train loss: 0.29101186990737915\n",
      "Epoch 16: train loss: 0.2864164710044861\n",
      "Epoch 16: train loss: 0.11947821080684662\n",
      "Epoch 16: train loss: 0.2810749113559723\n",
      "Epoch 16: train loss: 0.3180959224700928\n",
      "Epoch 16: train loss: 0.2940814197063446\n",
      "Epoch 16: train loss: 0.22352276742458344\n",
      "Epoch 16: train loss: 0.13820168375968933\n",
      "Epoch 16: train loss: 0.20554512739181519\n",
      "Epoch 16: train loss: 0.2440136820077896\n",
      "Epoch 16: train loss: 0.220195934176445\n",
      "Epoch 16: train loss: 0.5650432109832764\n",
      "Epoch 16: train loss: 0.3324957489967346\n",
      "Epoch 16: train loss: 0.1894151270389557\n",
      "Epoch 16: train loss: 0.2749168276786804\n",
      "Epoch 16: train loss: 0.17534926533699036\n",
      "Epoch 16: train loss: 0.21178650856018066\n",
      "Epoch 16: train loss: 0.22179225087165833\n",
      "Epoch 16: train loss: 0.13919584453105927\n",
      "Epoch 16: train loss: 0.27361899614334106\n",
      "Epoch 16: train loss: 0.5224432349205017\n",
      "Epoch 16: train loss: 0.23764833807945251\n",
      "Epoch 16: train loss: 0.21450287103652954\n",
      "Epoch 16: train loss: 0.13598410785198212\n",
      "Epoch 16: train loss: 0.14563219249248505\n",
      "Epoch 16: train loss: 0.21342919766902924\n",
      "Epoch 16: train loss: 0.2077179253101349\n",
      "Epoch 16: train loss: 0.2623720169067383\n",
      "Epoch 16: train loss: 0.335190087556839\n",
      "Epoch 16: train loss: 0.518596351146698\n",
      "Epoch 16: train loss: 0.13637982308864594\n",
      "Epoch 16: train loss: 0.3050655126571655\n",
      "Epoch 17: train loss: 0.21987248957157135\n",
      "Epoch 17: train loss: 0.264622300863266\n",
      "Epoch 17: train loss: 0.42370742559432983\n",
      "Epoch 17: train loss: 0.35097643733024597\n",
      "Epoch 17: train loss: 0.24324440956115723\n",
      "Epoch 17: train loss: 0.3500939607620239\n",
      "Epoch 17: train loss: 0.14745374023914337\n",
      "Epoch 17: train loss: 0.270885705947876\n",
      "Epoch 17: train loss: 0.5269664525985718\n",
      "Epoch 17: train loss: 0.15180939435958862\n",
      "Epoch 17: train loss: 0.2280948907136917\n",
      "Epoch 17: train loss: 0.32390493154525757\n",
      "Epoch 17: train loss: 0.18594205379486084\n",
      "Epoch 17: train loss: 0.2904658317565918\n",
      "Epoch 17: train loss: 0.3829972743988037\n",
      "Epoch 17: train loss: 0.37465766072273254\n",
      "Epoch 17: train loss: 0.18723757565021515\n",
      "Epoch 17: train loss: 0.2509550452232361\n",
      "Epoch 17: train loss: 0.20393341779708862\n",
      "Epoch 17: train loss: 0.22413162887096405\n",
      "Epoch 17: train loss: 0.23606310784816742\n",
      "Epoch 17: train loss: 0.15727850794792175\n",
      "Epoch 17: train loss: 0.17383918166160583\n",
      "Epoch 17: train loss: 0.2573569416999817\n",
      "Epoch 17: train loss: 0.2967960238456726\n",
      "Epoch 17: train loss: 0.31668558716773987\n",
      "Epoch 17: train loss: 0.17457620799541473\n",
      "Epoch 17: train loss: 0.09807678312063217\n",
      "Epoch 17: train loss: 0.386526495218277\n",
      "Epoch 17: train loss: 0.39582177996635437\n",
      "Epoch 17: train loss: 0.25859397649765015\n",
      "Epoch 17: train loss: 0.2098907232284546\n",
      "Epoch 17: train loss: 0.10767313838005066\n",
      "Epoch 17: train loss: 0.24879096448421478\n",
      "Epoch 17: train loss: 0.29681646823883057\n",
      "Epoch 17: train loss: 0.20701351761817932\n",
      "Epoch 17: train loss: 0.3241743743419647\n",
      "Epoch 17: train loss: 0.28564608097076416\n",
      "Epoch 17: train loss: 0.231282040476799\n",
      "Epoch 17: train loss: 0.17651326954364777\n",
      "Epoch 17: train loss: 0.15997326374053955\n",
      "Epoch 17: train loss: 0.18731531500816345\n",
      "Epoch 17: train loss: 0.3267851769924164\n",
      "Epoch 17: train loss: 0.17952506244182587\n",
      "Epoch 18: train loss: 0.2970702350139618\n",
      "Epoch 18: train loss: 0.1005183681845665\n",
      "Epoch 18: train loss: 0.2773120403289795\n",
      "Epoch 18: train loss: 0.11392275989055634\n",
      "Epoch 18: train loss: 0.1716349869966507\n",
      "Epoch 18: train loss: 0.4941578805446625\n",
      "Epoch 18: train loss: 0.25145015120506287\n",
      "Epoch 18: train loss: 0.2678486406803131\n",
      "Epoch 18: train loss: 0.23969142138957977\n",
      "Epoch 18: train loss: 0.1395472288131714\n",
      "Epoch 18: train loss: 0.07583866268396378\n",
      "Epoch 18: train loss: 0.15901988744735718\n",
      "Epoch 18: train loss: 0.2078516185283661\n",
      "Epoch 18: train loss: 0.2757570147514343\n",
      "Epoch 18: train loss: 0.19182373583316803\n",
      "Epoch 18: train loss: 0.595588743686676\n",
      "Epoch 18: train loss: 0.17676647007465363\n",
      "Epoch 18: train loss: 0.28456801176071167\n",
      "Epoch 18: train loss: 0.3094950318336487\n",
      "Epoch 18: train loss: 0.31537115573883057\n",
      "Epoch 18: train loss: 0.20986947417259216\n",
      "Epoch 18: train loss: 0.47101590037345886\n",
      "Epoch 18: train loss: 0.10112763941287994\n",
      "Epoch 18: train loss: 0.27933523058891296\n",
      "Epoch 18: train loss: 0.2729017436504364\n",
      "Epoch 18: train loss: 0.19995103776454926\n",
      "Epoch 18: train loss: 0.10140971094369888\n",
      "Epoch 18: train loss: 0.23098739981651306\n",
      "Epoch 18: train loss: 0.17280587553977966\n",
      "Epoch 18: train loss: 0.43417027592658997\n",
      "Epoch 18: train loss: 0.44221752882003784\n",
      "Epoch 18: train loss: 0.2914047837257385\n",
      "Epoch 18: train loss: 0.19565542042255402\n",
      "Epoch 18: train loss: 0.49240386486053467\n",
      "Epoch 18: train loss: 0.1779661774635315\n",
      "Epoch 18: train loss: 0.33293724060058594\n",
      "Epoch 18: train loss: 0.24216637015342712\n",
      "Epoch 18: train loss: 0.295449435710907\n",
      "Epoch 18: train loss: 0.44649577140808105\n",
      "Epoch 18: train loss: 0.14828504621982574\n",
      "Epoch 18: train loss: 0.27786287665367126\n",
      "Epoch 18: train loss: 0.2269897162914276\n",
      "Epoch 18: train loss: 0.1967349350452423\n",
      "Epoch 18: train loss: 0.12225809693336487\n",
      "Epoch 19: train loss: 0.23635704815387726\n",
      "Epoch 19: train loss: 0.192327082157135\n",
      "Epoch 19: train loss: 0.4403592646121979\n",
      "Epoch 19: train loss: 0.5294350385665894\n",
      "Epoch 19: train loss: 0.5433446764945984\n",
      "Epoch 19: train loss: 0.384229838848114\n",
      "Epoch 19: train loss: 0.13327564299106598\n",
      "Epoch 19: train loss: 0.22458919882774353\n",
      "Epoch 19: train loss: 0.22666913270950317\n",
      "Epoch 19: train loss: 0.14405296742916107\n",
      "Epoch 19: train loss: 0.20475725829601288\n",
      "Epoch 19: train loss: 0.156295046210289\n",
      "Epoch 19: train loss: 0.21629580855369568\n",
      "Epoch 19: train loss: 0.2439061999320984\n",
      "Epoch 19: train loss: 0.257072389125824\n",
      "Epoch 19: train loss: 0.23018412292003632\n",
      "Epoch 19: train loss: 0.12390188872814178\n",
      "Epoch 19: train loss: 0.38083258271217346\n",
      "Epoch 19: train loss: 0.5627858638763428\n",
      "Epoch 19: train loss: 0.07019587606191635\n",
      "Epoch 19: train loss: 0.334494024515152\n",
      "Epoch 19: train loss: 0.33337175846099854\n",
      "Epoch 19: train loss: 0.23553235828876495\n",
      "Epoch 19: train loss: 0.13511012494564056\n",
      "Epoch 19: train loss: 0.08108923584222794\n",
      "Epoch 19: train loss: 0.2588317394256592\n",
      "Epoch 19: train loss: 0.16017232835292816\n",
      "Epoch 19: train loss: 0.4803297817707062\n",
      "Epoch 19: train loss: 0.09233038872480392\n",
      "Epoch 19: train loss: 0.33083856105804443\n",
      "Epoch 19: train loss: 0.16831442713737488\n",
      "Epoch 19: train loss: 0.18843623995780945\n",
      "Epoch 19: train loss: 0.2130684107542038\n",
      "Epoch 19: train loss: 0.22402647137641907\n",
      "Epoch 19: train loss: 0.17857412993907928\n",
      "Epoch 19: train loss: 0.4119528532028198\n",
      "Epoch 19: train loss: 0.31406283378601074\n",
      "Epoch 19: train loss: 0.1933300793170929\n",
      "Epoch 19: train loss: 0.3122657537460327\n",
      "Epoch 19: train loss: 0.30255839228630066\n",
      "Epoch 19: train loss: 0.17945566773414612\n",
      "Epoch 19: train loss: 0.23526450991630554\n",
      "Epoch 19: train loss: 0.1806001216173172\n",
      "Epoch 19: train loss: 0.30141153931617737\n",
      "Epoch 20: train loss: 0.17897339165210724\n",
      "Epoch 20: train loss: 0.34760355949401855\n",
      "Epoch 20: train loss: 0.227171391248703\n",
      "Epoch 20: train loss: 0.4706021845340729\n",
      "Epoch 20: train loss: 0.3308030366897583\n",
      "Epoch 20: train loss: 0.24822208285331726\n",
      "Epoch 20: train loss: 0.16075928509235382\n",
      "Epoch 20: train loss: 0.20165540277957916\n",
      "Epoch 20: train loss: 0.26128682494163513\n",
      "Epoch 20: train loss: 0.334322065114975\n",
      "Epoch 20: train loss: 0.34838587045669556\n",
      "Epoch 20: train loss: 0.24028989672660828\n",
      "Epoch 20: train loss: 0.4090633690357208\n",
      "Epoch 20: train loss: 0.18584534525871277\n",
      "Epoch 20: train loss: 0.2925899028778076\n",
      "Epoch 20: train loss: 0.10243548452854156\n",
      "Epoch 20: train loss: 0.3497578799724579\n",
      "Epoch 20: train loss: 0.3815770447254181\n",
      "Epoch 20: train loss: 0.27716758847236633\n",
      "Epoch 20: train loss: 0.47142404317855835\n",
      "Epoch 20: train loss: 0.21316660940647125\n",
      "Epoch 20: train loss: 0.2742379605770111\n",
      "Epoch 20: train loss: 0.21759961545467377\n",
      "Epoch 20: train loss: 0.2680855393409729\n",
      "Epoch 20: train loss: 0.17185702919960022\n",
      "Epoch 20: train loss: 0.2443121075630188\n",
      "Epoch 20: train loss: 0.2356937974691391\n",
      "Epoch 20: train loss: 0.25199976563453674\n",
      "Epoch 20: train loss: 0.07788027077913284\n",
      "Epoch 20: train loss: 0.07625018805265427\n",
      "Epoch 20: train loss: 0.12196773290634155\n",
      "Epoch 20: train loss: 0.28273260593414307\n",
      "Epoch 20: train loss: 0.4121537208557129\n",
      "Epoch 20: train loss: 0.16693799197673798\n",
      "Epoch 20: train loss: 0.1852993220090866\n",
      "Epoch 20: train loss: 0.2429598569869995\n",
      "Epoch 20: train loss: 0.3785282075405121\n",
      "Epoch 20: train loss: 0.36352986097335815\n",
      "Epoch 20: train loss: 0.111788310110569\n",
      "Epoch 20: train loss: 0.2220861315727234\n",
      "Epoch 20: train loss: 0.2724965214729309\n",
      "Epoch 20: train loss: 0.19951453804969788\n",
      "Epoch 20: train loss: 0.317290723323822\n",
      "Epoch 20: train loss: 0.12736289203166962\n",
      "Epoch 21: train loss: 0.23408061265945435\n",
      "Epoch 21: train loss: 0.14962643384933472\n",
      "Epoch 21: train loss: 0.2157590091228485\n",
      "Epoch 21: train loss: 0.15828052163124084\n",
      "Epoch 21: train loss: 0.20352661609649658\n",
      "Epoch 21: train loss: 0.2785988748073578\n",
      "Epoch 21: train loss: 0.5260246396064758\n",
      "Epoch 21: train loss: 0.23361170291900635\n",
      "Epoch 21: train loss: 0.14344850182533264\n",
      "Epoch 21: train loss: 0.204633429646492\n",
      "Epoch 21: train loss: 0.2514186501502991\n",
      "Epoch 21: train loss: 0.20129433274269104\n",
      "Epoch 21: train loss: 0.21051707863807678\n",
      "Epoch 21: train loss: 0.3257019519805908\n",
      "Epoch 21: train loss: 0.18551521003246307\n",
      "Epoch 21: train loss: 0.21228794753551483\n",
      "Epoch 21: train loss: 0.1736905574798584\n",
      "Epoch 21: train loss: 0.3426603376865387\n",
      "Epoch 21: train loss: 0.3649255335330963\n",
      "Epoch 21: train loss: 0.17115487158298492\n",
      "Epoch 21: train loss: 0.19399870932102203\n",
      "Epoch 21: train loss: 0.3798750340938568\n",
      "Epoch 21: train loss: 0.34696754813194275\n",
      "Epoch 21: train loss: 0.09782663732767105\n",
      "Epoch 21: train loss: 0.19501033425331116\n",
      "Epoch 21: train loss: 0.36513981223106384\n",
      "Epoch 21: train loss: 0.23657575249671936\n",
      "Epoch 21: train loss: 0.18604141473770142\n",
      "Epoch 21: train loss: 0.10902427136898041\n",
      "Epoch 21: train loss: 0.3944282531738281\n",
      "Epoch 21: train loss: 0.2876969277858734\n",
      "Epoch 21: train loss: 0.2327270358800888\n",
      "Epoch 21: train loss: 0.26466768980026245\n",
      "Epoch 21: train loss: 0.2945660650730133\n",
      "Epoch 21: train loss: 0.19354207813739777\n",
      "Epoch 21: train loss: 0.29523032903671265\n",
      "Epoch 21: train loss: 0.2584074139595032\n",
      "Epoch 21: train loss: 0.4387369453907013\n",
      "Epoch 21: train loss: 0.2097686529159546\n",
      "Epoch 21: train loss: 0.36902907490730286\n",
      "Epoch 21: train loss: 0.2797653079032898\n",
      "Epoch 21: train loss: 0.24833758175373077\n",
      "Epoch 21: train loss: 0.32928135991096497\n",
      "Epoch 21: train loss: 0.21864300966262817\n",
      "Epoch 22: train loss: 0.10750136524438858\n",
      "Epoch 22: train loss: 0.14439630508422852\n",
      "Epoch 22: train loss: 0.32751262187957764\n",
      "Epoch 22: train loss: 0.31471821665763855\n",
      "Epoch 22: train loss: 0.2904561460018158\n",
      "Epoch 22: train loss: 0.2386326789855957\n",
      "Epoch 22: train loss: 0.3309647738933563\n",
      "Epoch 22: train loss: 0.29579639434814453\n",
      "Epoch 22: train loss: 0.32590559124946594\n",
      "Epoch 22: train loss: 0.22701631486415863\n",
      "Epoch 22: train loss: 0.3543912172317505\n",
      "Epoch 22: train loss: 0.25037136673927307\n",
      "Epoch 22: train loss: 0.27059701085090637\n",
      "Epoch 22: train loss: 0.3040767312049866\n",
      "Epoch 22: train loss: 0.24069121479988098\n",
      "Epoch 22: train loss: 0.1338748037815094\n",
      "Epoch 22: train loss: 0.45259732007980347\n",
      "Epoch 22: train loss: 0.3328288495540619\n",
      "Epoch 22: train loss: 0.23386815190315247\n",
      "Epoch 22: train loss: 0.2019629031419754\n",
      "Epoch 22: train loss: 0.16095156967639923\n",
      "Epoch 22: train loss: 0.13940323889255524\n",
      "Epoch 22: train loss: 0.14706456661224365\n",
      "Epoch 22: train loss: 0.30472755432128906\n",
      "Epoch 22: train loss: 0.37171077728271484\n",
      "Epoch 22: train loss: 0.5029082894325256\n",
      "Epoch 22: train loss: 0.24500709772109985\n",
      "Epoch 22: train loss: 0.1873781681060791\n",
      "Epoch 22: train loss: 0.2995937466621399\n",
      "Epoch 22: train loss: 0.11343276500701904\n",
      "Epoch 22: train loss: 0.13740983605384827\n",
      "Epoch 22: train loss: 0.2810237407684326\n",
      "Epoch 22: train loss: 0.29998138546943665\n",
      "Epoch 22: train loss: 0.26165592670440674\n",
      "Epoch 22: train loss: 0.23282811045646667\n",
      "Epoch 22: train loss: 0.1582987755537033\n",
      "Epoch 22: train loss: 0.231573224067688\n",
      "Epoch 22: train loss: 0.27645912766456604\n",
      "Epoch 22: train loss: 0.16561071574687958\n",
      "Epoch 22: train loss: 0.27760395407676697\n",
      "Epoch 22: train loss: 0.32190659642219543\n",
      "Epoch 22: train loss: 0.28284770250320435\n",
      "Epoch 22: train loss: 0.3111790716648102\n",
      "Epoch 22: train loss: 0.1904817521572113\n",
      "Epoch 23: train loss: 0.3986533284187317\n",
      "Epoch 23: train loss: 0.22482502460479736\n",
      "Epoch 23: train loss: 0.3270084857940674\n",
      "Epoch 23: train loss: 0.3637276291847229\n",
      "Epoch 23: train loss: 0.2896914482116699\n",
      "Epoch 23: train loss: 0.21708737313747406\n",
      "Epoch 23: train loss: 0.282928466796875\n",
      "Epoch 23: train loss: 0.2812354564666748\n",
      "Epoch 23: train loss: 0.13467490673065186\n",
      "Epoch 23: train loss: 0.45687335729599\n",
      "Epoch 23: train loss: 0.1882300227880478\n",
      "Epoch 23: train loss: 0.2532806396484375\n",
      "Epoch 23: train loss: 0.21905860304832458\n",
      "Epoch 23: train loss: 0.20316872000694275\n",
      "Epoch 23: train loss: 0.1579643189907074\n",
      "Epoch 23: train loss: 0.2178514003753662\n",
      "Epoch 23: train loss: 0.2255498319864273\n",
      "Epoch 23: train loss: 0.1237763911485672\n",
      "Epoch 23: train loss: 0.28202489018440247\n",
      "Epoch 23: train loss: 0.2529575228691101\n",
      "Epoch 23: train loss: 0.47155502438545227\n",
      "Epoch 23: train loss: 0.3789695203304291\n",
      "Epoch 23: train loss: 0.29243794083595276\n",
      "Epoch 23: train loss: 0.1610635370016098\n",
      "Epoch 23: train loss: 0.3117672801017761\n",
      "Epoch 23: train loss: 0.14173690974712372\n",
      "Epoch 23: train loss: 0.27077770233154297\n",
      "Epoch 23: train loss: 0.4089156985282898\n",
      "Epoch 23: train loss: 0.3262587785720825\n",
      "Epoch 23: train loss: 0.1999969631433487\n",
      "Epoch 23: train loss: 0.3810320496559143\n",
      "Epoch 23: train loss: 0.24741174280643463\n",
      "Epoch 23: train loss: 0.24261543154716492\n",
      "Epoch 23: train loss: 0.08344197273254395\n",
      "Epoch 23: train loss: 0.13981956243515015\n",
      "Epoch 23: train loss: 0.28242552280426025\n",
      "Epoch 23: train loss: 0.17637227475643158\n",
      "Epoch 23: train loss: 0.13710889220237732\n",
      "Epoch 23: train loss: 0.35927337408065796\n",
      "Epoch 23: train loss: 0.13039596378803253\n",
      "Epoch 23: train loss: 0.3320980966091156\n",
      "Epoch 23: train loss: 0.13415013253688812\n",
      "Epoch 23: train loss: 0.18966013193130493\n",
      "Epoch 23: train loss: 0.30569103360176086\n",
      "Epoch 24: train loss: 0.5610897541046143\n",
      "Epoch 24: train loss: 0.34794002771377563\n",
      "Epoch 24: train loss: 0.43648746609687805\n",
      "Epoch 24: train loss: 0.33178791403770447\n",
      "Epoch 24: train loss: 0.317036896944046\n",
      "Epoch 24: train loss: 0.21330730617046356\n",
      "Epoch 24: train loss: 0.1749018281698227\n",
      "Epoch 24: train loss: 0.09322691708803177\n",
      "Epoch 24: train loss: 0.2737201452255249\n",
      "Epoch 24: train loss: 0.435947984457016\n",
      "Epoch 24: train loss: 0.11545837670564651\n",
      "Epoch 24: train loss: 0.2186102271080017\n",
      "Epoch 24: train loss: 0.2510581314563751\n",
      "Epoch 24: train loss: 0.2858211100101471\n",
      "Epoch 24: train loss: 0.18333934247493744\n",
      "Epoch 24: train loss: 0.2201332449913025\n",
      "Epoch 24: train loss: 0.33134925365448\n",
      "Epoch 24: train loss: 0.11700550466775894\n",
      "Epoch 24: train loss: 0.3154153823852539\n",
      "Epoch 24: train loss: 0.2968750298023224\n",
      "Epoch 24: train loss: 0.2513139843940735\n",
      "Epoch 24: train loss: 0.1153007447719574\n",
      "Epoch 24: train loss: 0.346780925989151\n",
      "Epoch 24: train loss: 0.14545290172100067\n",
      "Epoch 24: train loss: 0.11680859327316284\n",
      "Epoch 24: train loss: 0.06696532666683197\n",
      "Epoch 24: train loss: 0.44698894023895264\n",
      "Epoch 24: train loss: 0.12856259942054749\n",
      "Epoch 24: train loss: 0.17804041504859924\n",
      "Epoch 24: train loss: 0.5066736340522766\n",
      "Epoch 24: train loss: 0.16531869769096375\n",
      "Epoch 24: train loss: 0.46765995025634766\n",
      "Epoch 24: train loss: 0.15456049144268036\n",
      "Epoch 24: train loss: 0.47768616676330566\n",
      "Epoch 24: train loss: 0.15599274635314941\n",
      "Epoch 24: train loss: 0.17868365347385406\n",
      "Epoch 24: train loss: 0.134491965174675\n",
      "Epoch 24: train loss: 0.3254161775112152\n",
      "Epoch 24: train loss: 0.1992826908826828\n",
      "Epoch 24: train loss: 0.17888295650482178\n",
      "Epoch 24: train loss: 0.16975542902946472\n",
      "Epoch 24: train loss: 0.35390758514404297\n",
      "Epoch 24: train loss: 0.2483883947134018\n",
      "Epoch 24: train loss: 0.20206567645072937\n",
      "Epoch 25: train loss: 0.3356437385082245\n",
      "Epoch 25: train loss: 0.29520007967948914\n",
      "Epoch 25: train loss: 0.11368438601493835\n",
      "Epoch 25: train loss: 0.20156541466712952\n",
      "Epoch 25: train loss: 0.136418879032135\n",
      "Epoch 25: train loss: 0.12932564318180084\n",
      "Epoch 25: train loss: 0.2618876099586487\n",
      "Epoch 25: train loss: 0.19518975913524628\n",
      "Epoch 25: train loss: 0.21494515240192413\n",
      "Epoch 25: train loss: 0.3553025722503662\n",
      "Epoch 25: train loss: 0.1473328024148941\n",
      "Epoch 25: train loss: 0.19155748188495636\n",
      "Epoch 25: train loss: 0.38230910897254944\n",
      "Epoch 25: train loss: 0.47319838404655457\n",
      "Epoch 25: train loss: 0.37521103024482727\n",
      "Epoch 25: train loss: 0.2715931534767151\n",
      "Epoch 25: train loss: 0.27630820870399475\n",
      "Epoch 25: train loss: 0.23071937263011932\n",
      "Epoch 25: train loss: 0.30640795826911926\n",
      "Epoch 25: train loss: 0.24243667721748352\n",
      "Epoch 25: train loss: 0.3448067009449005\n",
      "Epoch 25: train loss: 0.4416162073612213\n",
      "Epoch 25: train loss: 0.101598359644413\n",
      "Epoch 25: train loss: 0.1677117943763733\n",
      "Epoch 25: train loss: 0.23133204877376556\n",
      "Epoch 25: train loss: 0.17950433492660522\n",
      "Epoch 25: train loss: 0.41902369260787964\n",
      "Epoch 25: train loss: 0.20027123391628265\n",
      "Epoch 25: train loss: 0.3126612901687622\n",
      "Epoch 25: train loss: 0.2378913313150406\n",
      "Epoch 25: train loss: 0.18324418365955353\n",
      "Epoch 25: train loss: 0.42493316531181335\n",
      "Epoch 25: train loss: 0.2981879413127899\n",
      "Epoch 25: train loss: 0.10152800381183624\n",
      "Epoch 25: train loss: 0.112187460064888\n",
      "Epoch 25: train loss: 0.22230768203735352\n",
      "Epoch 25: train loss: 0.37306588888168335\n",
      "Epoch 25: train loss: 0.13858182728290558\n",
      "Epoch 25: train loss: 0.30105623602867126\n",
      "Epoch 25: train loss: 0.0856354832649231\n",
      "Epoch 25: train loss: 0.29190152883529663\n",
      "Epoch 25: train loss: 0.33478841185569763\n",
      "Epoch 25: train loss: 0.3083946108818054\n",
      "Epoch 25: train loss: 0.2741880416870117\n",
      "Epoch 26: train loss: 0.41453245282173157\n",
      "Epoch 26: train loss: 0.07380671054124832\n",
      "Epoch 26: train loss: 0.26505088806152344\n",
      "Epoch 26: train loss: 0.39284276962280273\n",
      "Epoch 26: train loss: 0.1382199227809906\n",
      "Epoch 26: train loss: 0.22895202040672302\n",
      "Epoch 26: train loss: 0.292270690202713\n",
      "Epoch 26: train loss: 0.26832160353660583\n",
      "Epoch 26: train loss: 0.42955583333969116\n",
      "Epoch 26: train loss: 0.30390289425849915\n",
      "Epoch 26: train loss: 0.24837619066238403\n",
      "Epoch 26: train loss: 0.1532808095216751\n",
      "Epoch 26: train loss: 0.1706208437681198\n",
      "Epoch 26: train loss: 0.312673419713974\n",
      "Epoch 26: train loss: 0.5993195176124573\n",
      "Epoch 26: train loss: 0.1791442483663559\n",
      "Epoch 26: train loss: 0.18335479497909546\n",
      "Epoch 26: train loss: 0.21033507585525513\n",
      "Epoch 26: train loss: 0.331836462020874\n",
      "Epoch 26: train loss: 0.23693610727787018\n",
      "Epoch 26: train loss: 0.21830326318740845\n",
      "Epoch 26: train loss: 0.1769213080406189\n",
      "Epoch 26: train loss: 0.5124305486679077\n",
      "Epoch 26: train loss: 0.12144939601421356\n",
      "Epoch 26: train loss: 0.44368958473205566\n",
      "Epoch 26: train loss: 0.3554999828338623\n",
      "Epoch 26: train loss: 0.19499793648719788\n",
      "Epoch 26: train loss: 0.08653973042964935\n",
      "Epoch 26: train loss: 0.2062821388244629\n",
      "Epoch 26: train loss: 0.32138165831565857\n",
      "Epoch 26: train loss: 0.4402587115764618\n",
      "Epoch 26: train loss: 0.3562765121459961\n",
      "Epoch 26: train loss: 0.14014887809753418\n",
      "Epoch 26: train loss: 0.1752457618713379\n",
      "Epoch 26: train loss: 0.17742671072483063\n",
      "Epoch 26: train loss: 0.22063769400119781\n",
      "Epoch 26: train loss: 0.2731840908527374\n",
      "Epoch 26: train loss: 0.0901666060090065\n",
      "Epoch 26: train loss: 0.10930363833904266\n",
      "Epoch 26: train loss: 0.31436634063720703\n",
      "Epoch 26: train loss: 0.1835411787033081\n",
      "Epoch 26: train loss: 0.18489563465118408\n",
      "Epoch 26: train loss: 0.14592847228050232\n",
      "Epoch 26: train loss: 0.22412264347076416\n",
      "Epoch 27: train loss: 0.23180638253688812\n",
      "Epoch 27: train loss: 0.04087119922041893\n",
      "Epoch 27: train loss: 0.15897366404533386\n",
      "Epoch 27: train loss: 0.1062224954366684\n",
      "Epoch 27: train loss: 0.2650739848613739\n",
      "Epoch 27: train loss: 0.19151030480861664\n",
      "Epoch 27: train loss: 0.4588698446750641\n",
      "Epoch 27: train loss: 0.24994422495365143\n",
      "Epoch 27: train loss: 0.4093290865421295\n",
      "Epoch 27: train loss: 0.47421762347221375\n",
      "Epoch 27: train loss: 0.07567610591650009\n",
      "Epoch 27: train loss: 0.18753153085708618\n",
      "Epoch 27: train loss: 0.18118515610694885\n",
      "Epoch 27: train loss: 0.36629435420036316\n",
      "Epoch 27: train loss: 0.4425400197505951\n",
      "Epoch 27: train loss: 0.19774900376796722\n",
      "Epoch 27: train loss: 0.13866697251796722\n",
      "Epoch 27: train loss: 0.14119502902030945\n",
      "Epoch 27: train loss: 0.21552813053131104\n",
      "Epoch 27: train loss: 0.29203957319259644\n",
      "Epoch 27: train loss: 0.23855677247047424\n",
      "Epoch 27: train loss: 0.4204506576061249\n",
      "Epoch 27: train loss: 0.20406389236450195\n",
      "Epoch 27: train loss: 0.25829628109931946\n",
      "Epoch 27: train loss: 0.2035839706659317\n",
      "Epoch 27: train loss: 0.358304500579834\n",
      "Epoch 27: train loss: 0.15537306666374207\n",
      "Epoch 27: train loss: 0.3514593541622162\n",
      "Epoch 27: train loss: 0.21007095277309418\n",
      "Epoch 27: train loss: 0.24652692675590515\n",
      "Epoch 27: train loss: 0.21466386318206787\n",
      "Epoch 27: train loss: 0.22439147531986237\n",
      "Epoch 27: train loss: 0.25493910908699036\n",
      "Epoch 27: train loss: 0.37669745087623596\n",
      "Epoch 27: train loss: 0.23457829654216766\n",
      "Epoch 27: train loss: 0.2174726128578186\n",
      "Epoch 27: train loss: 0.34267911314964294\n",
      "Epoch 27: train loss: 0.16822068393230438\n",
      "Epoch 27: train loss: 0.27153557538986206\n",
      "Epoch 27: train loss: 0.34347665309906006\n",
      "Epoch 27: train loss: 0.4591906666755676\n",
      "Epoch 27: train loss: 0.2043672651052475\n",
      "Epoch 27: train loss: 0.21908538043498993\n",
      "Epoch 27: train loss: 0.12179109454154968\n",
      "Epoch 28: train loss: 0.20679226517677307\n",
      "Epoch 28: train loss: 0.1250375509262085\n",
      "Epoch 28: train loss: 0.16921108961105347\n",
      "Epoch 28: train loss: 0.30061477422714233\n",
      "Epoch 28: train loss: 0.2801160514354706\n",
      "Epoch 28: train loss: 0.26913151144981384\n",
      "Epoch 28: train loss: 0.1796838641166687\n",
      "Epoch 28: train loss: 0.1532049924135208\n",
      "Epoch 28: train loss: 0.23815591633319855\n",
      "Epoch 28: train loss: 0.3404422998428345\n",
      "Epoch 28: train loss: 0.20162853598594666\n",
      "Epoch 28: train loss: 0.26424717903137207\n",
      "Epoch 28: train loss: 0.33049827814102173\n",
      "Epoch 28: train loss: 0.4808955788612366\n",
      "Epoch 28: train loss: 0.4530831277370453\n",
      "Epoch 28: train loss: 0.43562471866607666\n",
      "Epoch 28: train loss: 0.16538751125335693\n",
      "Epoch 28: train loss: 0.2212500125169754\n",
      "Epoch 28: train loss: 0.5216847658157349\n",
      "Epoch 28: train loss: 0.21563230454921722\n",
      "Epoch 28: train loss: 0.15729160606861115\n",
      "Epoch 28: train loss: 0.1869092881679535\n",
      "Epoch 28: train loss: 0.2849939167499542\n",
      "Epoch 28: train loss: 0.09697981923818588\n",
      "Epoch 28: train loss: 0.2564154267311096\n",
      "Epoch 28: train loss: 0.08725741505622864\n",
      "Epoch 28: train loss: 0.2579190731048584\n",
      "Epoch 28: train loss: 0.15139979124069214\n",
      "Epoch 28: train loss: 0.23077256977558136\n",
      "Epoch 28: train loss: 0.5883999466896057\n",
      "Epoch 28: train loss: 0.330863893032074\n",
      "Epoch 28: train loss: 0.15684978663921356\n",
      "Epoch 28: train loss: 0.2555055618286133\n",
      "Epoch 28: train loss: 0.18872110545635223\n",
      "Epoch 28: train loss: 0.3340417444705963\n",
      "Epoch 28: train loss: 0.18205894529819489\n",
      "Epoch 28: train loss: 0.10369442403316498\n",
      "Epoch 28: train loss: 0.17261503636837006\n",
      "Epoch 28: train loss: 0.32966408133506775\n",
      "Epoch 28: train loss: 0.1208912804722786\n",
      "Epoch 28: train loss: 0.20589976012706757\n",
      "Epoch 28: train loss: 0.24429060518741608\n",
      "Epoch 28: train loss: 0.38527771830558777\n",
      "Epoch 28: train loss: 0.30743348598480225\n",
      "Epoch 29: train loss: 0.3616437017917633\n",
      "Epoch 29: train loss: 0.2599242925643921\n",
      "Epoch 29: train loss: 0.38719961047172546\n",
      "Epoch 29: train loss: 0.07909978926181793\n",
      "Epoch 29: train loss: 0.27494174242019653\n",
      "Epoch 29: train loss: 0.3087640106678009\n",
      "Epoch 29: train loss: 0.3230477571487427\n",
      "Epoch 29: train loss: 0.25193729996681213\n",
      "Epoch 29: train loss: 0.22628569602966309\n",
      "Epoch 29: train loss: 0.1684863418340683\n",
      "Epoch 29: train loss: 0.18630391359329224\n",
      "Epoch 29: train loss: 0.22981423139572144\n",
      "Epoch 29: train loss: 0.1595180183649063\n",
      "Epoch 29: train loss: 0.3733242452144623\n",
      "Epoch 29: train loss: 0.2350274622440338\n",
      "Epoch 29: train loss: 0.28894343972206116\n",
      "Epoch 29: train loss: 0.2385748028755188\n",
      "Epoch 29: train loss: 0.0831543579697609\n",
      "Epoch 29: train loss: 0.24301275610923767\n",
      "Epoch 29: train loss: 0.23234625160694122\n",
      "Epoch 29: train loss: 0.252032995223999\n",
      "Epoch 29: train loss: 0.14222478866577148\n",
      "Epoch 29: train loss: 0.1205633357167244\n",
      "Epoch 29: train loss: 0.043600961565971375\n",
      "Epoch 29: train loss: 0.1768212914466858\n",
      "Epoch 29: train loss: 0.2203913778066635\n",
      "Epoch 29: train loss: 0.18769462406635284\n",
      "Epoch 29: train loss: 0.6349585056304932\n",
      "Epoch 29: train loss: 0.27648892998695374\n",
      "Epoch 29: train loss: 0.12476131319999695\n",
      "Epoch 29: train loss: 0.33230891823768616\n",
      "Epoch 29: train loss: 0.1155032366514206\n",
      "Epoch 29: train loss: 0.2626703679561615\n",
      "Epoch 29: train loss: 0.3113704025745392\n",
      "Epoch 29: train loss: 0.37960997223854065\n",
      "Epoch 29: train loss: 0.5681253671646118\n",
      "Epoch 29: train loss: 0.35089346766471863\n",
      "Epoch 29: train loss: 0.14444679021835327\n",
      "Epoch 29: train loss: 0.1517181545495987\n",
      "Epoch 29: train loss: 0.24019476771354675\n",
      "Epoch 29: train loss: 0.22424553334712982\n",
      "Epoch 29: train loss: 0.40016305446624756\n",
      "Epoch 29: train loss: 0.38844528794288635\n",
      "Epoch 29: train loss: 0.25853386521339417\n",
      "Epoch 30: train loss: 0.25084951519966125\n",
      "Epoch 30: train loss: 0.32300055027008057\n",
      "Epoch 30: train loss: 0.15196242928504944\n",
      "Epoch 30: train loss: 0.4702882170677185\n",
      "Epoch 30: train loss: 0.2401968240737915\n",
      "Epoch 30: train loss: 0.24254514276981354\n",
      "Epoch 30: train loss: 0.28181442618370056\n",
      "Epoch 30: train loss: 0.2761147916316986\n",
      "Epoch 30: train loss: 0.24533610045909882\n",
      "Epoch 30: train loss: 0.15226995944976807\n",
      "Epoch 30: train loss: 0.09472635388374329\n",
      "Epoch 30: train loss: 0.3595518171787262\n",
      "Epoch 30: train loss: 0.334100604057312\n",
      "Epoch 30: train loss: 0.2847011685371399\n",
      "Epoch 30: train loss: 0.24794220924377441\n",
      "Epoch 30: train loss: 0.2192886620759964\n",
      "Epoch 30: train loss: 0.3107113540172577\n",
      "Epoch 30: train loss: 0.16736510396003723\n",
      "Epoch 30: train loss: 0.21311970055103302\n",
      "Epoch 30: train loss: 0.11145758628845215\n",
      "Epoch 30: train loss: 0.44404417276382446\n",
      "Epoch 30: train loss: 0.27095967531204224\n",
      "Epoch 30: train loss: 0.49228647351264954\n",
      "Epoch 30: train loss: 0.23712177574634552\n",
      "Epoch 30: train loss: 0.42588329315185547\n",
      "Epoch 30: train loss: 0.2771624028682709\n",
      "Epoch 30: train loss: 0.21532806754112244\n",
      "Epoch 30: train loss: 0.20205560326576233\n",
      "Epoch 30: train loss: 0.1481054574251175\n",
      "Epoch 30: train loss: 0.31971797347068787\n",
      "Epoch 30: train loss: 0.15477854013442993\n",
      "Epoch 30: train loss: 0.1878826916217804\n",
      "Epoch 30: train loss: 0.3102511763572693\n",
      "Epoch 30: train loss: 0.10386713594198227\n",
      "Epoch 30: train loss: 0.18567520380020142\n",
      "Epoch 30: train loss: 0.0966612845659256\n",
      "Epoch 30: train loss: 0.1357123851776123\n",
      "Epoch 30: train loss: 0.13533000648021698\n",
      "Epoch 30: train loss: 0.29051581025123596\n",
      "Epoch 30: train loss: 0.28025346994400024\n",
      "Epoch 30: train loss: 0.19105404615402222\n",
      "Epoch 30: train loss: 0.375948429107666\n",
      "Epoch 30: train loss: 0.27769339084625244\n",
      "Epoch 30: train loss: 0.5828274488449097\n",
      "Epoch 31: train loss: 0.7499297261238098\n",
      "Epoch 31: train loss: 0.32637903094291687\n",
      "Epoch 31: train loss: 0.2635572552680969\n",
      "Epoch 31: train loss: 0.18070557713508606\n",
      "Epoch 31: train loss: 0.4533989429473877\n",
      "Epoch 31: train loss: 0.32517388463020325\n",
      "Epoch 31: train loss: 0.2651253044605255\n",
      "Epoch 31: train loss: 0.26644936203956604\n",
      "Epoch 31: train loss: 0.3258323669433594\n",
      "Epoch 31: train loss: 0.1645321398973465\n",
      "Epoch 31: train loss: 0.25880709290504456\n",
      "Epoch 31: train loss: 0.2491590529680252\n",
      "Epoch 31: train loss: 0.2750478982925415\n",
      "Epoch 31: train loss: 0.18420670926570892\n",
      "Epoch 31: train loss: 0.3092096447944641\n",
      "Epoch 31: train loss: 0.33940955996513367\n",
      "Epoch 31: train loss: 0.1650976538658142\n",
      "Epoch 31: train loss: 0.18452651798725128\n",
      "Epoch 31: train loss: 0.4647829830646515\n",
      "Epoch 31: train loss: 0.22694817185401917\n",
      "Epoch 31: train loss: 0.15482893586158752\n",
      "Epoch 31: train loss: 0.13857024908065796\n",
      "Epoch 31: train loss: 0.15980936586856842\n",
      "Epoch 31: train loss: 0.14334255456924438\n",
      "Epoch 31: train loss: 0.0967431366443634\n",
      "Epoch 31: train loss: 0.22556166350841522\n",
      "Epoch 31: train loss: 0.25852417945861816\n",
      "Epoch 31: train loss: 0.20119747519493103\n",
      "Epoch 31: train loss: 0.31906455755233765\n",
      "Epoch 31: train loss: 0.1719401329755783\n",
      "Epoch 31: train loss: 0.22212137281894684\n",
      "Epoch 31: train loss: 0.12315651029348373\n",
      "Epoch 31: train loss: 0.3685179352760315\n",
      "Epoch 31: train loss: 0.2552013099193573\n",
      "Epoch 31: train loss: 0.18687556684017181\n",
      "Epoch 31: train loss: 0.35268405079841614\n",
      "Epoch 31: train loss: 0.07290823757648468\n",
      "Epoch 31: train loss: 0.22137154638767242\n",
      "Epoch 31: train loss: 0.340180903673172\n",
      "Epoch 31: train loss: 0.4887898564338684\n",
      "Epoch 31: train loss: 0.33423635363578796\n",
      "Epoch 31: train loss: 0.19545437395572662\n",
      "Epoch 31: train loss: 0.15598469972610474\n",
      "Epoch 31: train loss: 0.2588684856891632\n",
      "Epoch 32: train loss: 0.18023796379566193\n",
      "Epoch 32: train loss: 0.417248398065567\n",
      "Epoch 32: train loss: 0.41294559836387634\n",
      "Epoch 32: train loss: 0.27459192276000977\n",
      "Epoch 32: train loss: 0.26286375522613525\n",
      "Epoch 32: train loss: 0.1746341586112976\n",
      "Epoch 32: train loss: 0.37824225425720215\n",
      "Epoch 32: train loss: 0.31570279598236084\n",
      "Epoch 32: train loss: 0.22328723967075348\n",
      "Epoch 32: train loss: 0.3094993531703949\n",
      "Epoch 32: train loss: 0.27972015738487244\n",
      "Epoch 32: train loss: 0.371835857629776\n",
      "Epoch 32: train loss: 0.10641725361347198\n",
      "Epoch 32: train loss: 0.19618964195251465\n",
      "Epoch 32: train loss: 0.2816210389137268\n",
      "Epoch 32: train loss: 0.41946834325790405\n",
      "Epoch 32: train loss: 0.3820696771144867\n",
      "Epoch 32: train loss: 0.15289093554019928\n",
      "Epoch 32: train loss: 0.19855162501335144\n",
      "Epoch 32: train loss: 0.47249355912208557\n",
      "Epoch 32: train loss: 0.32451850175857544\n",
      "Epoch 32: train loss: 0.2557954490184784\n",
      "Epoch 32: train loss: 0.25241929292678833\n",
      "Epoch 32: train loss: 0.1965559422969818\n",
      "Epoch 32: train loss: 0.2526921331882477\n",
      "Epoch 32: train loss: 0.14287444949150085\n",
      "Epoch 32: train loss: 0.45553871989250183\n",
      "Epoch 32: train loss: 0.28495311737060547\n",
      "Epoch 32: train loss: 0.12484004348516464\n",
      "Epoch 32: train loss: 0.19478246569633484\n",
      "Epoch 32: train loss: 0.3377121090888977\n",
      "Epoch 32: train loss: 0.20943397283554077\n",
      "Epoch 32: train loss: 0.09229270368814468\n",
      "Epoch 32: train loss: 0.21917349100112915\n",
      "Epoch 32: train loss: 0.14658020436763763\n",
      "Epoch 32: train loss: 0.25883108377456665\n",
      "Epoch 32: train loss: 0.20400811731815338\n",
      "Epoch 32: train loss: 0.134332075715065\n",
      "Epoch 32: train loss: 0.18753832578659058\n",
      "Epoch 32: train loss: 0.32688480615615845\n",
      "Epoch 32: train loss: 0.09103601425886154\n",
      "Epoch 32: train loss: 0.19309361279010773\n",
      "Epoch 32: train loss: 0.0656721442937851\n",
      "Epoch 32: train loss: 0.2768462002277374\n",
      "Epoch 33: train loss: 0.2534942030906677\n",
      "Epoch 33: train loss: 0.37851861119270325\n",
      "Epoch 33: train loss: 0.12450966984033585\n",
      "Epoch 33: train loss: 0.0652712732553482\n",
      "Epoch 33: train loss: 0.3228551149368286\n",
      "Epoch 33: train loss: 0.11874464154243469\n",
      "Epoch 33: train loss: 0.3661472201347351\n",
      "Epoch 33: train loss: 0.12473475188016891\n",
      "Epoch 33: train loss: 0.3093641698360443\n",
      "Epoch 33: train loss: 0.17823933064937592\n",
      "Epoch 33: train loss: 0.49718111753463745\n",
      "Epoch 33: train loss: 0.12568490207195282\n",
      "Epoch 33: train loss: 0.20281490683555603\n",
      "Epoch 33: train loss: 0.3142060339450836\n",
      "Epoch 33: train loss: 0.0973595380783081\n",
      "Epoch 33: train loss: 0.36038562655448914\n",
      "Epoch 33: train loss: 0.25354745984077454\n",
      "Epoch 33: train loss: 0.2269822657108307\n",
      "Epoch 33: train loss: 0.3207125961780548\n",
      "Epoch 33: train loss: 0.38756096363067627\n",
      "Epoch 33: train loss: 0.22863450646400452\n",
      "Epoch 33: train loss: 0.37361589074134827\n",
      "Epoch 33: train loss: 0.24455736577510834\n",
      "Epoch 33: train loss: 0.17843253910541534\n",
      "Epoch 33: train loss: 0.3533674478530884\n",
      "Epoch 33: train loss: 0.2676001489162445\n",
      "Epoch 33: train loss: 0.26388266682624817\n",
      "Epoch 33: train loss: 0.3138069808483124\n",
      "Epoch 33: train loss: 0.15908004343509674\n",
      "Epoch 33: train loss: 0.26625382900238037\n",
      "Epoch 33: train loss: 0.23728737235069275\n",
      "Epoch 33: train loss: 0.28606727719306946\n",
      "Epoch 33: train loss: 0.2290371060371399\n",
      "Epoch 33: train loss: 0.1947116106748581\n",
      "Epoch 33: train loss: 0.2035127878189087\n",
      "Epoch 33: train loss: 0.34150874614715576\n",
      "Epoch 33: train loss: 0.4459293484687805\n",
      "Epoch 33: train loss: 0.05800391733646393\n",
      "Epoch 33: train loss: 0.1928558051586151\n",
      "Epoch 33: train loss: 0.16446906328201294\n",
      "Epoch 33: train loss: 0.1838981807231903\n",
      "Epoch 33: train loss: 0.28231364488601685\n",
      "Epoch 33: train loss: 0.42450541257858276\n",
      "Epoch 33: train loss: 0.1838337928056717\n",
      "Epoch 34: train loss: 0.21724742650985718\n",
      "Epoch 34: train loss: 0.2532042860984802\n",
      "Epoch 34: train loss: 0.34326574206352234\n",
      "Epoch 34: train loss: 0.24645310640335083\n",
      "Epoch 34: train loss: 0.1368892639875412\n",
      "Epoch 34: train loss: 0.3124081492424011\n",
      "Epoch 34: train loss: 0.1852891594171524\n",
      "Epoch 34: train loss: 0.6201992034912109\n",
      "Epoch 34: train loss: 0.24704575538635254\n",
      "Epoch 34: train loss: 0.1525774896144867\n",
      "Epoch 34: train loss: 0.44428005814552307\n",
      "Epoch 34: train loss: 0.3029572367668152\n",
      "Epoch 34: train loss: 0.20559565722942352\n",
      "Epoch 34: train loss: 0.13152995705604553\n",
      "Epoch 34: train loss: 0.13851359486579895\n",
      "Epoch 34: train loss: 0.23153889179229736\n",
      "Epoch 34: train loss: 0.21112751960754395\n",
      "Epoch 34: train loss: 0.2622627317905426\n",
      "Epoch 34: train loss: 0.117578886449337\n",
      "Epoch 34: train loss: 0.13066568970680237\n",
      "Epoch 34: train loss: 0.07553784549236298\n",
      "Epoch 34: train loss: 0.11828536540269852\n",
      "Epoch 34: train loss: 0.32093921303749084\n",
      "Epoch 34: train loss: 0.20974360406398773\n",
      "Epoch 34: train loss: 0.36532557010650635\n",
      "Epoch 34: train loss: 0.1921733021736145\n",
      "Epoch 34: train loss: 0.5257957577705383\n",
      "Epoch 34: train loss: 0.3226877748966217\n",
      "Epoch 34: train loss: 0.6577718257904053\n",
      "Epoch 34: train loss: 0.3393658995628357\n",
      "Epoch 34: train loss: 0.18857723474502563\n",
      "Epoch 34: train loss: 0.3108671009540558\n",
      "Epoch 34: train loss: 0.2501680254936218\n",
      "Epoch 34: train loss: 0.1925211250782013\n",
      "Epoch 34: train loss: 0.2806345224380493\n",
      "Epoch 34: train loss: 0.26162824034690857\n",
      "Epoch 34: train loss: 0.20391874015331268\n",
      "Epoch 34: train loss: 0.2054029256105423\n",
      "Epoch 34: train loss: 0.1483132392168045\n",
      "Epoch 34: train loss: 0.24531899392604828\n",
      "Epoch 34: train loss: 0.17141801118850708\n",
      "Epoch 34: train loss: 0.21805550158023834\n",
      "Epoch 34: train loss: 0.32177698612213135\n",
      "Epoch 34: train loss: 0.4322083592414856\n",
      "Epoch 35: train loss: 0.44744065403938293\n",
      "Epoch 35: train loss: 0.4340977966785431\n",
      "Epoch 35: train loss: 0.1458532214164734\n",
      "Epoch 35: train loss: 0.35451018810272217\n",
      "Epoch 35: train loss: 0.2031335085630417\n",
      "Epoch 35: train loss: 0.25921106338500977\n",
      "Epoch 35: train loss: 0.19195562601089478\n",
      "Epoch 35: train loss: 0.20821917057037354\n",
      "Epoch 35: train loss: 0.2313905507326126\n",
      "Epoch 35: train loss: 0.17743660509586334\n",
      "Epoch 35: train loss: 0.18936702609062195\n",
      "Epoch 35: train loss: 0.25015988945961\n",
      "Epoch 35: train loss: 0.2939746379852295\n",
      "Epoch 35: train loss: 0.11079593002796173\n",
      "Epoch 35: train loss: 0.1405973732471466\n",
      "Epoch 35: train loss: 0.2994541823863983\n",
      "Epoch 35: train loss: 0.3467109501361847\n",
      "Epoch 35: train loss: 0.27865737676620483\n",
      "Epoch 35: train loss: 0.04939443618059158\n",
      "Epoch 35: train loss: 0.23523978888988495\n",
      "Epoch 35: train loss: 0.48145532608032227\n",
      "Epoch 35: train loss: 0.36249077320098877\n",
      "Epoch 35: train loss: 0.26925110816955566\n",
      "Epoch 35: train loss: 0.38382798433303833\n",
      "Epoch 35: train loss: 0.21814531087875366\n",
      "Epoch 35: train loss: 0.20303982496261597\n",
      "Epoch 35: train loss: 0.21318328380584717\n",
      "Epoch 35: train loss: 0.13547135889530182\n",
      "Epoch 35: train loss: 0.14862056076526642\n",
      "Epoch 35: train loss: 0.34759095311164856\n",
      "Epoch 35: train loss: 0.28017958998680115\n",
      "Epoch 35: train loss: 0.23446546494960785\n",
      "Epoch 35: train loss: 0.2064667046070099\n",
      "Epoch 35: train loss: 0.37864845991134644\n",
      "Epoch 35: train loss: 0.15293096005916595\n",
      "Epoch 35: train loss: 0.1473647505044937\n",
      "Epoch 35: train loss: 0.2663156986236572\n",
      "Epoch 35: train loss: 0.3051586151123047\n",
      "Epoch 35: train loss: 0.4407842457294464\n",
      "Epoch 35: train loss: 0.18616580963134766\n",
      "Epoch 35: train loss: 0.22454902529716492\n",
      "Epoch 35: train loss: 0.13621839880943298\n",
      "Epoch 35: train loss: 0.30617591738700867\n",
      "Epoch 35: train loss: 0.2006232887506485\n",
      "Epoch 36: train loss: 0.5536301136016846\n",
      "Epoch 36: train loss: 0.32704880833625793\n",
      "Epoch 36: train loss: 0.18563544750213623\n",
      "Epoch 36: train loss: 0.17770063877105713\n",
      "Epoch 36: train loss: 0.1525498628616333\n",
      "Epoch 36: train loss: 0.23375818133354187\n",
      "Epoch 36: train loss: 0.24350084364414215\n",
      "Epoch 36: train loss: 0.20029109716415405\n",
      "Epoch 36: train loss: 0.26190078258514404\n",
      "Epoch 36: train loss: 0.15566585958003998\n",
      "Epoch 36: train loss: 0.47741082310676575\n",
      "Epoch 36: train loss: 0.2906564772129059\n",
      "Epoch 36: train loss: 0.1939876228570938\n",
      "Epoch 36: train loss: 0.23689289391040802\n",
      "Epoch 36: train loss: 0.3834785521030426\n",
      "Epoch 36: train loss: 0.20853714644908905\n",
      "Epoch 36: train loss: 0.46428534388542175\n",
      "Epoch 36: train loss: 0.2707465887069702\n",
      "Epoch 36: train loss: 0.15249648690223694\n",
      "Epoch 36: train loss: 0.20538461208343506\n",
      "Epoch 36: train loss: 0.266463965177536\n",
      "Epoch 36: train loss: 0.4315494894981384\n",
      "Epoch 36: train loss: 0.2643812894821167\n",
      "Epoch 36: train loss: 0.2700905501842499\n",
      "Epoch 36: train loss: 0.2035246193408966\n",
      "Epoch 36: train loss: 0.1633780151605606\n",
      "Epoch 36: train loss: 0.1853552907705307\n",
      "Epoch 36: train loss: 0.25772175192832947\n",
      "Epoch 36: train loss: 0.38154640793800354\n",
      "Epoch 36: train loss: 0.2503780424594879\n",
      "Epoch 36: train loss: 0.12254942208528519\n",
      "Epoch 36: train loss: 0.11534389853477478\n",
      "Epoch 36: train loss: 0.1809150129556656\n",
      "Epoch 36: train loss: 0.2783423960208893\n",
      "Epoch 36: train loss: 0.2749325633049011\n",
      "Epoch 36: train loss: 0.2964881360530853\n",
      "Epoch 36: train loss: 0.2161749005317688\n",
      "Epoch 36: train loss: 0.08365587890148163\n",
      "Epoch 36: train loss: 0.3595235347747803\n",
      "Epoch 36: train loss: 0.09210295975208282\n",
      "Epoch 36: train loss: 0.21538038551807404\n",
      "Epoch 36: train loss: 0.22430360317230225\n",
      "Epoch 36: train loss: 0.13641878962516785\n",
      "Epoch 36: train loss: 0.3911922872066498\n",
      "Epoch 37: train loss: 0.27412766218185425\n",
      "Epoch 37: train loss: 0.274152934551239\n",
      "Epoch 37: train loss: 0.300755113363266\n",
      "Epoch 37: train loss: 0.07056926935911179\n",
      "Epoch 37: train loss: 0.2840569019317627\n",
      "Epoch 37: train loss: 0.25573891401290894\n",
      "Epoch 37: train loss: 0.2619172930717468\n",
      "Epoch 37: train loss: 0.17781178653240204\n",
      "Epoch 37: train loss: 0.15131480991840363\n",
      "Epoch 37: train loss: 0.3201249837875366\n",
      "Epoch 37: train loss: 0.3957926034927368\n",
      "Epoch 37: train loss: 0.18728986382484436\n",
      "Epoch 37: train loss: 0.4452851116657257\n",
      "Epoch 37: train loss: 0.20968171954154968\n",
      "Epoch 37: train loss: 0.218849316239357\n",
      "Epoch 37: train loss: 0.49062052369117737\n",
      "Epoch 37: train loss: 0.3562350571155548\n",
      "Epoch 37: train loss: 0.1962781399488449\n",
      "Epoch 37: train loss: 0.17406286299228668\n",
      "Epoch 37: train loss: 0.32210999727249146\n",
      "Epoch 37: train loss: 0.20810173451900482\n",
      "Epoch 37: train loss: 0.19191457331180573\n",
      "Epoch 37: train loss: 0.1616913080215454\n",
      "Epoch 37: train loss: 0.2098875492811203\n",
      "Epoch 37: train loss: 0.5174864530563354\n",
      "Epoch 37: train loss: 0.29619404673576355\n",
      "Epoch 37: train loss: 0.22209757566452026\n",
      "Epoch 37: train loss: 0.2995831072330475\n",
      "Epoch 37: train loss: 0.07833853363990784\n",
      "Epoch 37: train loss: 0.253246545791626\n",
      "Epoch 37: train loss: 0.17198848724365234\n",
      "Epoch 37: train loss: 0.23836244642734528\n",
      "Epoch 37: train loss: 0.30185186862945557\n",
      "Epoch 37: train loss: 0.06565888226032257\n",
      "Epoch 37: train loss: 0.31229689717292786\n",
      "Epoch 37: train loss: 0.2911764979362488\n",
      "Epoch 37: train loss: 0.2249499261379242\n",
      "Epoch 37: train loss: 0.06848009675741196\n",
      "Epoch 37: train loss: 0.3693869113922119\n",
      "Epoch 37: train loss: 0.28578561544418335\n",
      "Epoch 37: train loss: 0.401792973279953\n",
      "Epoch 37: train loss: 0.14098282158374786\n",
      "Epoch 37: train loss: 0.23126767575740814\n",
      "Epoch 37: train loss: 0.14349345862865448\n",
      "Epoch 38: train loss: 0.31696125864982605\n",
      "Epoch 38: train loss: 0.1903536021709442\n",
      "Epoch 38: train loss: 0.5232254266738892\n",
      "Epoch 38: train loss: 0.1611676663160324\n",
      "Epoch 38: train loss: 0.0542558878660202\n",
      "Epoch 38: train loss: 0.36987072229385376\n",
      "Epoch 38: train loss: 0.11406353116035461\n",
      "Epoch 38: train loss: 0.4592200219631195\n",
      "Epoch 38: train loss: 0.15070515871047974\n",
      "Epoch 38: train loss: 0.23904399573802948\n",
      "Epoch 38: train loss: 0.17766615748405457\n",
      "Epoch 38: train loss: 0.2495204210281372\n",
      "Epoch 38: train loss: 0.36070722341537476\n",
      "Epoch 38: train loss: 0.18250203132629395\n",
      "Epoch 38: train loss: 0.31621819734573364\n",
      "Epoch 38: train loss: 0.2489650696516037\n",
      "Epoch 38: train loss: 0.1265030801296234\n",
      "Epoch 38: train loss: 0.3452063500881195\n",
      "Epoch 38: train loss: 0.15522105991840363\n",
      "Epoch 38: train loss: 0.15732651948928833\n",
      "Epoch 38: train loss: 0.16968408226966858\n",
      "Epoch 38: train loss: 0.29988354444503784\n",
      "Epoch 38: train loss: 0.16513726115226746\n",
      "Epoch 38: train loss: 0.2418653815984726\n",
      "Epoch 38: train loss: 0.22321349382400513\n",
      "Epoch 38: train loss: 0.15543018281459808\n",
      "Epoch 38: train loss: 0.10082196444272995\n",
      "Epoch 38: train loss: 0.2026059329509735\n",
      "Epoch 38: train loss: 0.10459966957569122\n",
      "Epoch 38: train loss: 0.3561282157897949\n",
      "Epoch 38: train loss: 0.35424935817718506\n",
      "Epoch 38: train loss: 0.36119192838668823\n",
      "Epoch 38: train loss: 0.37391331791877747\n",
      "Epoch 38: train loss: 0.48801562190055847\n",
      "Epoch 38: train loss: 0.21912653744220734\n",
      "Epoch 38: train loss: 0.2981584072113037\n",
      "Epoch 38: train loss: 0.2116791009902954\n",
      "Epoch 38: train loss: 0.49972227215766907\n",
      "Epoch 38: train loss: 0.3262946605682373\n",
      "Epoch 38: train loss: 0.12632222473621368\n",
      "Epoch 38: train loss: 0.21976448595523834\n",
      "Epoch 38: train loss: 0.21443510055541992\n",
      "Epoch 38: train loss: 0.3070136606693268\n",
      "Epoch 38: train loss: 0.16572386026382446\n",
      "Epoch 39: train loss: 0.408557653427124\n",
      "Epoch 39: train loss: 0.1708310842514038\n",
      "Epoch 39: train loss: 0.20761539041996002\n",
      "Epoch 39: train loss: 0.3161323070526123\n",
      "Epoch 39: train loss: 0.24063166975975037\n",
      "Epoch 39: train loss: 0.27779915928840637\n",
      "Epoch 39: train loss: 0.2017751932144165\n",
      "Epoch 39: train loss: 0.23320037126541138\n",
      "Epoch 39: train loss: 0.37796831130981445\n",
      "Epoch 39: train loss: 0.19277323782444\n",
      "Epoch 39: train loss: 0.31454312801361084\n",
      "Epoch 39: train loss: 0.10315379500389099\n",
      "Epoch 39: train loss: 0.4962385296821594\n",
      "Epoch 39: train loss: 0.20826807618141174\n",
      "Epoch 39: train loss: 0.2911742329597473\n",
      "Epoch 39: train loss: 0.46156781911849976\n",
      "Epoch 39: train loss: 0.26225537061691284\n",
      "Epoch 39: train loss: 0.2076990157365799\n",
      "Epoch 39: train loss: 0.31557321548461914\n",
      "Epoch 39: train loss: 0.14181667566299438\n",
      "Epoch 39: train loss: 0.32955875992774963\n",
      "Epoch 39: train loss: 0.23608723282814026\n",
      "Epoch 39: train loss: 0.23385170102119446\n",
      "Epoch 39: train loss: 0.3894960880279541\n",
      "Epoch 39: train loss: 0.11163985729217529\n",
      "Epoch 39: train loss: 0.25225532054901123\n",
      "Epoch 39: train loss: 0.29674986004829407\n",
      "Epoch 39: train loss: 0.13687320053577423\n",
      "Epoch 39: train loss: 0.20649218559265137\n",
      "Epoch 39: train loss: 0.1760849803686142\n",
      "Epoch 39: train loss: 0.19070406258106232\n",
      "Epoch 39: train loss: 0.29598739743232727\n",
      "Epoch 39: train loss: 0.18902726471424103\n",
      "Epoch 39: train loss: 0.3108954131603241\n",
      "Epoch 39: train loss: 0.19496648013591766\n",
      "Epoch 39: train loss: 0.1407167911529541\n",
      "Epoch 39: train loss: 0.3358611464500427\n",
      "Epoch 39: train loss: 0.3157373368740082\n",
      "Epoch 39: train loss: 0.24500298500061035\n",
      "Epoch 39: train loss: 0.3206917941570282\n",
      "Epoch 39: train loss: 0.16720648109912872\n",
      "Epoch 39: train loss: 0.24885936081409454\n",
      "Epoch 39: train loss: 0.12281020730733871\n",
      "Epoch 39: train loss: 0.2094426453113556\n",
      "Epoch 40: train loss: 0.11320912837982178\n",
      "Epoch 40: train loss: 0.05642588064074516\n",
      "Epoch 40: train loss: 0.2746846377849579\n",
      "Epoch 40: train loss: 0.2170781046152115\n",
      "Epoch 40: train loss: 0.36076900362968445\n",
      "Epoch 40: train loss: 0.34469881653785706\n",
      "Epoch 40: train loss: 0.2716044783592224\n",
      "Epoch 40: train loss: 0.0710519477725029\n",
      "Epoch 40: train loss: 0.14728134870529175\n",
      "Epoch 40: train loss: 0.0632331371307373\n",
      "Epoch 40: train loss: 0.20227128267288208\n",
      "Epoch 40: train loss: 0.2632342278957367\n",
      "Epoch 40: train loss: 0.6246581077575684\n",
      "Epoch 40: train loss: 0.18478810787200928\n",
      "Epoch 40: train loss: 0.1327587217092514\n",
      "Epoch 40: train loss: 0.37851622700691223\n",
      "Epoch 40: train loss: 0.12136375159025192\n",
      "Epoch 40: train loss: 0.29867812991142273\n",
      "Epoch 40: train loss: 0.10414890944957733\n",
      "Epoch 40: train loss: 0.25839507579803467\n",
      "Epoch 40: train loss: 0.3415067195892334\n",
      "Epoch 40: train loss: 0.15077772736549377\n",
      "Epoch 40: train loss: 0.252090722322464\n",
      "Epoch 40: train loss: 0.1572486311197281\n",
      "Epoch 40: train loss: 0.3488738238811493\n",
      "Epoch 40: train loss: 0.42108577489852905\n",
      "Epoch 40: train loss: 0.2846037745475769\n",
      "Epoch 40: train loss: 0.21597450971603394\n",
      "Epoch 40: train loss: 0.35516294836997986\n",
      "Epoch 40: train loss: 0.21125495433807373\n",
      "Epoch 40: train loss: 0.19499288499355316\n",
      "Epoch 40: train loss: 0.23753991723060608\n",
      "Epoch 40: train loss: 0.25407838821411133\n",
      "Epoch 40: train loss: 0.26024869084358215\n",
      "Epoch 40: train loss: 0.17450277507305145\n",
      "Epoch 40: train loss: 0.3220318853855133\n",
      "Epoch 40: train loss: 0.30880141258239746\n",
      "Epoch 40: train loss: 0.2789677679538727\n",
      "Epoch 40: train loss: 0.30292147397994995\n",
      "Epoch 40: train loss: 0.27049073576927185\n",
      "Epoch 40: train loss: 0.3518392741680145\n",
      "Epoch 40: train loss: 0.4270409941673279\n",
      "Epoch 40: train loss: 0.28240475058555603\n",
      "Epoch 40: train loss: 0.13707460463047028\n",
      "Epoch 41: train loss: 0.2021489292383194\n",
      "Epoch 41: train loss: 0.33219316601753235\n",
      "Epoch 41: train loss: 0.1618710160255432\n",
      "Epoch 41: train loss: 0.14818958938121796\n",
      "Epoch 41: train loss: 0.18841694295406342\n",
      "Epoch 41: train loss: 0.24471016228199005\n",
      "Epoch 41: train loss: 0.4199334979057312\n",
      "Epoch 41: train loss: 0.1325039565563202\n",
      "Epoch 41: train loss: 0.3417400121688843\n",
      "Epoch 41: train loss: 0.27686044573783875\n",
      "Epoch 41: train loss: 0.24474364519119263\n",
      "Epoch 41: train loss: 0.41061171889305115\n",
      "Epoch 41: train loss: 0.23703336715698242\n",
      "Epoch 41: train loss: 0.21729964017868042\n",
      "Epoch 41: train loss: 0.11860790848731995\n",
      "Epoch 41: train loss: 0.3083925247192383\n",
      "Epoch 41: train loss: 0.27403250336647034\n",
      "Epoch 41: train loss: 0.26232433319091797\n",
      "Epoch 41: train loss: 0.20936915278434753\n",
      "Epoch 41: train loss: 0.12470521032810211\n",
      "Epoch 41: train loss: 0.3926599323749542\n",
      "Epoch 41: train loss: 0.19030553102493286\n",
      "Epoch 41: train loss: 0.18308255076408386\n",
      "Epoch 41: train loss: 0.4731220602989197\n",
      "Epoch 41: train loss: 0.4491489827632904\n",
      "Epoch 41: train loss: 0.2979748547077179\n",
      "Epoch 41: train loss: 0.2443215250968933\n",
      "Epoch 41: train loss: 0.2471000850200653\n",
      "Epoch 41: train loss: 0.17469587922096252\n",
      "Epoch 41: train loss: 0.19194598495960236\n",
      "Epoch 41: train loss: 0.1551932841539383\n",
      "Epoch 41: train loss: 0.1343163102865219\n",
      "Epoch 41: train loss: 0.19552087783813477\n",
      "Epoch 41: train loss: 0.29187119007110596\n",
      "Epoch 41: train loss: 0.21760879456996918\n",
      "Epoch 41: train loss: 0.1909554898738861\n",
      "Epoch 41: train loss: 0.27366137504577637\n",
      "Epoch 41: train loss: 0.16196680068969727\n",
      "Epoch 41: train loss: 0.20473240315914154\n",
      "Epoch 41: train loss: 0.22649365663528442\n",
      "Epoch 41: train loss: 0.26798897981643677\n",
      "Epoch 41: train loss: 0.13281257450580597\n",
      "Epoch 41: train loss: 0.6245805025100708\n",
      "Epoch 41: train loss: 0.28874877095222473\n",
      "Epoch 42: train loss: 0.32549944519996643\n",
      "Epoch 42: train loss: 0.16954128444194794\n",
      "Epoch 42: train loss: 0.4520967900753021\n",
      "Epoch 42: train loss: 0.368065744638443\n",
      "Epoch 42: train loss: 0.24008266627788544\n",
      "Epoch 42: train loss: 0.320785790681839\n",
      "Epoch 42: train loss: 0.25315457582473755\n",
      "Epoch 42: train loss: 0.19100171327590942\n",
      "Epoch 42: train loss: 0.22169986367225647\n",
      "Epoch 42: train loss: 0.23921653628349304\n",
      "Epoch 42: train loss: 0.1908848136663437\n",
      "Epoch 42: train loss: 0.25430604815483093\n",
      "Epoch 42: train loss: 0.34186697006225586\n",
      "Epoch 42: train loss: 0.19833055138587952\n",
      "Epoch 42: train loss: 0.4129544496536255\n",
      "Epoch 42: train loss: 0.42502307891845703\n",
      "Epoch 42: train loss: 0.10243673622608185\n",
      "Epoch 42: train loss: 0.2820419669151306\n",
      "Epoch 42: train loss: 0.37721386551856995\n",
      "Epoch 42: train loss: 0.3250458836555481\n",
      "Epoch 42: train loss: 0.19696545600891113\n",
      "Epoch 42: train loss: 0.3811071515083313\n",
      "Epoch 42: train loss: 0.15564154088497162\n",
      "Epoch 42: train loss: 0.19091005623340607\n",
      "Epoch 42: train loss: 0.21463173627853394\n",
      "Epoch 42: train loss: 0.23143215477466583\n",
      "Epoch 42: train loss: 0.4497218728065491\n",
      "Epoch 42: train loss: 0.20106394588947296\n",
      "Epoch 42: train loss: 0.17130962014198303\n",
      "Epoch 42: train loss: 0.24764342606067657\n",
      "Epoch 42: train loss: 0.4086609184741974\n",
      "Epoch 42: train loss: 0.18088866770267487\n",
      "Epoch 42: train loss: 0.1385323405265808\n",
      "Epoch 42: train loss: 0.1286298781633377\n",
      "Epoch 42: train loss: 0.057487454265356064\n",
      "Epoch 42: train loss: 0.29200735688209534\n",
      "Epoch 42: train loss: 0.24044674634933472\n",
      "Epoch 42: train loss: 0.18046115338802338\n",
      "Epoch 42: train loss: 0.18823473155498505\n",
      "Epoch 42: train loss: 0.11537059396505356\n",
      "Epoch 42: train loss: 0.30020442605018616\n",
      "Epoch 42: train loss: 0.2247072160243988\n",
      "Epoch 42: train loss: 0.24671384692192078\n",
      "Epoch 42: train loss: 0.20081210136413574\n",
      "Epoch 43: train loss: 0.23986123502254486\n",
      "Epoch 43: train loss: 0.1765771061182022\n",
      "Epoch 43: train loss: 0.28201597929000854\n",
      "Epoch 43: train loss: 0.4508591294288635\n",
      "Epoch 43: train loss: 0.1305055171251297\n",
      "Epoch 43: train loss: 0.19132015109062195\n",
      "Epoch 43: train loss: 0.38392555713653564\n",
      "Epoch 43: train loss: 0.14177638292312622\n",
      "Epoch 43: train loss: 0.0961238220334053\n",
      "Epoch 43: train loss: 0.22515442967414856\n",
      "Epoch 43: train loss: 0.18385055661201477\n",
      "Epoch 43: train loss: 0.30260783433914185\n",
      "Epoch 43: train loss: 0.19045953452587128\n",
      "Epoch 43: train loss: 0.3593307435512543\n",
      "Epoch 43: train loss: 0.3032480478286743\n",
      "Epoch 43: train loss: 0.20032435655593872\n",
      "Epoch 43: train loss: 0.5120871067047119\n",
      "Epoch 43: train loss: 0.3777100741863251\n",
      "Epoch 43: train loss: 0.11000487208366394\n",
      "Epoch 43: train loss: 0.14007794857025146\n",
      "Epoch 43: train loss: 0.09672465920448303\n",
      "Epoch 43: train loss: 0.5710055232048035\n",
      "Epoch 43: train loss: 0.10273367911577225\n",
      "Epoch 43: train loss: 0.16371482610702515\n",
      "Epoch 43: train loss: 0.3507803976535797\n",
      "Epoch 43: train loss: 0.22065220773220062\n",
      "Epoch 43: train loss: 0.3285694420337677\n",
      "Epoch 43: train loss: 0.12601234018802643\n",
      "Epoch 43: train loss: 0.3769279420375824\n",
      "Epoch 43: train loss: 0.16197949647903442\n",
      "Epoch 43: train loss: 0.21907255053520203\n",
      "Epoch 43: train loss: 0.310863196849823\n",
      "Epoch 43: train loss: 0.22840538620948792\n",
      "Epoch 43: train loss: 0.21792194247245789\n",
      "Epoch 43: train loss: 0.4071253836154938\n",
      "Epoch 43: train loss: 0.3434309959411621\n",
      "Epoch 43: train loss: 0.3385545015335083\n",
      "Epoch 43: train loss: 0.182224303483963\n",
      "Epoch 43: train loss: 0.0586363710463047\n",
      "Epoch 43: train loss: 0.28018298745155334\n",
      "Epoch 43: train loss: 0.2706402540206909\n",
      "Epoch 43: train loss: 0.32910922169685364\n",
      "Epoch 43: train loss: 0.13839887082576752\n",
      "Epoch 43: train loss: 0.1739063262939453\n",
      "Epoch 44: train loss: 0.2747633457183838\n",
      "Epoch 44: train loss: 0.20825156569480896\n",
      "Epoch 44: train loss: 0.17694197595119476\n",
      "Epoch 44: train loss: 0.160897895693779\n",
      "Epoch 44: train loss: 0.15569043159484863\n",
      "Epoch 44: train loss: 0.3155384063720703\n",
      "Epoch 44: train loss: 0.2944680154323578\n",
      "Epoch 44: train loss: 0.13217172026634216\n",
      "Epoch 44: train loss: 0.30714666843414307\n",
      "Epoch 44: train loss: 0.09904041141271591\n",
      "Epoch 44: train loss: 0.4205881655216217\n",
      "Epoch 44: train loss: 0.2579275965690613\n",
      "Epoch 44: train loss: 0.2531045377254486\n",
      "Epoch 44: train loss: 0.326335608959198\n",
      "Epoch 44: train loss: 0.2627620995044708\n",
      "Epoch 44: train loss: 0.44581231474876404\n",
      "Epoch 44: train loss: 0.17982208728790283\n",
      "Epoch 44: train loss: 0.4016381800174713\n",
      "Epoch 44: train loss: 0.2699297070503235\n",
      "Epoch 44: train loss: 0.3406306207180023\n",
      "Epoch 44: train loss: 0.1994580179452896\n",
      "Epoch 44: train loss: 0.4795907735824585\n",
      "Epoch 44: train loss: 0.2728036940097809\n",
      "Epoch 44: train loss: 0.254591703414917\n",
      "Epoch 44: train loss: 0.2407437264919281\n",
      "Epoch 44: train loss: 0.11034825444221497\n",
      "Epoch 44: train loss: 0.07577463239431381\n",
      "Epoch 44: train loss: 0.227166548371315\n",
      "Epoch 44: train loss: 0.3372637927532196\n",
      "Epoch 44: train loss: 0.22368669509887695\n",
      "Epoch 44: train loss: 0.31972938776016235\n",
      "Epoch 44: train loss: 0.40403273701667786\n",
      "Epoch 44: train loss: 0.284601628780365\n",
      "Epoch 44: train loss: 0.32700470089912415\n",
      "Epoch 44: train loss: 0.20605067908763885\n",
      "Epoch 44: train loss: 0.23118995130062103\n",
      "Epoch 44: train loss: 0.07397530972957611\n",
      "Epoch 44: train loss: 0.19681976735591888\n",
      "Epoch 44: train loss: 0.15805861353874207\n",
      "Epoch 44: train loss: 0.3065374195575714\n",
      "Epoch 44: train loss: 0.14680856466293335\n",
      "Epoch 44: train loss: 0.32159513235092163\n",
      "Epoch 44: train loss: 0.07280638813972473\n",
      "Epoch 44: train loss: 0.32627126574516296\n",
      "Epoch 45: train loss: 0.06915802508592606\n",
      "Epoch 45: train loss: 0.31183090806007385\n",
      "Epoch 45: train loss: 0.4116138219833374\n",
      "Epoch 45: train loss: 0.1995088905096054\n",
      "Epoch 45: train loss: 0.15319493412971497\n",
      "Epoch 45: train loss: 0.22713899612426758\n",
      "Epoch 45: train loss: 0.17910653352737427\n",
      "Epoch 45: train loss: 0.20188339054584503\n",
      "Epoch 45: train loss: 0.21089397370815277\n",
      "Epoch 45: train loss: 0.12317294627428055\n",
      "Epoch 45: train loss: 0.32384559512138367\n",
      "Epoch 45: train loss: 0.16004154086112976\n",
      "Epoch 45: train loss: 0.25482654571533203\n",
      "Epoch 45: train loss: 0.1663273423910141\n",
      "Epoch 45: train loss: 0.3633188009262085\n",
      "Epoch 45: train loss: 0.5689015984535217\n",
      "Epoch 45: train loss: 0.09959732741117477\n",
      "Epoch 45: train loss: 0.1347847282886505\n",
      "Epoch 45: train loss: 0.2737778127193451\n",
      "Epoch 45: train loss: 0.3079923987388611\n",
      "Epoch 45: train loss: 0.27323347330093384\n",
      "Epoch 45: train loss: 0.11375730484724045\n",
      "Epoch 45: train loss: 0.16262295842170715\n",
      "Epoch 45: train loss: 0.42310822010040283\n",
      "Epoch 45: train loss: 0.327152818441391\n",
      "Epoch 45: train loss: 0.19892488420009613\n",
      "Epoch 45: train loss: 0.23256532847881317\n",
      "Epoch 45: train loss: 0.24627384543418884\n",
      "Epoch 45: train loss: 0.13538584113121033\n",
      "Epoch 45: train loss: 0.22019627690315247\n",
      "Epoch 45: train loss: 0.3554188311100006\n",
      "Epoch 45: train loss: 0.18445004522800446\n",
      "Epoch 45: train loss: 0.22544360160827637\n",
      "Epoch 45: train loss: 0.3666301965713501\n",
      "Epoch 45: train loss: 0.17758581042289734\n",
      "Epoch 45: train loss: 0.18780845403671265\n",
      "Epoch 45: train loss: 0.35729628801345825\n",
      "Epoch 45: train loss: 0.18407322466373444\n",
      "Epoch 45: train loss: 0.34039825201034546\n",
      "Epoch 45: train loss: 0.3062945008277893\n",
      "Epoch 45: train loss: 0.28737276792526245\n",
      "Epoch 45: train loss: 0.3494485318660736\n",
      "Epoch 45: train loss: 0.48978349566459656\n",
      "Epoch 45: train loss: 0.098355732858181\n",
      "Epoch 46: train loss: 0.10413192957639694\n",
      "Epoch 46: train loss: 0.1854560524225235\n",
      "Epoch 46: train loss: 0.32706135511398315\n",
      "Epoch 46: train loss: 0.4866352081298828\n",
      "Epoch 46: train loss: 0.28963029384613037\n",
      "Epoch 46: train loss: 0.11681024730205536\n",
      "Epoch 46: train loss: 0.1280667930841446\n",
      "Epoch 46: train loss: 0.1074267253279686\n",
      "Epoch 46: train loss: 0.2509034276008606\n",
      "Epoch 46: train loss: 0.14972160756587982\n",
      "Epoch 46: train loss: 0.08267668634653091\n",
      "Epoch 46: train loss: 0.21041859686374664\n",
      "Epoch 46: train loss: 0.06706550717353821\n",
      "Epoch 46: train loss: 0.635087788105011\n",
      "Epoch 46: train loss: 0.4129765033721924\n",
      "Epoch 46: train loss: 0.18813639879226685\n",
      "Epoch 46: train loss: 0.21885265409946442\n",
      "Epoch 46: train loss: 0.17165710031986237\n",
      "Epoch 46: train loss: 0.14877258241176605\n",
      "Epoch 46: train loss: 0.4159794747829437\n",
      "Epoch 46: train loss: 0.34741148352622986\n",
      "Epoch 46: train loss: 0.22406813502311707\n",
      "Epoch 46: train loss: 0.3281560242176056\n",
      "Epoch 46: train loss: 0.20876148343086243\n",
      "Epoch 46: train loss: 0.24384640157222748\n",
      "Epoch 46: train loss: 0.38249871134757996\n",
      "Epoch 46: train loss: 0.37078964710235596\n",
      "Epoch 46: train loss: 0.13045702874660492\n",
      "Epoch 46: train loss: 0.20808899402618408\n",
      "Epoch 46: train loss: 0.10993755608797073\n",
      "Epoch 46: train loss: 0.06963259726762772\n",
      "Epoch 46: train loss: 0.26877063512802124\n",
      "Epoch 46: train loss: 0.13713990151882172\n",
      "Epoch 46: train loss: 0.252241313457489\n",
      "Epoch 46: train loss: 0.5913047194480896\n",
      "Epoch 46: train loss: 0.37634575366973877\n",
      "Epoch 46: train loss: 0.23433949053287506\n",
      "Epoch 46: train loss: 0.3903871774673462\n",
      "Epoch 46: train loss: 0.29672154784202576\n",
      "Epoch 46: train loss: 0.19116759300231934\n",
      "Epoch 46: train loss: 0.22962412238121033\n",
      "Epoch 46: train loss: 0.12960296869277954\n",
      "Epoch 46: train loss: 0.3014064133167267\n",
      "Epoch 46: train loss: 0.42151758074760437\n",
      "Epoch 47: train loss: 0.19415242969989777\n",
      "Epoch 47: train loss: 0.11122636497020721\n",
      "Epoch 47: train loss: 0.21922798454761505\n",
      "Epoch 47: train loss: 0.13902974128723145\n",
      "Epoch 47: train loss: 0.4956117570400238\n",
      "Epoch 47: train loss: 0.16404305398464203\n",
      "Epoch 47: train loss: 0.1520104706287384\n",
      "Epoch 47: train loss: 0.14287075400352478\n",
      "Epoch 47: train loss: 0.07642225176095963\n",
      "Epoch 47: train loss: 0.3250337839126587\n",
      "Epoch 47: train loss: 0.21221265196800232\n",
      "Epoch 47: train loss: 0.1229071170091629\n",
      "Epoch 47: train loss: 0.2567529082298279\n",
      "Epoch 47: train loss: 0.17457494139671326\n",
      "Epoch 47: train loss: 0.07209072262048721\n",
      "Epoch 47: train loss: 0.3090832233428955\n",
      "Epoch 47: train loss: 0.12516382336616516\n",
      "Epoch 47: train loss: 0.1408243030309677\n",
      "Epoch 47: train loss: 0.2488057017326355\n",
      "Epoch 47: train loss: 0.14297030866146088\n",
      "Epoch 47: train loss: 0.04330422356724739\n",
      "Epoch 47: train loss: 0.4419208765029907\n",
      "Epoch 47: train loss: 0.3817342221736908\n",
      "Epoch 47: train loss: 0.3546932339668274\n",
      "Epoch 47: train loss: 0.45769625902175903\n",
      "Epoch 47: train loss: 0.15985459089279175\n",
      "Epoch 47: train loss: 0.10194713622331619\n",
      "Epoch 47: train loss: 0.11189275979995728\n",
      "Epoch 47: train loss: 0.26306334137916565\n",
      "Epoch 47: train loss: 0.7328988909721375\n",
      "Epoch 47: train loss: 0.22406795620918274\n",
      "Epoch 47: train loss: 0.39836814999580383\n",
      "Epoch 47: train loss: 0.33808472752571106\n",
      "Epoch 47: train loss: 0.3899526596069336\n",
      "Epoch 47: train loss: 0.23951497673988342\n",
      "Epoch 47: train loss: 0.25246909260749817\n",
      "Epoch 47: train loss: 0.34925252199172974\n",
      "Epoch 47: train loss: 0.2626406252384186\n",
      "Epoch 47: train loss: 0.4206541180610657\n",
      "Epoch 47: train loss: 0.3251529932022095\n",
      "Epoch 47: train loss: 0.2170240730047226\n",
      "Epoch 47: train loss: 0.3586304187774658\n",
      "Epoch 47: train loss: 0.29381221532821655\n",
      "Epoch 47: train loss: 0.17758876085281372\n",
      "Epoch 48: train loss: 0.38628363609313965\n",
      "Epoch 48: train loss: 0.4123803675174713\n",
      "Epoch 48: train loss: 0.16776522994041443\n",
      "Epoch 48: train loss: 0.3122461438179016\n",
      "Epoch 48: train loss: 0.3488689661026001\n",
      "Epoch 48: train loss: 0.18783050775527954\n",
      "Epoch 48: train loss: 0.13089600205421448\n",
      "Epoch 48: train loss: 0.09383417665958405\n",
      "Epoch 48: train loss: 0.3145564794540405\n",
      "Epoch 48: train loss: 0.09113183617591858\n",
      "Epoch 48: train loss: 0.1942920684814453\n",
      "Epoch 48: train loss: 0.23925599455833435\n",
      "Epoch 48: train loss: 0.03764359652996063\n",
      "Epoch 48: train loss: 0.3883270025253296\n",
      "Epoch 48: train loss: 0.22647538781166077\n",
      "Epoch 48: train loss: 0.32671743631362915\n",
      "Epoch 48: train loss: 0.5976668000221252\n",
      "Epoch 48: train loss: 0.4520566165447235\n",
      "Epoch 48: train loss: 0.5119892358779907\n",
      "Epoch 48: train loss: 0.1929847151041031\n",
      "Epoch 48: train loss: 0.24558201432228088\n",
      "Epoch 48: train loss: 0.06607814878225327\n",
      "Epoch 48: train loss: 0.3111153841018677\n",
      "Epoch 48: train loss: 0.174360454082489\n",
      "Epoch 48: train loss: 0.255361407995224\n",
      "Epoch 48: train loss: 0.23968912661075592\n",
      "Epoch 48: train loss: 0.3266531825065613\n",
      "Epoch 48: train loss: 0.295401394367218\n",
      "Epoch 48: train loss: 0.24440442025661469\n",
      "Epoch 48: train loss: 0.2242365926504135\n",
      "Epoch 48: train loss: 0.27598240971565247\n",
      "Epoch 48: train loss: 0.23177911341190338\n",
      "Epoch 48: train loss: 0.23307467997074127\n",
      "Epoch 48: train loss: 0.15042810142040253\n",
      "Epoch 48: train loss: 0.24273772537708282\n",
      "Epoch 48: train loss: 0.23111814260482788\n",
      "Epoch 48: train loss: 0.16785800457000732\n",
      "Epoch 48: train loss: 0.3260956406593323\n",
      "Epoch 48: train loss: 0.31903690099716187\n",
      "Epoch 48: train loss: 0.3521297872066498\n",
      "Epoch 48: train loss: 0.18060488998889923\n",
      "Epoch 48: train loss: 0.1091158539056778\n",
      "Epoch 48: train loss: 0.1196698546409607\n",
      "Epoch 48: train loss: 0.27937841415405273\n",
      "Epoch 49: train loss: 0.44409823417663574\n",
      "Epoch 49: train loss: 0.2961457669734955\n",
      "Epoch 49: train loss: 0.2593747079372406\n",
      "Epoch 49: train loss: 0.4297223687171936\n",
      "Epoch 49: train loss: 0.384962797164917\n",
      "Epoch 49: train loss: 0.23357689380645752\n",
      "Epoch 49: train loss: 0.19932354986667633\n",
      "Epoch 49: train loss: 0.16059066355228424\n",
      "Epoch 49: train loss: 0.18693828582763672\n",
      "Epoch 49: train loss: 0.26015418767929077\n",
      "Epoch 49: train loss: 0.11779610812664032\n",
      "Epoch 49: train loss: 0.07172752916812897\n",
      "Epoch 49: train loss: 0.3038221597671509\n",
      "Epoch 49: train loss: 0.38559308648109436\n",
      "Epoch 49: train loss: 0.1765669733285904\n",
      "Epoch 49: train loss: 0.3215337097644806\n",
      "Epoch 49: train loss: 0.13388332724571228\n",
      "Epoch 49: train loss: 0.23867137730121613\n",
      "Epoch 49: train loss: 0.07953999936580658\n",
      "Epoch 49: train loss: 0.2600443661212921\n",
      "Epoch 49: train loss: 0.3132949471473694\n",
      "Epoch 49: train loss: 0.3180292844772339\n",
      "Epoch 49: train loss: 0.3330220580101013\n",
      "Epoch 49: train loss: 0.20374706387519836\n",
      "Epoch 49: train loss: 0.3784795105457306\n",
      "Epoch 49: train loss: 0.23610252141952515\n",
      "Epoch 49: train loss: 0.2678739130496979\n",
      "Epoch 49: train loss: 0.16835510730743408\n",
      "Epoch 49: train loss: 0.2659072279930115\n",
      "Epoch 49: train loss: 0.608974277973175\n",
      "Epoch 49: train loss: 0.23972830176353455\n",
      "Epoch 49: train loss: 0.13662636280059814\n",
      "Epoch 49: train loss: 0.17500998079776764\n",
      "Epoch 49: train loss: 0.26886871457099915\n",
      "Epoch 49: train loss: 0.17910484969615936\n",
      "Epoch 49: train loss: 0.31805211305618286\n",
      "Epoch 49: train loss: 0.1529683768749237\n",
      "Epoch 49: train loss: 0.20493929088115692\n",
      "Epoch 49: train loss: 0.30067959427833557\n",
      "Epoch 49: train loss: 0.19090139865875244\n",
      "Epoch 49: train loss: 0.13327263295650482\n",
      "Epoch 49: train loss: 0.26217886805534363\n",
      "Epoch 49: train loss: 0.3325444459915161\n",
      "Epoch 49: train loss: 0.20607087016105652\n",
      "Epoch 50: train loss: 0.2652519643306732\n",
      "Epoch 50: train loss: 0.2402331382036209\n",
      "Epoch 50: train loss: 0.14627395570278168\n",
      "Epoch 50: train loss: 0.27016544342041016\n",
      "Epoch 50: train loss: 0.41730839014053345\n",
      "Epoch 50: train loss: 0.21217922866344452\n",
      "Epoch 50: train loss: 0.30344393849372864\n",
      "Epoch 50: train loss: 0.16943185031414032\n",
      "Epoch 50: train loss: 0.08525031059980392\n",
      "Epoch 50: train loss: 0.30588018894195557\n",
      "Epoch 50: train loss: 0.25111669301986694\n",
      "Epoch 50: train loss: 0.5901645421981812\n",
      "Epoch 50: train loss: 0.17154482007026672\n",
      "Epoch 50: train loss: 0.1851312667131424\n",
      "Epoch 50: train loss: 0.24968591332435608\n",
      "Epoch 50: train loss: 0.4001576006412506\n",
      "Epoch 50: train loss: 0.27132540941238403\n",
      "Epoch 50: train loss: 0.32217180728912354\n",
      "Epoch 50: train loss: 0.1314862221479416\n",
      "Epoch 50: train loss: 0.20691229403018951\n",
      "Epoch 50: train loss: 0.34098783135414124\n",
      "Epoch 50: train loss: 0.2040620595216751\n",
      "Epoch 50: train loss: 0.31489449739456177\n",
      "Epoch 50: train loss: 0.19363467395305634\n",
      "Epoch 50: train loss: 0.26462846994400024\n",
      "Epoch 50: train loss: 0.3099353015422821\n",
      "Epoch 50: train loss: 0.3689914643764496\n",
      "Epoch 50: train loss: 0.3036544919013977\n",
      "Epoch 50: train loss: 0.187615767121315\n",
      "Epoch 50: train loss: 0.2508905231952667\n",
      "Epoch 50: train loss: 0.17989395558834076\n",
      "Epoch 50: train loss: 0.14513806998729706\n",
      "Epoch 50: train loss: 0.15914341807365417\n",
      "Epoch 50: train loss: 0.38389769196510315\n",
      "Epoch 50: train loss: 0.1733218878507614\n",
      "Epoch 50: train loss: 0.24492821097373962\n",
      "Epoch 50: train loss: 0.07011455297470093\n",
      "Epoch 50: train loss: 0.20552119612693787\n",
      "Epoch 50: train loss: 0.21326911449432373\n",
      "Epoch 50: train loss: 0.3957027196884155\n",
      "Epoch 50: train loss: 0.2130240947008133\n",
      "Epoch 50: train loss: 0.15353934466838837\n",
      "Epoch 50: train loss: 0.1157391145825386\n",
      "Epoch 50: train loss: 0.46057966351509094\n",
      "Epoch 51: train loss: 0.05675603449344635\n",
      "Epoch 51: train loss: 0.21670928597450256\n",
      "Epoch 51: train loss: 0.32489821314811707\n",
      "Epoch 51: train loss: 0.28899142146110535\n",
      "Epoch 51: train loss: 0.17144200205802917\n",
      "Epoch 51: train loss: 0.18268997967243195\n",
      "Epoch 51: train loss: 0.29891717433929443\n",
      "Epoch 51: train loss: 0.06443281471729279\n",
      "Epoch 51: train loss: 0.41124269366264343\n",
      "Epoch 51: train loss: 0.3116952180862427\n",
      "Epoch 51: train loss: 0.13379333913326263\n",
      "Epoch 51: train loss: 0.38680875301361084\n",
      "Epoch 51: train loss: 0.20146436989307404\n",
      "Epoch 51: train loss: 0.3079049289226532\n",
      "Epoch 51: train loss: 0.41054797172546387\n",
      "Epoch 51: train loss: 0.26344209909439087\n",
      "Epoch 51: train loss: 0.18467220664024353\n",
      "Epoch 51: train loss: 0.14506055414676666\n",
      "Epoch 51: train loss: 0.13658078014850616\n",
      "Epoch 51: train loss: 0.3280662000179291\n",
      "Epoch 51: train loss: 0.12313701957464218\n",
      "Epoch 51: train loss: 0.3319910764694214\n",
      "Epoch 51: train loss: 0.3725723326206207\n",
      "Epoch 51: train loss: 0.45574507117271423\n",
      "Epoch 51: train loss: 0.09832928329706192\n",
      "Epoch 51: train loss: 0.16571830213069916\n",
      "Epoch 51: train loss: 0.18180450797080994\n",
      "Epoch 51: train loss: 0.38109782338142395\n",
      "Epoch 51: train loss: 0.3448166847229004\n",
      "Epoch 51: train loss: 0.28518784046173096\n",
      "Epoch 51: train loss: 0.08656984567642212\n",
      "Epoch 51: train loss: 0.2962051331996918\n",
      "Epoch 51: train loss: 0.3308255970478058\n",
      "Epoch 51: train loss: 0.23804977536201477\n",
      "Epoch 51: train loss: 0.2865435481071472\n",
      "Epoch 51: train loss: 0.28007078170776367\n",
      "Epoch 51: train loss: 0.352805495262146\n",
      "Epoch 51: train loss: 0.25985097885131836\n",
      "Epoch 51: train loss: 0.4083418548107147\n",
      "Epoch 51: train loss: 0.15738309919834137\n",
      "Epoch 51: train loss: 0.1398046910762787\n",
      "Epoch 51: train loss: 0.21519653499126434\n",
      "Epoch 51: train loss: 0.18263821303844452\n",
      "Epoch 51: train loss: 0.2696762979030609\n",
      "Epoch 52: train loss: 0.24518494307994843\n",
      "Epoch 52: train loss: 0.3637637794017792\n",
      "Epoch 52: train loss: 0.21047984063625336\n",
      "Epoch 52: train loss: 0.2164725810289383\n",
      "Epoch 52: train loss: 0.23931872844696045\n",
      "Epoch 52: train loss: 0.143042653799057\n",
      "Epoch 52: train loss: 0.20000283420085907\n",
      "Epoch 52: train loss: 0.3700990378856659\n",
      "Epoch 52: train loss: 0.13684266805648804\n",
      "Epoch 52: train loss: 0.3123730719089508\n",
      "Epoch 52: train loss: 0.2871605455875397\n",
      "Epoch 52: train loss: 0.08706682920455933\n",
      "Epoch 52: train loss: 0.42588678002357483\n",
      "Epoch 52: train loss: 0.29478025436401367\n",
      "Epoch 52: train loss: 0.3026939332485199\n",
      "Epoch 52: train loss: 0.2263597548007965\n",
      "Epoch 52: train loss: 0.3077790439128876\n",
      "Epoch 52: train loss: 0.45305249094963074\n",
      "Epoch 52: train loss: 0.4019321799278259\n",
      "Epoch 52: train loss: 0.32715022563934326\n",
      "Epoch 52: train loss: 0.2667367160320282\n",
      "Epoch 52: train loss: 0.3573766052722931\n",
      "Epoch 52: train loss: 0.3830963373184204\n",
      "Epoch 52: train loss: 0.216933935880661\n",
      "Epoch 52: train loss: 0.31010881066322327\n",
      "Epoch 52: train loss: 0.23199868202209473\n",
      "Epoch 52: train loss: 0.25505244731903076\n",
      "Epoch 52: train loss: 0.19732829928398132\n",
      "Epoch 52: train loss: 0.3028947710990906\n",
      "Epoch 52: train loss: 0.28398603200912476\n",
      "Epoch 52: train loss: 0.10846953094005585\n",
      "Epoch 52: train loss: 0.3296063244342804\n",
      "Epoch 52: train loss: 0.1972539722919464\n",
      "Epoch 52: train loss: 0.2914467751979828\n",
      "Epoch 52: train loss: 0.2091182917356491\n",
      "Epoch 52: train loss: 0.15843424201011658\n",
      "Epoch 52: train loss: 0.20384147763252258\n",
      "Epoch 52: train loss: 0.11877553164958954\n",
      "Epoch 52: train loss: 0.05694644898176193\n",
      "Epoch 52: train loss: 0.13966041803359985\n",
      "Epoch 52: train loss: 0.1676633656024933\n",
      "Epoch 52: train loss: 0.12144651263952255\n",
      "Epoch 52: train loss: 0.27909615635871887\n",
      "Epoch 52: train loss: 0.123020239174366\n",
      "Epoch 53: train loss: 0.2401185780763626\n",
      "Epoch 53: train loss: 0.32587409019470215\n",
      "Epoch 53: train loss: 0.34215763211250305\n",
      "Epoch 53: train loss: 0.13424177467823029\n",
      "Epoch 53: train loss: 0.13636204600334167\n",
      "Epoch 53: train loss: 0.10974650830030441\n",
      "Epoch 53: train loss: 0.25067973136901855\n",
      "Epoch 53: train loss: 0.24245940148830414\n",
      "Epoch 53: train loss: 0.1291547268629074\n",
      "Epoch 53: train loss: 0.4721018075942993\n",
      "Epoch 53: train loss: 0.4827688932418823\n",
      "Epoch 53: train loss: 0.3822529911994934\n",
      "Epoch 53: train loss: 0.1998656988143921\n",
      "Epoch 53: train loss: 0.23614098131656647\n",
      "Epoch 53: train loss: 0.1547715663909912\n",
      "Epoch 53: train loss: 0.1967373639345169\n",
      "Epoch 53: train loss: 0.18526381254196167\n",
      "Epoch 53: train loss: 0.26270389556884766\n",
      "Epoch 53: train loss: 0.17932648956775665\n",
      "Epoch 53: train loss: 0.3471382260322571\n",
      "Epoch 53: train loss: 0.3764045834541321\n",
      "Epoch 53: train loss: 0.1827547550201416\n",
      "Epoch 53: train loss: 0.18076682090759277\n",
      "Epoch 53: train loss: 0.28949737548828125\n",
      "Epoch 53: train loss: 0.37942180037498474\n",
      "Epoch 53: train loss: 0.31203851103782654\n",
      "Epoch 53: train loss: 0.2737284004688263\n",
      "Epoch 53: train loss: 0.2659848928451538\n",
      "Epoch 53: train loss: 0.258051335811615\n",
      "Epoch 53: train loss: 0.15846987068653107\n",
      "Epoch 53: train loss: 0.21600653231143951\n",
      "Epoch 53: train loss: 0.3775339424610138\n",
      "Epoch 53: train loss: 0.42910483479499817\n",
      "Epoch 53: train loss: 0.3848656415939331\n",
      "Epoch 53: train loss: 0.30863726139068604\n",
      "Epoch 53: train loss: 0.1991223692893982\n",
      "Epoch 53: train loss: 0.0754239484667778\n",
      "Epoch 53: train loss: 0.24921701848506927\n",
      "Epoch 53: train loss: 0.24205036461353302\n",
      "Epoch 53: train loss: 0.1274186074733734\n",
      "Epoch 53: train loss: 0.3670518100261688\n",
      "Epoch 53: train loss: 0.283967524766922\n",
      "Epoch 53: train loss: 0.2346394956111908\n",
      "Epoch 53: train loss: 0.08060982823371887\n",
      "Epoch 54: train loss: 0.4331521689891815\n",
      "Epoch 54: train loss: 0.39743396639823914\n",
      "Epoch 54: train loss: 0.22112683951854706\n",
      "Epoch 54: train loss: 0.2844754755496979\n",
      "Epoch 54: train loss: 0.15312452614307404\n",
      "Epoch 54: train loss: 0.254971981048584\n",
      "Epoch 54: train loss: 0.4206945598125458\n",
      "Epoch 54: train loss: 0.381287544965744\n",
      "Epoch 54: train loss: 0.35234948992729187\n",
      "Epoch 54: train loss: 0.18465444445610046\n",
      "Epoch 54: train loss: 0.20636893808841705\n",
      "Epoch 54: train loss: 0.22989363968372345\n",
      "Epoch 54: train loss: 0.16128171980381012\n",
      "Epoch 54: train loss: 0.1951623111963272\n",
      "Epoch 54: train loss: 0.1968362033367157\n",
      "Epoch 54: train loss: 0.09258204698562622\n",
      "Epoch 54: train loss: 0.18417605757713318\n",
      "Epoch 54: train loss: 0.16111834347248077\n",
      "Epoch 54: train loss: 0.41487765312194824\n",
      "Epoch 54: train loss: 0.08875332027673721\n",
      "Epoch 54: train loss: 0.22661983966827393\n",
      "Epoch 54: train loss: 0.3972666263580322\n",
      "Epoch 54: train loss: 0.3012526333332062\n",
      "Epoch 54: train loss: 0.35039177536964417\n",
      "Epoch 54: train loss: 0.051163047552108765\n",
      "Epoch 54: train loss: 0.15418341755867004\n",
      "Epoch 54: train loss: 0.17485256493091583\n",
      "Epoch 54: train loss: 0.29545527696609497\n",
      "Epoch 54: train loss: 0.17624147236347198\n",
      "Epoch 54: train loss: 0.1664808839559555\n",
      "Epoch 54: train loss: 0.458074688911438\n",
      "Epoch 54: train loss: 0.2933503985404968\n",
      "Epoch 54: train loss: 0.2117481231689453\n",
      "Epoch 54: train loss: 0.258238285779953\n",
      "Epoch 54: train loss: 0.40756216645240784\n",
      "Epoch 54: train loss: 0.22600853443145752\n",
      "Epoch 54: train loss: 0.13841955363750458\n",
      "Epoch 54: train loss: 0.2376202642917633\n",
      "Epoch 54: train loss: 0.33821433782577515\n",
      "Epoch 54: train loss: 0.2324165552854538\n",
      "Epoch 54: train loss: 0.21065029501914978\n",
      "Epoch 54: train loss: 0.27627289295196533\n",
      "Epoch 54: train loss: 0.28333064913749695\n",
      "Epoch 54: train loss: 0.2578047215938568\n",
      "Epoch 55: train loss: 0.2076285183429718\n",
      "Epoch 55: train loss: 0.36590492725372314\n",
      "Epoch 55: train loss: 0.3582671582698822\n",
      "Epoch 55: train loss: 0.23828379809856415\n",
      "Epoch 55: train loss: 0.15928952395915985\n",
      "Epoch 55: train loss: 0.1807863563299179\n",
      "Epoch 55: train loss: 0.17307938635349274\n",
      "Epoch 55: train loss: 0.165938138961792\n",
      "Epoch 55: train loss: 0.31405267119407654\n",
      "Epoch 55: train loss: 0.20407842099666595\n",
      "Epoch 55: train loss: 0.31995731592178345\n",
      "Epoch 55: train loss: 0.18966877460479736\n",
      "Epoch 55: train loss: 0.09999604523181915\n",
      "Epoch 55: train loss: 0.3015733063220978\n",
      "Epoch 55: train loss: 0.35766172409057617\n",
      "Epoch 55: train loss: 0.21956536173820496\n",
      "Epoch 55: train loss: 0.15772300958633423\n",
      "Epoch 55: train loss: 0.5180721879005432\n",
      "Epoch 55: train loss: 0.2981494069099426\n",
      "Epoch 55: train loss: 0.25207918882369995\n",
      "Epoch 55: train loss: 0.3037625551223755\n",
      "Epoch 55: train loss: 0.24504123628139496\n",
      "Epoch 55: train loss: 0.29302120208740234\n",
      "Epoch 55: train loss: 0.2506750822067261\n",
      "Epoch 55: train loss: 0.2036237269639969\n",
      "Epoch 55: train loss: 0.23299603164196014\n",
      "Epoch 55: train loss: 0.18061204254627228\n",
      "Epoch 55: train loss: 0.24158617854118347\n",
      "Epoch 55: train loss: 0.26358166337013245\n",
      "Epoch 55: train loss: 0.26318469643592834\n",
      "Epoch 55: train loss: 0.30044177174568176\n",
      "Epoch 55: train loss: 0.14806494116783142\n",
      "Epoch 55: train loss: 0.18558543920516968\n",
      "Epoch 55: train loss: 0.18248647451400757\n",
      "Epoch 55: train loss: 0.20520202815532684\n",
      "Epoch 55: train loss: 0.2243296355009079\n",
      "Epoch 55: train loss: 0.28557509183883667\n",
      "Epoch 55: train loss: 0.1453523188829422\n",
      "Epoch 55: train loss: 0.4302540421485901\n",
      "Epoch 55: train loss: 0.2553728520870209\n",
      "Epoch 55: train loss: 0.20434947311878204\n",
      "Epoch 55: train loss: 0.49945470690727234\n",
      "Epoch 55: train loss: 0.29158201813697815\n",
      "Epoch 55: train loss: 0.205391988158226\n",
      "Epoch 56: train loss: 0.32446470856666565\n",
      "Epoch 56: train loss: 0.25141558051109314\n",
      "Epoch 56: train loss: 0.09647742658853531\n",
      "Epoch 56: train loss: 0.21161888539791107\n",
      "Epoch 56: train loss: 0.2275366187095642\n",
      "Epoch 56: train loss: 0.12509386241436005\n",
      "Epoch 56: train loss: 0.15499423444271088\n",
      "Epoch 56: train loss: 0.1508929431438446\n",
      "Epoch 56: train loss: 0.47877055406570435\n",
      "Epoch 56: train loss: 0.30594566464424133\n",
      "Epoch 56: train loss: 0.18743211030960083\n",
      "Epoch 56: train loss: 0.19004061818122864\n",
      "Epoch 56: train loss: 0.28033334016799927\n",
      "Epoch 56: train loss: 0.09930702298879623\n",
      "Epoch 56: train loss: 0.26887479424476624\n",
      "Epoch 56: train loss: 0.2855950593948364\n",
      "Epoch 56: train loss: 0.20435155928134918\n",
      "Epoch 56: train loss: 0.28200867772102356\n",
      "Epoch 56: train loss: 0.2167508602142334\n",
      "Epoch 56: train loss: 0.42923691868782043\n",
      "Epoch 56: train loss: 0.24793744087219238\n",
      "Epoch 56: train loss: 0.14185155928134918\n",
      "Epoch 56: train loss: 0.22432568669319153\n",
      "Epoch 56: train loss: 0.19087176024913788\n",
      "Epoch 56: train loss: 0.11031480133533478\n",
      "Epoch 56: train loss: 0.4129740595817566\n",
      "Epoch 56: train loss: 0.7055933475494385\n",
      "Epoch 56: train loss: 0.21346835792064667\n",
      "Epoch 56: train loss: 0.08717217296361923\n",
      "Epoch 56: train loss: 0.3944609761238098\n",
      "Epoch 56: train loss: 0.4792317748069763\n",
      "Epoch 56: train loss: 0.3181156814098358\n",
      "Epoch 56: train loss: 0.2085118293762207\n",
      "Epoch 56: train loss: 0.10843898355960846\n",
      "Epoch 56: train loss: 0.18123623728752136\n",
      "Epoch 56: train loss: 0.3664228916168213\n",
      "Epoch 56: train loss: 0.2525773346424103\n",
      "Epoch 56: train loss: 0.2754019796848297\n",
      "Epoch 56: train loss: 0.18166705965995789\n",
      "Epoch 56: train loss: 0.23798151314258575\n",
      "Epoch 56: train loss: 0.19199351966381073\n",
      "Epoch 56: train loss: 0.32965216040611267\n",
      "Epoch 56: train loss: 0.2711485028266907\n",
      "Epoch 56: train loss: 0.14002344012260437\n",
      "Epoch 57: train loss: 0.22447499632835388\n",
      "Epoch 57: train loss: 0.20445242524147034\n",
      "Epoch 57: train loss: 0.2930234372615814\n",
      "Epoch 57: train loss: 0.12811094522476196\n",
      "Epoch 57: train loss: 0.45064660906791687\n",
      "Epoch 57: train loss: 0.19894732534885406\n",
      "Epoch 57: train loss: 0.29841670393943787\n",
      "Epoch 57: train loss: 0.2584584355354309\n",
      "Epoch 57: train loss: 0.21373331546783447\n",
      "Epoch 57: train loss: 0.36136916279792786\n",
      "Epoch 57: train loss: 0.15468259155750275\n",
      "Epoch 57: train loss: 0.3278064429759979\n",
      "Epoch 57: train loss: 0.19796232879161835\n",
      "Epoch 57: train loss: 0.33605054020881653\n",
      "Epoch 57: train loss: 0.21211065351963043\n",
      "Epoch 57: train loss: 0.11968020349740982\n",
      "Epoch 57: train loss: 0.3635600507259369\n",
      "Epoch 57: train loss: 0.13740502297878265\n",
      "Epoch 57: train loss: 0.31243452429771423\n",
      "Epoch 57: train loss: 0.3013943135738373\n",
      "Epoch 57: train loss: 0.16282017529010773\n",
      "Epoch 57: train loss: 0.2401150017976761\n",
      "Epoch 57: train loss: 0.08628733456134796\n",
      "Epoch 57: train loss: 0.08338072150945663\n",
      "Epoch 57: train loss: 0.36828967928886414\n",
      "Epoch 57: train loss: 0.19378474354743958\n",
      "Epoch 57: train loss: 0.29610681533813477\n",
      "Epoch 57: train loss: 0.25608929991722107\n",
      "Epoch 57: train loss: 0.18091808259487152\n",
      "Epoch 57: train loss: 0.24672411382198334\n",
      "Epoch 57: train loss: 0.0698346272110939\n",
      "Epoch 57: train loss: 0.2756637930870056\n",
      "Epoch 57: train loss: 0.05880036950111389\n",
      "Epoch 57: train loss: 0.10944994539022446\n",
      "Epoch 57: train loss: 0.2683359682559967\n",
      "Epoch 57: train loss: 0.4139002859592438\n",
      "Epoch 57: train loss: 0.21967187523841858\n",
      "Epoch 57: train loss: 0.3389972746372223\n",
      "Epoch 57: train loss: 0.29950058460235596\n",
      "Epoch 57: train loss: 0.5212879776954651\n",
      "Epoch 57: train loss: 0.3442995250225067\n",
      "Epoch 57: train loss: 0.21899425983428955\n",
      "Epoch 57: train loss: 0.3006003797054291\n",
      "Epoch 57: train loss: 0.3339475393295288\n",
      "Epoch 58: train loss: 0.31242692470550537\n",
      "Epoch 58: train loss: 0.1832367330789566\n",
      "Epoch 58: train loss: 0.3313921391963959\n",
      "Epoch 58: train loss: 0.2681173086166382\n",
      "Epoch 58: train loss: 0.1234617829322815\n",
      "Epoch 58: train loss: 0.28785818815231323\n",
      "Epoch 58: train loss: 0.19430804252624512\n",
      "Epoch 58: train loss: 0.48940885066986084\n",
      "Epoch 58: train loss: 0.2239953577518463\n",
      "Epoch 58: train loss: 0.20836474001407623\n",
      "Epoch 58: train loss: 0.1911112517118454\n",
      "Epoch 58: train loss: 0.28630152344703674\n",
      "Epoch 58: train loss: 0.5557883381843567\n",
      "Epoch 58: train loss: 0.31989166140556335\n",
      "Epoch 58: train loss: 0.12192544341087341\n",
      "Epoch 58: train loss: 0.129625141620636\n",
      "Epoch 58: train loss: 0.11550900340080261\n",
      "Epoch 58: train loss: 0.312664270401001\n",
      "Epoch 58: train loss: 0.23041102290153503\n",
      "Epoch 58: train loss: 0.6212335824966431\n",
      "Epoch 58: train loss: 0.3724292516708374\n",
      "Epoch 58: train loss: 0.265638142824173\n",
      "Epoch 58: train loss: 0.11037652939558029\n",
      "Epoch 58: train loss: 0.2757444381713867\n",
      "Epoch 58: train loss: 0.17752137780189514\n",
      "Epoch 58: train loss: 0.1801939159631729\n",
      "Epoch 58: train loss: 0.12650194764137268\n",
      "Epoch 58: train loss: 0.22873033583164215\n",
      "Epoch 58: train loss: 0.24166518449783325\n",
      "Epoch 58: train loss: 0.12078888714313507\n",
      "Epoch 58: train loss: 0.2669529318809509\n",
      "Epoch 58: train loss: 0.3599749803543091\n",
      "Epoch 58: train loss: 0.1322195827960968\n",
      "Epoch 58: train loss: 0.14459344744682312\n",
      "Epoch 58: train loss: 0.34206464886665344\n",
      "Epoch 58: train loss: 0.25245314836502075\n",
      "Epoch 58: train loss: 0.30847734212875366\n",
      "Epoch 58: train loss: 0.07573787122964859\n",
      "Epoch 58: train loss: 0.2753271460533142\n",
      "Epoch 58: train loss: 0.14925891160964966\n",
      "Epoch 58: train loss: 0.16185738146305084\n",
      "Epoch 58: train loss: 0.42119932174682617\n",
      "Epoch 58: train loss: 0.1645280122756958\n",
      "Epoch 58: train loss: 0.35183149576187134\n",
      "Epoch 59: train loss: 0.4899314045906067\n",
      "Epoch 59: train loss: 0.22525744140148163\n",
      "Epoch 59: train loss: 0.24742643535137177\n",
      "Epoch 59: train loss: 0.08083765208721161\n",
      "Epoch 59: train loss: 0.12577682733535767\n",
      "Epoch 59: train loss: 0.19833344221115112\n",
      "Epoch 59: train loss: 0.25145989656448364\n",
      "Epoch 59: train loss: 0.07873013615608215\n",
      "Epoch 59: train loss: 0.38427305221557617\n",
      "Epoch 59: train loss: 0.09275869280099869\n",
      "Epoch 59: train loss: 0.38783636689186096\n",
      "Epoch 59: train loss: 0.42674171924591064\n",
      "Epoch 59: train loss: 0.24636881053447723\n",
      "Epoch 59: train loss: 0.21875514090061188\n",
      "Epoch 59: train loss: 0.28582945466041565\n",
      "Epoch 59: train loss: 0.2527168095111847\n",
      "Epoch 59: train loss: 0.2697652280330658\n",
      "Epoch 59: train loss: 0.2689601182937622\n",
      "Epoch 59: train loss: 0.357672780752182\n",
      "Epoch 59: train loss: 0.26847267150878906\n",
      "Epoch 59: train loss: 0.18158689141273499\n",
      "Epoch 59: train loss: 0.1555027961730957\n",
      "Epoch 59: train loss: 0.18876080214977264\n",
      "Epoch 59: train loss: 0.23469240963459015\n",
      "Epoch 59: train loss: 0.37990066409111023\n",
      "Epoch 59: train loss: 0.6089582443237305\n",
      "Epoch 59: train loss: 0.2270016074180603\n",
      "Epoch 59: train loss: 0.2611675262451172\n",
      "Epoch 59: train loss: 0.4196798503398895\n",
      "Epoch 59: train loss: 0.2037586122751236\n",
      "Epoch 59: train loss: 0.14236237108707428\n",
      "Epoch 59: train loss: 0.3301548659801483\n",
      "Epoch 59: train loss: 0.34692156314849854\n",
      "Epoch 59: train loss: 0.1374596506357193\n",
      "Epoch 59: train loss: 0.1613834649324417\n",
      "Epoch 59: train loss: 0.08902057260274887\n",
      "Epoch 59: train loss: 0.23417547345161438\n",
      "Epoch 59: train loss: 0.21337704360485077\n",
      "Epoch 59: train loss: 0.3045465648174286\n",
      "Epoch 59: train loss: 0.3667622208595276\n",
      "Epoch 59: train loss: 0.18854990601539612\n",
      "Epoch 59: train loss: 0.10617595165967941\n",
      "Epoch 59: train loss: 0.10599006712436676\n",
      "Epoch 59: train loss: 0.28681936860084534\n",
      "Epoch 60: train loss: 0.3299722969532013\n",
      "Epoch 60: train loss: 0.13581238687038422\n",
      "Epoch 60: train loss: 0.17779618501663208\n",
      "Epoch 60: train loss: 0.24406099319458008\n",
      "Epoch 60: train loss: 0.08140141516923904\n",
      "Epoch 60: train loss: 0.23718562722206116\n",
      "Epoch 60: train loss: 0.18516646325588226\n",
      "Epoch 60: train loss: 0.2722017168998718\n",
      "Epoch 60: train loss: 0.15857018530368805\n",
      "Epoch 60: train loss: 0.3037357032299042\n",
      "Epoch 60: train loss: 0.30128297209739685\n",
      "Epoch 60: train loss: 0.41087278723716736\n",
      "Epoch 60: train loss: 0.17021596431732178\n",
      "Epoch 60: train loss: 0.15276899933815002\n",
      "Epoch 60: train loss: 0.28531211614608765\n",
      "Epoch 60: train loss: 0.23834370076656342\n",
      "Epoch 60: train loss: 0.13189466297626495\n",
      "Epoch 60: train loss: 0.2154776006937027\n",
      "Epoch 60: train loss: 0.18794071674346924\n",
      "Epoch 60: train loss: 0.3877996504306793\n",
      "Epoch 60: train loss: 0.2066381275653839\n",
      "Epoch 60: train loss: 0.19137141108512878\n",
      "Epoch 60: train loss: 0.17243841290473938\n",
      "Epoch 60: train loss: 0.13888120651245117\n",
      "Epoch 60: train loss: 0.13307936489582062\n",
      "Epoch 60: train loss: 0.26851189136505127\n",
      "Epoch 60: train loss: 0.5023707151412964\n",
      "Epoch 60: train loss: 0.2251940220594406\n",
      "Epoch 60: train loss: 0.257425457239151\n",
      "Epoch 60: train loss: 0.44845882058143616\n",
      "Epoch 60: train loss: 0.7502466440200806\n",
      "Epoch 60: train loss: 0.13202336430549622\n",
      "Epoch 60: train loss: 0.23411011695861816\n",
      "Epoch 60: train loss: 0.203529953956604\n",
      "Epoch 60: train loss: 0.26589325070381165\n",
      "Epoch 60: train loss: 0.43266797065734863\n",
      "Epoch 60: train loss: 0.21948866546154022\n",
      "Epoch 60: train loss: 0.4472418427467346\n",
      "Epoch 60: train loss: 0.16495193541049957\n",
      "Epoch 60: train loss: 0.24199126660823822\n",
      "Epoch 60: train loss: 0.23852361738681793\n",
      "Epoch 60: train loss: 0.2527483105659485\n",
      "Epoch 60: train loss: 0.23760785162448883\n",
      "Epoch 60: train loss: 0.18109527230262756\n",
      "Epoch 61: train loss: 0.2815938889980316\n",
      "Epoch 61: train loss: 0.24930869042873383\n",
      "Epoch 61: train loss: 0.301100492477417\n",
      "Epoch 61: train loss: 0.33482664823532104\n",
      "Epoch 61: train loss: 0.27124297618865967\n",
      "Epoch 61: train loss: 0.1401984691619873\n",
      "Epoch 61: train loss: 0.5407731533050537\n",
      "Epoch 61: train loss: 0.2883187532424927\n",
      "Epoch 61: train loss: 0.19291143119335175\n",
      "Epoch 61: train loss: 0.150539830327034\n",
      "Epoch 61: train loss: 0.1560460776090622\n",
      "Epoch 61: train loss: 0.4415673613548279\n",
      "Epoch 61: train loss: 0.3070787191390991\n",
      "Epoch 61: train loss: 0.13131093978881836\n",
      "Epoch 61: train loss: 0.2183043360710144\n",
      "Epoch 61: train loss: 0.2321644127368927\n",
      "Epoch 61: train loss: 0.2597719430923462\n",
      "Epoch 61: train loss: 0.16426238417625427\n",
      "Epoch 61: train loss: 0.2489943951368332\n",
      "Epoch 61: train loss: 0.13698630034923553\n",
      "Epoch 61: train loss: 0.17176416516304016\n",
      "Epoch 61: train loss: 0.42022740840911865\n",
      "Epoch 61: train loss: 0.24641156196594238\n",
      "Epoch 61: train loss: 0.25521373748779297\n",
      "Epoch 61: train loss: 0.16452346742153168\n",
      "Epoch 61: train loss: 0.24122750759124756\n",
      "Epoch 61: train loss: 0.37165382504463196\n",
      "Epoch 61: train loss: 0.07890409231185913\n",
      "Epoch 61: train loss: 0.3388696312904358\n",
      "Epoch 61: train loss: 0.13312341272830963\n",
      "Epoch 61: train loss: 0.18763592839241028\n",
      "Epoch 61: train loss: 0.29342204332351685\n",
      "Epoch 61: train loss: 0.3417472541332245\n",
      "Epoch 61: train loss: 0.3308151066303253\n",
      "Epoch 61: train loss: 0.20201946794986725\n",
      "Epoch 61: train loss: 0.3997120261192322\n",
      "Epoch 61: train loss: 0.10273933410644531\n",
      "Epoch 61: train loss: 0.37387657165527344\n",
      "Epoch 61: train loss: 0.09453398734331131\n",
      "Epoch 61: train loss: 0.3211919665336609\n",
      "Epoch 61: train loss: 0.3601284921169281\n",
      "Epoch 61: train loss: 0.25583603978157043\n",
      "Epoch 61: train loss: 0.12211671471595764\n",
      "Epoch 61: train loss: 0.12193205952644348\n",
      "Epoch 62: train loss: 0.23834916949272156\n",
      "Epoch 62: train loss: 0.17341312766075134\n",
      "Epoch 62: train loss: 0.16871130466461182\n",
      "Epoch 62: train loss: 0.10391290485858917\n",
      "Epoch 62: train loss: 0.17125271260738373\n",
      "Epoch 62: train loss: 0.262988805770874\n",
      "Epoch 62: train loss: 0.31517547369003296\n",
      "Epoch 62: train loss: 0.19077260792255402\n",
      "Epoch 62: train loss: 0.18981555104255676\n",
      "Epoch 62: train loss: 0.36666131019592285\n",
      "Epoch 62: train loss: 0.27245309948921204\n",
      "Epoch 62: train loss: 0.40923184156417847\n",
      "Epoch 62: train loss: 0.14440958201885223\n",
      "Epoch 62: train loss: 0.5930299758911133\n",
      "Epoch 62: train loss: 0.11174730211496353\n",
      "Epoch 62: train loss: 0.09542578458786011\n",
      "Epoch 62: train loss: 0.2507494390010834\n",
      "Epoch 62: train loss: 0.2664109170436859\n",
      "Epoch 62: train loss: 0.2706756591796875\n",
      "Epoch 62: train loss: 0.2579883635044098\n",
      "Epoch 62: train loss: 0.1403503566980362\n",
      "Epoch 62: train loss: 0.28282788395881653\n",
      "Epoch 62: train loss: 0.2544202506542206\n",
      "Epoch 62: train loss: 0.3044111132621765\n",
      "Epoch 62: train loss: 0.17794539034366608\n",
      "Epoch 62: train loss: 0.17750431597232819\n",
      "Epoch 62: train loss: 0.3278237283229828\n",
      "Epoch 62: train loss: 0.18258921802043915\n",
      "Epoch 62: train loss: 0.5022408962249756\n",
      "Epoch 62: train loss: 0.14713089168071747\n",
      "Epoch 62: train loss: 0.2126924842596054\n",
      "Epoch 62: train loss: 0.33162569999694824\n",
      "Epoch 62: train loss: 0.2726457715034485\n",
      "Epoch 62: train loss: 0.2743232250213623\n",
      "Epoch 62: train loss: 0.12628497183322906\n",
      "Epoch 62: train loss: 0.28765538334846497\n",
      "Epoch 62: train loss: 0.30826324224472046\n",
      "Epoch 62: train loss: 0.4167420566082001\n",
      "Epoch 62: train loss: 0.21393078565597534\n",
      "Epoch 62: train loss: 0.20258945226669312\n",
      "Epoch 62: train loss: 0.37387245893478394\n",
      "Epoch 62: train loss: 0.3656046986579895\n",
      "Epoch 62: train loss: 0.1670849621295929\n",
      "Epoch 62: train loss: 0.13384172320365906\n",
      "Epoch 63: train loss: 0.36285504698753357\n",
      "Epoch 63: train loss: 0.17705075442790985\n",
      "Epoch 63: train loss: 0.28683051466941833\n",
      "Epoch 63: train loss: 0.11617821455001831\n",
      "Epoch 63: train loss: 0.1374967396259308\n",
      "Epoch 63: train loss: 0.18564727902412415\n",
      "Epoch 63: train loss: 0.180568128824234\n",
      "Epoch 63: train loss: 0.32977473735809326\n",
      "Epoch 63: train loss: 0.2638782560825348\n",
      "Epoch 63: train loss: 0.28767356276512146\n",
      "Epoch 63: train loss: 0.15307986736297607\n",
      "Epoch 63: train loss: 0.4274269938468933\n",
      "Epoch 63: train loss: 0.16161926090717316\n",
      "Epoch 63: train loss: 0.2264583855867386\n",
      "Epoch 63: train loss: 0.31463542580604553\n",
      "Epoch 63: train loss: 0.19626101851463318\n",
      "Epoch 63: train loss: 0.14788499474525452\n",
      "Epoch 63: train loss: 0.20844627916812897\n",
      "Epoch 63: train loss: 0.13571810722351074\n",
      "Epoch 63: train loss: 0.32430052757263184\n",
      "Epoch 63: train loss: 0.3497743010520935\n",
      "Epoch 63: train loss: 0.1796393245458603\n",
      "Epoch 63: train loss: 0.3889387547969818\n",
      "Epoch 63: train loss: 0.2296741008758545\n",
      "Epoch 63: train loss: 0.2575336694717407\n",
      "Epoch 63: train loss: 0.15424269437789917\n",
      "Epoch 63: train loss: 0.44933369755744934\n",
      "Epoch 63: train loss: 0.19733816385269165\n",
      "Epoch 63: train loss: 0.2473570853471756\n",
      "Epoch 63: train loss: 0.08338750898838043\n",
      "Epoch 63: train loss: 0.5410763621330261\n",
      "Epoch 63: train loss: 0.16340088844299316\n",
      "Epoch 63: train loss: 0.21317283809185028\n",
      "Epoch 63: train loss: 0.3992992043495178\n",
      "Epoch 63: train loss: 0.369247168302536\n",
      "Epoch 63: train loss: 0.22805722057819366\n",
      "Epoch 63: train loss: 0.18175329267978668\n",
      "Epoch 63: train loss: 0.5229083299636841\n",
      "Epoch 63: train loss: 0.14595578610897064\n",
      "Epoch 63: train loss: 0.3145912289619446\n",
      "Epoch 63: train loss: 0.17880050837993622\n",
      "Epoch 63: train loss: 0.12459563463926315\n",
      "Epoch 63: train loss: 0.26041144132614136\n",
      "Epoch 63: train loss: 0.1655164659023285\n",
      "Epoch 64: train loss: 0.19889138638973236\n",
      "Epoch 64: train loss: 0.24718870222568512\n",
      "Epoch 64: train loss: 0.12776120007038116\n",
      "Epoch 64: train loss: 0.48452284932136536\n",
      "Epoch 64: train loss: 0.3233463168144226\n",
      "Epoch 64: train loss: 0.1323636919260025\n",
      "Epoch 64: train loss: 0.38322457671165466\n",
      "Epoch 64: train loss: 0.13899727165699005\n",
      "Epoch 64: train loss: 0.20173734426498413\n",
      "Epoch 64: train loss: 0.3142336905002594\n",
      "Epoch 64: train loss: 0.2742860019207001\n",
      "Epoch 64: train loss: 0.23913291096687317\n",
      "Epoch 64: train loss: 0.2003708779811859\n",
      "Epoch 64: train loss: 0.345563679933548\n",
      "Epoch 64: train loss: 0.16288228332996368\n",
      "Epoch 64: train loss: 0.42505887150764465\n",
      "Epoch 64: train loss: 0.3035556972026825\n",
      "Epoch 64: train loss: 0.21349114179611206\n",
      "Epoch 64: train loss: 0.15375478565692902\n",
      "Epoch 64: train loss: 0.13731016218662262\n",
      "Epoch 64: train loss: 0.23526382446289062\n",
      "Epoch 64: train loss: 0.23021623492240906\n",
      "Epoch 64: train loss: 0.17877839505672455\n",
      "Epoch 64: train loss: 0.25594374537467957\n",
      "Epoch 64: train loss: 0.2973228991031647\n",
      "Epoch 64: train loss: 0.10251884162425995\n",
      "Epoch 64: train loss: 0.19311094284057617\n",
      "Epoch 64: train loss: 0.1838686615228653\n",
      "Epoch 64: train loss: 0.34570711851119995\n",
      "Epoch 64: train loss: 0.27908363938331604\n",
      "Epoch 64: train loss: 0.25253114104270935\n",
      "Epoch 64: train loss: 0.12981271743774414\n",
      "Epoch 64: train loss: 0.15830819308757782\n",
      "Epoch 64: train loss: 0.385185569524765\n",
      "Epoch 64: train loss: 0.2195533812046051\n",
      "Epoch 64: train loss: 0.3692950904369354\n",
      "Epoch 64: train loss: 0.3681323826313019\n",
      "Epoch 64: train loss: 0.3319719135761261\n",
      "Epoch 64: train loss: 0.4490322768688202\n",
      "Epoch 64: train loss: 0.23170426487922668\n",
      "Epoch 64: train loss: 0.22522178292274475\n",
      "Epoch 64: train loss: 0.1994067132472992\n",
      "Epoch 64: train loss: 0.16655895113945007\n",
      "Epoch 64: train loss: 0.2366752326488495\n",
      "Epoch 65: train loss: 0.32631123065948486\n",
      "Epoch 65: train loss: 0.1920260339975357\n",
      "Epoch 65: train loss: 0.2675696909427643\n",
      "Epoch 65: train loss: 0.2280614823102951\n",
      "Epoch 65: train loss: 0.2980915606021881\n",
      "Epoch 65: train loss: 0.17872454226016998\n",
      "Epoch 65: train loss: 0.17821945250034332\n",
      "Epoch 65: train loss: 0.36359846591949463\n",
      "Epoch 65: train loss: 0.18653325736522675\n",
      "Epoch 65: train loss: 0.27142488956451416\n",
      "Epoch 65: train loss: 0.2798185646533966\n",
      "Epoch 65: train loss: 0.2890114486217499\n",
      "Epoch 65: train loss: 0.24400119483470917\n",
      "Epoch 65: train loss: 0.26902031898498535\n",
      "Epoch 65: train loss: 0.32745790481567383\n",
      "Epoch 65: train loss: 0.3576149642467499\n",
      "Epoch 65: train loss: 0.0931302160024643\n",
      "Epoch 65: train loss: 0.12287668883800507\n",
      "Epoch 65: train loss: 0.2162589281797409\n",
      "Epoch 65: train loss: 0.1778552085161209\n",
      "Epoch 65: train loss: 0.2750948965549469\n",
      "Epoch 65: train loss: 0.21253527700901031\n",
      "Epoch 65: train loss: 0.32983559370040894\n",
      "Epoch 65: train loss: 0.09746269136667252\n",
      "Epoch 65: train loss: 0.21431605517864227\n",
      "Epoch 65: train loss: 0.22394326329231262\n",
      "Epoch 65: train loss: 0.3576487898826599\n",
      "Epoch 65: train loss: 0.14725373685359955\n",
      "Epoch 65: train loss: 0.3209633529186249\n",
      "Epoch 65: train loss: 0.186701238155365\n",
      "Epoch 65: train loss: 0.147483229637146\n",
      "Epoch 65: train loss: 0.43848729133605957\n",
      "Epoch 65: train loss: 0.274516224861145\n",
      "Epoch 65: train loss: 0.5464129447937012\n",
      "Epoch 65: train loss: 0.19577059149742126\n",
      "Epoch 65: train loss: 0.27593880891799927\n",
      "Epoch 65: train loss: 0.2014574110507965\n",
      "Epoch 65: train loss: 0.2269868701696396\n",
      "Epoch 65: train loss: 0.32682567834854126\n",
      "Epoch 65: train loss: 0.20365634560585022\n",
      "Epoch 65: train loss: 0.21951216459274292\n",
      "Epoch 65: train loss: 0.2408471405506134\n",
      "Epoch 65: train loss: 0.18425096571445465\n",
      "Epoch 65: train loss: 0.32569125294685364\n",
      "Epoch 66: train loss: 0.5402677655220032\n",
      "Epoch 66: train loss: 0.17349280416965485\n",
      "Epoch 66: train loss: 0.1636643409729004\n",
      "Epoch 66: train loss: 0.22612926363945007\n",
      "Epoch 66: train loss: 0.11407619714736938\n",
      "Epoch 66: train loss: 0.20245292782783508\n",
      "Epoch 66: train loss: 0.17513613402843475\n",
      "Epoch 66: train loss: 0.2843458354473114\n",
      "Epoch 66: train loss: 0.28755855560302734\n",
      "Epoch 66: train loss: 0.5021846890449524\n",
      "Epoch 66: train loss: 0.22611844539642334\n",
      "Epoch 66: train loss: 0.12520678341388702\n",
      "Epoch 66: train loss: 0.23912805318832397\n",
      "Epoch 66: train loss: 0.24936829507350922\n",
      "Epoch 66: train loss: 0.07711713016033173\n",
      "Epoch 66: train loss: 0.3631192743778229\n",
      "Epoch 66: train loss: 0.050484903156757355\n",
      "Epoch 66: train loss: 0.4538145363330841\n",
      "Epoch 66: train loss: 0.12724442780017853\n",
      "Epoch 66: train loss: 0.24454669654369354\n",
      "Epoch 66: train loss: 0.15648184716701508\n",
      "Epoch 66: train loss: 0.3286770284175873\n",
      "Epoch 66: train loss: 0.13359682261943817\n",
      "Epoch 66: train loss: 0.17263829708099365\n",
      "Epoch 66: train loss: 0.22832635045051575\n",
      "Epoch 66: train loss: 0.05375555157661438\n",
      "Epoch 66: train loss: 0.1537780910730362\n",
      "Epoch 66: train loss: 0.28821930289268494\n",
      "Epoch 66: train loss: 0.2164677083492279\n",
      "Epoch 66: train loss: 0.17284880578517914\n",
      "Epoch 66: train loss: 0.22251184284687042\n",
      "Epoch 66: train loss: 0.3546355962753296\n",
      "Epoch 66: train loss: 0.4634510576725006\n",
      "Epoch 66: train loss: 0.19652040302753448\n",
      "Epoch 66: train loss: 0.33535072207450867\n",
      "Epoch 66: train loss: 0.2779381275177002\n",
      "Epoch 66: train loss: 0.2987729609012604\n",
      "Epoch 66: train loss: 0.3299126923084259\n",
      "Epoch 66: train loss: 0.21736939251422882\n",
      "Epoch 66: train loss: 0.2623557150363922\n",
      "Epoch 66: train loss: 0.4220072031021118\n",
      "Epoch 66: train loss: 0.19859619438648224\n",
      "Epoch 66: train loss: 0.2622997462749481\n",
      "Epoch 66: train loss: 0.3399088680744171\n",
      "Epoch 67: train loss: 0.28150632977485657\n",
      "Epoch 67: train loss: 0.3174169659614563\n",
      "Epoch 67: train loss: 0.21249088644981384\n",
      "Epoch 67: train loss: 0.29918381571769714\n",
      "Epoch 67: train loss: 0.40868690609931946\n",
      "Epoch 67: train loss: 0.243536114692688\n",
      "Epoch 67: train loss: 0.1498957872390747\n",
      "Epoch 67: train loss: 0.23464219272136688\n",
      "Epoch 67: train loss: 0.12820099294185638\n",
      "Epoch 67: train loss: 0.2601938545703888\n",
      "Epoch 67: train loss: 0.42087075114250183\n",
      "Epoch 67: train loss: 0.11746982485055923\n",
      "Epoch 67: train loss: 0.22070170938968658\n",
      "Epoch 67: train loss: 0.43680375814437866\n",
      "Epoch 67: train loss: 0.18927156925201416\n",
      "Epoch 67: train loss: 0.48037824034690857\n",
      "Epoch 67: train loss: 0.37030714750289917\n",
      "Epoch 67: train loss: 0.614205539226532\n",
      "Epoch 67: train loss: 0.22205325961112976\n",
      "Epoch 67: train loss: 0.16712458431720734\n",
      "Epoch 67: train loss: 0.17037898302078247\n",
      "Epoch 67: train loss: 0.19653356075286865\n",
      "Epoch 67: train loss: 0.3642815947532654\n",
      "Epoch 67: train loss: 0.2465120255947113\n",
      "Epoch 67: train loss: 0.14776015281677246\n",
      "Epoch 67: train loss: 0.17961065471172333\n",
      "Epoch 67: train loss: 0.2545807659626007\n",
      "Epoch 67: train loss: 0.23094874620437622\n",
      "Epoch 67: train loss: 0.1910071074962616\n",
      "Epoch 67: train loss: 0.34876707196235657\n",
      "Epoch 67: train loss: 0.3824007213115692\n",
      "Epoch 67: train loss: 0.34309718012809753\n",
      "Epoch 67: train loss: 0.3791085481643677\n",
      "Epoch 67: train loss: 0.1716746985912323\n",
      "Epoch 67: train loss: 0.28335151076316833\n",
      "Epoch 67: train loss: 0.09254703670740128\n",
      "Epoch 67: train loss: 0.07812453806400299\n",
      "Epoch 67: train loss: 0.11174269765615463\n",
      "Epoch 67: train loss: 0.14668266475200653\n",
      "Epoch 67: train loss: 0.24809227883815765\n",
      "Epoch 67: train loss: 0.1193891242146492\n",
      "Epoch 67: train loss: 0.3758508563041687\n",
      "Epoch 67: train loss: 0.18949535489082336\n",
      "Epoch 67: train loss: 0.15467561781406403\n",
      "Epoch 68: train loss: 0.512291431427002\n",
      "Epoch 68: train loss: 0.11448091268539429\n",
      "Epoch 68: train loss: 0.5053606629371643\n",
      "Epoch 68: train loss: 0.20591488480567932\n",
      "Epoch 68: train loss: 0.1758032590150833\n",
      "Epoch 68: train loss: 0.4883440136909485\n",
      "Epoch 68: train loss: 0.2909621596336365\n",
      "Epoch 68: train loss: 0.22416768968105316\n",
      "Epoch 68: train loss: 0.2986907958984375\n",
      "Epoch 68: train loss: 0.09442567080259323\n",
      "Epoch 68: train loss: 0.14687669277191162\n",
      "Epoch 68: train loss: 0.5480431318283081\n",
      "Epoch 68: train loss: 0.3864123821258545\n",
      "Epoch 68: train loss: 0.1852256953716278\n",
      "Epoch 68: train loss: 0.24014467000961304\n",
      "Epoch 68: train loss: 0.21127884089946747\n",
      "Epoch 68: train loss: 0.3335179388523102\n",
      "Epoch 68: train loss: 0.21125978231430054\n",
      "Epoch 68: train loss: 0.25225457549095154\n",
      "Epoch 68: train loss: 0.19484326243400574\n",
      "Epoch 68: train loss: 0.3225410580635071\n",
      "Epoch 68: train loss: 0.335422545671463\n",
      "Epoch 68: train loss: 0.20512160658836365\n",
      "Epoch 68: train loss: 0.2860526740550995\n",
      "Epoch 68: train loss: 0.1311623603105545\n",
      "Epoch 68: train loss: 0.21745844185352325\n",
      "Epoch 68: train loss: 0.23725824058055878\n",
      "Epoch 68: train loss: 0.09645037353038788\n",
      "Epoch 68: train loss: 0.40150541067123413\n",
      "Epoch 68: train loss: 0.2532869875431061\n",
      "Epoch 68: train loss: 0.04244675487279892\n",
      "Epoch 68: train loss: 0.18937048316001892\n",
      "Epoch 68: train loss: 0.1093461886048317\n",
      "Epoch 68: train loss: 0.11110850423574448\n",
      "Epoch 68: train loss: 0.2859135866165161\n",
      "Epoch 68: train loss: 0.3792455792427063\n",
      "Epoch 68: train loss: 0.06757837533950806\n",
      "Epoch 68: train loss: 0.4825519919395447\n",
      "Epoch 68: train loss: 0.05960165336728096\n",
      "Epoch 68: train loss: 0.2733241319656372\n",
      "Epoch 68: train loss: 0.15840093791484833\n",
      "Epoch 68: train loss: 0.23278552293777466\n",
      "Epoch 68: train loss: 0.22154349088668823\n",
      "Epoch 68: train loss: 0.39048004150390625\n",
      "Epoch 69: train loss: 0.2117074429988861\n",
      "Epoch 69: train loss: 0.14071454107761383\n",
      "Epoch 69: train loss: 0.28250497579574585\n",
      "Epoch 69: train loss: 0.218816339969635\n",
      "Epoch 69: train loss: 0.11376123130321503\n",
      "Epoch 69: train loss: 0.0744251236319542\n",
      "Epoch 69: train loss: 0.20520447194576263\n",
      "Epoch 69: train loss: 0.31139853596687317\n",
      "Epoch 69: train loss: 0.16553890705108643\n",
      "Epoch 69: train loss: 0.1658884584903717\n",
      "Epoch 69: train loss: 0.06198229268193245\n",
      "Epoch 69: train loss: 0.5334291458129883\n",
      "Epoch 69: train loss: 0.25826922059059143\n",
      "Epoch 69: train loss: 0.15460284054279327\n",
      "Epoch 69: train loss: 0.14557522535324097\n",
      "Epoch 69: train loss: 0.07818321883678436\n",
      "Epoch 69: train loss: 0.1315976232290268\n",
      "Epoch 69: train loss: 0.1898057609796524\n",
      "Epoch 69: train loss: 0.4760170578956604\n",
      "Epoch 69: train loss: 0.27541229128837585\n",
      "Epoch 69: train loss: 0.3343753218650818\n",
      "Epoch 69: train loss: 0.19717685878276825\n",
      "Epoch 69: train loss: 0.2955624461174011\n",
      "Epoch 69: train loss: 0.3299765884876251\n",
      "Epoch 69: train loss: 0.2652372121810913\n",
      "Epoch 69: train loss: 0.36531147360801697\n",
      "Epoch 69: train loss: 0.2645602524280548\n",
      "Epoch 69: train loss: 0.24336133897304535\n",
      "Epoch 69: train loss: 0.36612269282341003\n",
      "Epoch 69: train loss: 0.39799967408180237\n",
      "Epoch 69: train loss: 0.18349480628967285\n",
      "Epoch 69: train loss: 0.2675316333770752\n",
      "Epoch 69: train loss: 0.32051682472229004\n",
      "Epoch 69: train loss: 0.16840912401676178\n",
      "Epoch 69: train loss: 0.09460565447807312\n",
      "Epoch 69: train loss: 0.2599124610424042\n",
      "Epoch 69: train loss: 0.1546798050403595\n",
      "Epoch 69: train loss: 0.33074477314949036\n",
      "Epoch 69: train loss: 0.3602098524570465\n",
      "Epoch 69: train loss: 0.2214364856481552\n",
      "Epoch 69: train loss: 0.3827381134033203\n",
      "Epoch 69: train loss: 0.40608546137809753\n",
      "Epoch 69: train loss: 0.28925150632858276\n",
      "Epoch 69: train loss: 0.23990398645401\n",
      "Epoch 70: train loss: 0.43990522623062134\n",
      "Epoch 70: train loss: 0.17820335924625397\n",
      "Epoch 70: train loss: 0.13831256330013275\n",
      "Epoch 70: train loss: 0.1992131620645523\n",
      "Epoch 70: train loss: 0.3908219039440155\n",
      "Epoch 70: train loss: 0.1897502839565277\n",
      "Epoch 70: train loss: 0.3948880136013031\n",
      "Epoch 70: train loss: 0.294563353061676\n",
      "Epoch 70: train loss: 0.19893519580364227\n",
      "Epoch 70: train loss: 0.24563248455524445\n",
      "Epoch 70: train loss: 0.2578931152820587\n",
      "Epoch 70: train loss: 0.13115666806697845\n",
      "Epoch 70: train loss: 0.13683129847049713\n",
      "Epoch 70: train loss: 0.16923190653324127\n",
      "Epoch 70: train loss: 0.21251878142356873\n",
      "Epoch 70: train loss: 0.379826158285141\n",
      "Epoch 70: train loss: 0.19230230152606964\n",
      "Epoch 70: train loss: 0.3890320956707001\n",
      "Epoch 70: train loss: 0.07016219943761826\n",
      "Epoch 70: train loss: 0.31124013662338257\n",
      "Epoch 70: train loss: 0.39542028307914734\n",
      "Epoch 70: train loss: 0.045717108994722366\n",
      "Epoch 70: train loss: 0.28376659750938416\n",
      "Epoch 70: train loss: 0.20504212379455566\n",
      "Epoch 70: train loss: 0.18810226023197174\n",
      "Epoch 70: train loss: 0.20611560344696045\n",
      "Epoch 70: train loss: 0.36699196696281433\n",
      "Epoch 70: train loss: 0.25010520219802856\n",
      "Epoch 70: train loss: 0.15845943987369537\n",
      "Epoch 70: train loss: 0.14828045666217804\n",
      "Epoch 70: train loss: 0.27788326144218445\n",
      "Epoch 70: train loss: 0.4109632074832916\n",
      "Epoch 70: train loss: 0.13229252398014069\n",
      "Epoch 70: train loss: 0.4323657155036926\n",
      "Epoch 70: train loss: 0.31742385029792786\n",
      "Epoch 70: train loss: 0.2736516296863556\n",
      "Epoch 70: train loss: 0.22681741416454315\n",
      "Epoch 70: train loss: 0.18522119522094727\n",
      "Epoch 70: train loss: 0.28901687264442444\n",
      "Epoch 70: train loss: 0.15359313786029816\n",
      "Epoch 70: train loss: 0.24495887756347656\n",
      "Epoch 70: train loss: 0.2616014778614044\n",
      "Epoch 70: train loss: 0.18585771322250366\n",
      "Epoch 70: train loss: 0.4292006492614746\n",
      "Epoch 71: train loss: 0.12399442493915558\n",
      "Epoch 71: train loss: 0.31811419129371643\n",
      "Epoch 71: train loss: 0.3141905665397644\n",
      "Epoch 71: train loss: 0.2985268533229828\n",
      "Epoch 71: train loss: 0.3722496032714844\n",
      "Epoch 71: train loss: 0.27917563915252686\n",
      "Epoch 71: train loss: 0.17569707334041595\n",
      "Epoch 71: train loss: 0.12606050074100494\n",
      "Epoch 71: train loss: 0.060370754450559616\n",
      "Epoch 71: train loss: 0.4089629650115967\n",
      "Epoch 71: train loss: 0.31574341654777527\n",
      "Epoch 71: train loss: 0.16814741492271423\n",
      "Epoch 71: train loss: 0.37946459650993347\n",
      "Epoch 71: train loss: 0.08877328038215637\n",
      "Epoch 71: train loss: 0.35855790972709656\n",
      "Epoch 71: train loss: 0.12807273864746094\n",
      "Epoch 71: train loss: 0.2969174385070801\n",
      "Epoch 71: train loss: 0.30728352069854736\n",
      "Epoch 71: train loss: 0.3185741901397705\n",
      "Epoch 71: train loss: 0.341430127620697\n",
      "Epoch 71: train loss: 0.2940980792045593\n",
      "Epoch 71: train loss: 0.22420112788677216\n",
      "Epoch 71: train loss: 0.18857581913471222\n",
      "Epoch 71: train loss: 0.22953878343105316\n",
      "Epoch 71: train loss: 0.2979552447795868\n",
      "Epoch 71: train loss: 0.36850467324256897\n",
      "Epoch 71: train loss: 0.16083447635173798\n",
      "Epoch 71: train loss: 0.23099328577518463\n",
      "Epoch 71: train loss: 0.2841755449771881\n",
      "Epoch 71: train loss: 0.26762068271636963\n",
      "Epoch 71: train loss: 0.25148093700408936\n",
      "Epoch 71: train loss: 0.18125326931476593\n",
      "Epoch 71: train loss: 0.24958500266075134\n",
      "Epoch 71: train loss: 0.22313234210014343\n",
      "Epoch 71: train loss: 0.1326131969690323\n",
      "Epoch 71: train loss: 0.17798089981079102\n",
      "Epoch 71: train loss: 0.2651630938053131\n",
      "Epoch 71: train loss: 0.21961699426174164\n",
      "Epoch 71: train loss: 0.17369675636291504\n",
      "Epoch 71: train loss: 0.2989462912082672\n",
      "Epoch 71: train loss: 0.1304498165845871\n",
      "Epoch 71: train loss: 0.28536027669906616\n",
      "Epoch 71: train loss: 0.5209065079689026\n",
      "Epoch 71: train loss: 0.1669633537530899\n",
      "Epoch 72: train loss: 0.14870689809322357\n",
      "Epoch 72: train loss: 0.33952975273132324\n",
      "Epoch 72: train loss: 0.09864957630634308\n",
      "Epoch 72: train loss: 0.34613049030303955\n",
      "Epoch 72: train loss: 0.2885889410972595\n",
      "Epoch 72: train loss: 0.1276545226573944\n",
      "Epoch 72: train loss: 0.29517748951911926\n",
      "Epoch 72: train loss: 0.23946616053581238\n",
      "Epoch 72: train loss: 0.3212544023990631\n",
      "Epoch 72: train loss: 0.19004805386066437\n",
      "Epoch 72: train loss: 0.21499834954738617\n",
      "Epoch 72: train loss: 0.3603580892086029\n",
      "Epoch 72: train loss: 0.195210263133049\n",
      "Epoch 72: train loss: 0.5526206493377686\n",
      "Epoch 72: train loss: 0.2706569731235504\n",
      "Epoch 72: train loss: 0.1884322315454483\n",
      "Epoch 72: train loss: 0.26482972502708435\n",
      "Epoch 72: train loss: 0.2203110009431839\n",
      "Epoch 72: train loss: 0.3270825445652008\n",
      "Epoch 72: train loss: 0.25651323795318604\n",
      "Epoch 72: train loss: 0.3124917447566986\n",
      "Epoch 72: train loss: 0.5308911800384521\n",
      "Epoch 72: train loss: 0.11606553196907043\n",
      "Epoch 72: train loss: 0.19092519581317902\n",
      "Epoch 72: train loss: 0.20940564572811127\n",
      "Epoch 72: train loss: 0.2975378632545471\n",
      "Epoch 72: train loss: 0.10272108763456345\n",
      "Epoch 72: train loss: 0.15617161989212036\n",
      "Epoch 72: train loss: 0.14728319644927979\n",
      "Epoch 72: train loss: 0.17368030548095703\n",
      "Epoch 72: train loss: 0.4052850008010864\n",
      "Epoch 72: train loss: 0.14927305281162262\n",
      "Epoch 72: train loss: 0.36514121294021606\n",
      "Epoch 72: train loss: 0.2652016282081604\n",
      "Epoch 72: train loss: 0.2368311733007431\n",
      "Epoch 72: train loss: 0.1825377643108368\n",
      "Epoch 72: train loss: 0.08866055309772491\n",
      "Epoch 72: train loss: 0.23936083912849426\n",
      "Epoch 72: train loss: 0.4349864721298218\n",
      "Epoch 72: train loss: 0.22758106887340546\n",
      "Epoch 72: train loss: 0.19006690382957458\n",
      "Epoch 72: train loss: 0.13576965034008026\n",
      "Epoch 72: train loss: 0.2619830071926117\n",
      "Epoch 72: train loss: 0.2811601161956787\n",
      "Epoch 73: train loss: 0.26623064279556274\n",
      "Epoch 73: train loss: 0.24747779965400696\n",
      "Epoch 73: train loss: 0.3025300204753876\n",
      "Epoch 73: train loss: 0.12307360023260117\n",
      "Epoch 73: train loss: 0.38973674178123474\n",
      "Epoch 73: train loss: 0.21693772077560425\n",
      "Epoch 73: train loss: 0.18582655489444733\n",
      "Epoch 73: train loss: 0.2658237814903259\n",
      "Epoch 73: train loss: 0.26326891779899597\n",
      "Epoch 73: train loss: 0.22478137910366058\n",
      "Epoch 73: train loss: 0.23377694189548492\n",
      "Epoch 73: train loss: 0.35999596118927\n",
      "Epoch 73: train loss: 0.18425200879573822\n",
      "Epoch 73: train loss: 0.23594160377979279\n",
      "Epoch 73: train loss: 0.28290605545043945\n",
      "Epoch 73: train loss: 0.3135566711425781\n",
      "Epoch 73: train loss: 0.2497330904006958\n",
      "Epoch 73: train loss: 0.23703300952911377\n",
      "Epoch 73: train loss: 0.15693078935146332\n",
      "Epoch 73: train loss: 0.42704761028289795\n",
      "Epoch 73: train loss: 0.10759146511554718\n",
      "Epoch 73: train loss: 0.10101622343063354\n",
      "Epoch 73: train loss: 0.42277559638023376\n",
      "Epoch 73: train loss: 0.340793251991272\n",
      "Epoch 73: train loss: 0.24861009418964386\n",
      "Epoch 73: train loss: 0.15568016469478607\n",
      "Epoch 73: train loss: 0.36522236466407776\n",
      "Epoch 73: train loss: 0.07198556512594223\n",
      "Epoch 73: train loss: 0.2610510289669037\n",
      "Epoch 73: train loss: 0.22289152443408966\n",
      "Epoch 73: train loss: 0.35972851514816284\n",
      "Epoch 73: train loss: 0.21098288893699646\n",
      "Epoch 73: train loss: 0.2734842002391815\n",
      "Epoch 73: train loss: 0.21453894674777985\n",
      "Epoch 73: train loss: 0.31992030143737793\n",
      "Epoch 73: train loss: 0.11326803267002106\n",
      "Epoch 73: train loss: 0.27325209975242615\n",
      "Epoch 73: train loss: 0.3582838177680969\n",
      "Epoch 73: train loss: 0.21921463310718536\n",
      "Epoch 73: train loss: 0.13794219493865967\n",
      "Epoch 73: train loss: 0.23273415863513947\n",
      "Epoch 73: train loss: 0.2149137705564499\n",
      "Epoch 73: train loss: 0.24252431094646454\n",
      "Epoch 73: train loss: 0.32184746861457825\n",
      "Epoch 74: train loss: 0.17790347337722778\n",
      "Epoch 74: train loss: 0.4497472047805786\n",
      "Epoch 74: train loss: 0.20784959197044373\n",
      "Epoch 74: train loss: 0.21599030494689941\n",
      "Epoch 74: train loss: 0.18923209607601166\n",
      "Epoch 74: train loss: 0.16639485955238342\n",
      "Epoch 74: train loss: 0.3957020342350006\n",
      "Epoch 74: train loss: 0.25756680965423584\n",
      "Epoch 74: train loss: 0.31659674644470215\n",
      "Epoch 74: train loss: 0.3798276484012604\n",
      "Epoch 74: train loss: 0.13454362750053406\n",
      "Epoch 74: train loss: 0.19380971789360046\n",
      "Epoch 74: train loss: 0.23119771480560303\n",
      "Epoch 74: train loss: 0.34352320432662964\n",
      "Epoch 74: train loss: 0.11905743926763535\n",
      "Epoch 74: train loss: 0.19658680260181427\n",
      "Epoch 74: train loss: 0.4286370873451233\n",
      "Epoch 74: train loss: 0.07513280212879181\n",
      "Epoch 74: train loss: 0.5411399602890015\n",
      "Epoch 74: train loss: 0.2656572461128235\n",
      "Epoch 74: train loss: 0.1626010239124298\n",
      "Epoch 74: train loss: 0.16481494903564453\n",
      "Epoch 74: train loss: 0.14112643897533417\n",
      "Epoch 74: train loss: 0.22562839090824127\n",
      "Epoch 74: train loss: 0.14016054570674896\n",
      "Epoch 74: train loss: 0.19553987681865692\n",
      "Epoch 74: train loss: 0.20762385427951813\n",
      "Epoch 74: train loss: 0.22265517711639404\n",
      "Epoch 74: train loss: 0.24436505138874054\n",
      "Epoch 74: train loss: 0.23352481424808502\n",
      "Epoch 74: train loss: 0.3197806775569916\n",
      "Epoch 74: train loss: 0.2858085632324219\n",
      "Epoch 74: train loss: 0.2748534381389618\n",
      "Epoch 74: train loss: 0.2919900417327881\n",
      "Epoch 74: train loss: 0.26387155055999756\n",
      "Epoch 74: train loss: 0.1928204596042633\n",
      "Epoch 74: train loss: 0.1375609189271927\n",
      "Epoch 74: train loss: 0.3841770887374878\n",
      "Epoch 74: train loss: 0.20049530267715454\n",
      "Epoch 74: train loss: 0.2603508532047272\n",
      "Epoch 74: train loss: 0.16151872277259827\n",
      "Epoch 74: train loss: 0.47278323769569397\n",
      "Epoch 74: train loss: 0.1551031619310379\n",
      "Epoch 74: train loss: 0.3115050494670868\n",
      "Epoch 75: train loss: 0.36897823214530945\n",
      "Epoch 75: train loss: 0.18277356028556824\n",
      "Epoch 75: train loss: 0.2573334574699402\n",
      "Epoch 75: train loss: 0.17860323190689087\n",
      "Epoch 75: train loss: 0.18591557443141937\n",
      "Epoch 75: train loss: 0.09339682012796402\n",
      "Epoch 75: train loss: 0.2805299162864685\n",
      "Epoch 75: train loss: 0.3381060063838959\n",
      "Epoch 75: train loss: 0.20081281661987305\n",
      "Epoch 75: train loss: 0.22912243008613586\n",
      "Epoch 75: train loss: 0.21267277002334595\n",
      "Epoch 75: train loss: 0.20738400518894196\n",
      "Epoch 75: train loss: 0.2190406620502472\n",
      "Epoch 75: train loss: 0.2740507423877716\n",
      "Epoch 75: train loss: 0.28011295199394226\n",
      "Epoch 75: train loss: 0.31142890453338623\n",
      "Epoch 75: train loss: 0.15661612153053284\n",
      "Epoch 75: train loss: 0.28076305985450745\n",
      "Epoch 75: train loss: 0.16857722401618958\n",
      "Epoch 75: train loss: 0.10057533532381058\n",
      "Epoch 75: train loss: 0.30352771282196045\n",
      "Epoch 75: train loss: 0.5423653721809387\n",
      "Epoch 75: train loss: 0.05852733552455902\n",
      "Epoch 75: train loss: 0.31671127676963806\n",
      "Epoch 75: train loss: 0.12682156264781952\n",
      "Epoch 75: train loss: 0.23795117437839508\n",
      "Epoch 75: train loss: 0.2355894148349762\n",
      "Epoch 75: train loss: 0.2055698037147522\n",
      "Epoch 75: train loss: 0.28511276841163635\n",
      "Epoch 75: train loss: 0.14811864495277405\n",
      "Epoch 75: train loss: 0.15431316196918488\n",
      "Epoch 75: train loss: 0.29771098494529724\n",
      "Epoch 75: train loss: 0.08923203498125076\n",
      "Epoch 75: train loss: 0.479991614818573\n",
      "Epoch 75: train loss: 0.14984573423862457\n",
      "Epoch 75: train loss: 0.21447034180164337\n",
      "Epoch 75: train loss: 0.45892277359962463\n",
      "Epoch 75: train loss: 0.3583126664161682\n",
      "Epoch 75: train loss: 0.36341869831085205\n",
      "Epoch 75: train loss: 0.2754375636577606\n",
      "Epoch 75: train loss: 0.24415187537670135\n",
      "Epoch 75: train loss: 0.2902703285217285\n",
      "Epoch 75: train loss: 0.29587435722351074\n",
      "Epoch 75: train loss: 0.2733738124370575\n",
      "Epoch 76: train loss: 0.2119620144367218\n",
      "Epoch 76: train loss: 0.5665507316589355\n",
      "Epoch 76: train loss: 0.468025267124176\n",
      "Epoch 76: train loss: 0.22950929403305054\n",
      "Epoch 76: train loss: 0.37554216384887695\n",
      "Epoch 76: train loss: 0.32608482241630554\n",
      "Epoch 76: train loss: 0.30467328429222107\n",
      "Epoch 76: train loss: 0.1432681530714035\n",
      "Epoch 76: train loss: 0.30483123660087585\n",
      "Epoch 76: train loss: 0.21072562038898468\n",
      "Epoch 76: train loss: 0.33926519751548767\n",
      "Epoch 76: train loss: 0.21907681226730347\n",
      "Epoch 76: train loss: 0.1968068927526474\n",
      "Epoch 76: train loss: 0.16671885550022125\n",
      "Epoch 76: train loss: 0.46040263772010803\n",
      "Epoch 76: train loss: 0.22857193648815155\n",
      "Epoch 76: train loss: 0.21624478697776794\n",
      "Epoch 76: train loss: 0.3249449133872986\n",
      "Epoch 76: train loss: 0.2438361644744873\n",
      "Epoch 76: train loss: 0.21276718378067017\n",
      "Epoch 76: train loss: 0.16954989731311798\n",
      "Epoch 76: train loss: 0.21667605638504028\n",
      "Epoch 76: train loss: 0.08431892842054367\n",
      "Epoch 76: train loss: 0.5170479416847229\n",
      "Epoch 76: train loss: 0.1489943563938141\n",
      "Epoch 76: train loss: 0.08834482729434967\n",
      "Epoch 76: train loss: 0.11517810076475143\n",
      "Epoch 76: train loss: 0.12435980886220932\n",
      "Epoch 76: train loss: 0.2522670030593872\n",
      "Epoch 76: train loss: 0.13170623779296875\n",
      "Epoch 76: train loss: 0.10690375417470932\n",
      "Epoch 76: train loss: 0.3047471344470978\n",
      "Epoch 76: train loss: 0.3782905042171478\n",
      "Epoch 76: train loss: 0.09937424212694168\n",
      "Epoch 76: train loss: 0.29428496956825256\n",
      "Epoch 76: train loss: 0.21437357366085052\n",
      "Epoch 76: train loss: 0.07783243060112\n",
      "Epoch 76: train loss: 0.2743171751499176\n",
      "Epoch 76: train loss: 0.1422557532787323\n",
      "Epoch 76: train loss: 0.44482821226119995\n",
      "Epoch 76: train loss: 0.3161357343196869\n",
      "Epoch 76: train loss: 0.2641400396823883\n",
      "Epoch 76: train loss: 0.28608307242393494\n",
      "Epoch 76: train loss: 0.1408546268939972\n",
      "Epoch 77: train loss: 0.2742573916912079\n",
      "Epoch 77: train loss: 0.16185413300991058\n",
      "Epoch 77: train loss: 0.25272807478904724\n",
      "Epoch 77: train loss: 0.26409539580345154\n",
      "Epoch 77: train loss: 0.26604923605918884\n",
      "Epoch 77: train loss: 0.09238535165786743\n",
      "Epoch 77: train loss: 0.11846372485160828\n",
      "Epoch 77: train loss: 0.17593923211097717\n",
      "Epoch 77: train loss: 0.15010827779769897\n",
      "Epoch 77: train loss: 0.2190186232328415\n",
      "Epoch 77: train loss: 0.4115053415298462\n",
      "Epoch 77: train loss: 0.2523330748081207\n",
      "Epoch 77: train loss: 0.21831081807613373\n",
      "Epoch 77: train loss: 0.1262396275997162\n",
      "Epoch 77: train loss: 0.3642052412033081\n",
      "Epoch 77: train loss: 0.3330015242099762\n",
      "Epoch 77: train loss: 0.1616188883781433\n",
      "Epoch 77: train loss: 0.2298462837934494\n",
      "Epoch 77: train loss: 0.20097950100898743\n",
      "Epoch 77: train loss: 0.22521378099918365\n",
      "Epoch 77: train loss: 0.2029700130224228\n",
      "Epoch 77: train loss: 0.22489282488822937\n",
      "Epoch 77: train loss: 0.29290634393692017\n",
      "Epoch 77: train loss: 0.2783055901527405\n",
      "Epoch 77: train loss: 0.07347419112920761\n",
      "Epoch 77: train loss: 0.13391216099262238\n",
      "Epoch 77: train loss: 0.14231349527835846\n",
      "Epoch 77: train loss: 0.17839133739471436\n",
      "Epoch 77: train loss: 0.26514896750450134\n",
      "Epoch 77: train loss: 0.4169919788837433\n",
      "Epoch 77: train loss: 0.4937649965286255\n",
      "Epoch 77: train loss: 0.253444641828537\n",
      "Epoch 77: train loss: 0.21245354413986206\n",
      "Epoch 77: train loss: 0.3101955056190491\n",
      "Epoch 77: train loss: 0.2903645932674408\n",
      "Epoch 77: train loss: 0.21080097556114197\n",
      "Epoch 77: train loss: 0.4082174003124237\n",
      "Epoch 77: train loss: 0.3140077590942383\n",
      "Epoch 77: train loss: 0.4332089424133301\n",
      "Epoch 77: train loss: 0.25991666316986084\n",
      "Epoch 77: train loss: 0.27042531967163086\n",
      "Epoch 77: train loss: 0.329639196395874\n",
      "Epoch 77: train loss: 0.3214539587497711\n",
      "Epoch 77: train loss: 0.19536608457565308\n",
      "Epoch 78: train loss: 0.4495709240436554\n",
      "Epoch 78: train loss: 0.27709296345710754\n",
      "Epoch 78: train loss: 0.2391139417886734\n",
      "Epoch 78: train loss: 0.25040411949157715\n",
      "Epoch 78: train loss: 0.2836257219314575\n",
      "Epoch 78: train loss: 0.2155003547668457\n",
      "Epoch 78: train loss: 0.2871593236923218\n",
      "Epoch 78: train loss: 0.3641526699066162\n",
      "Epoch 78: train loss: 0.08141772449016571\n",
      "Epoch 78: train loss: 0.27631187438964844\n",
      "Epoch 78: train loss: 0.06396037340164185\n",
      "Epoch 78: train loss: 0.37741848826408386\n",
      "Epoch 78: train loss: 0.1018073782324791\n",
      "Epoch 78: train loss: 0.3443620204925537\n",
      "Epoch 78: train loss: 0.19035778939723969\n",
      "Epoch 78: train loss: 0.15703120827674866\n",
      "Epoch 78: train loss: 0.27590593695640564\n",
      "Epoch 78: train loss: 0.1840657889842987\n",
      "Epoch 78: train loss: 0.23811089992523193\n",
      "Epoch 78: train loss: 0.4223592281341553\n",
      "Epoch 78: train loss: 0.23394006490707397\n",
      "Epoch 78: train loss: 0.18050026893615723\n",
      "Epoch 78: train loss: 0.3579149842262268\n",
      "Epoch 78: train loss: 0.2546689808368683\n",
      "Epoch 78: train loss: 0.4233189821243286\n",
      "Epoch 78: train loss: 0.20856258273124695\n",
      "Epoch 78: train loss: 0.19094504415988922\n",
      "Epoch 78: train loss: 0.38668251037597656\n",
      "Epoch 78: train loss: 0.2562275528907776\n",
      "Epoch 78: train loss: 0.25579631328582764\n",
      "Epoch 78: train loss: 0.31193289160728455\n",
      "Epoch 78: train loss: 0.10835564136505127\n",
      "Epoch 78: train loss: 0.2241002321243286\n",
      "Epoch 78: train loss: 0.20730842649936676\n",
      "Epoch 78: train loss: 0.20442351698875427\n",
      "Epoch 78: train loss: 0.09434370696544647\n",
      "Epoch 78: train loss: 0.1928815245628357\n",
      "Epoch 78: train loss: 0.2205682098865509\n",
      "Epoch 78: train loss: 0.22235378623008728\n",
      "Epoch 78: train loss: 0.1643887758255005\n",
      "Epoch 78: train loss: 0.266907662153244\n",
      "Epoch 78: train loss: 0.10510876774787903\n",
      "Epoch 78: train loss: 0.5707479119300842\n",
      "Epoch 78: train loss: 0.34044697880744934\n",
      "Epoch 79: train loss: 0.10364783555269241\n",
      "Epoch 79: train loss: 0.24494554102420807\n",
      "Epoch 79: train loss: 0.12645596265792847\n",
      "Epoch 79: train loss: 0.11145676672458649\n",
      "Epoch 79: train loss: 0.08423034846782684\n",
      "Epoch 79: train loss: 0.18869437277317047\n",
      "Epoch 79: train loss: 0.1659451425075531\n",
      "Epoch 79: train loss: 0.38089054822921753\n",
      "Epoch 79: train loss: 0.3009761869907379\n",
      "Epoch 79: train loss: 0.383351594209671\n",
      "Epoch 79: train loss: 0.3770987391471863\n",
      "Epoch 79: train loss: 0.42611607909202576\n",
      "Epoch 79: train loss: 0.1170964166522026\n",
      "Epoch 79: train loss: 0.4678002595901489\n",
      "Epoch 79: train loss: 0.1716066300868988\n",
      "Epoch 79: train loss: 0.33027249574661255\n",
      "Epoch 79: train loss: 0.12472563236951828\n",
      "Epoch 79: train loss: 0.3197079002857208\n",
      "Epoch 79: train loss: 0.2036464363336563\n",
      "Epoch 79: train loss: 0.328497976064682\n",
      "Epoch 79: train loss: 0.42007023096084595\n",
      "Epoch 79: train loss: 0.13993144035339355\n",
      "Epoch 79: train loss: 0.21473698318004608\n",
      "Epoch 79: train loss: 0.1724623441696167\n",
      "Epoch 79: train loss: 0.2702988088130951\n",
      "Epoch 79: train loss: 0.21935246884822845\n",
      "Epoch 79: train loss: 0.17538689076900482\n",
      "Epoch 79: train loss: 0.6298602223396301\n",
      "Epoch 79: train loss: 0.1804988980293274\n",
      "Epoch 79: train loss: 0.0850503221154213\n",
      "Epoch 79: train loss: 0.3307606875896454\n",
      "Epoch 79: train loss: 0.08684500306844711\n",
      "Epoch 79: train loss: 0.2389070689678192\n",
      "Epoch 79: train loss: 0.18324531614780426\n",
      "Epoch 79: train loss: 0.3697020709514618\n",
      "Epoch 79: train loss: 0.3841301202774048\n",
      "Epoch 79: train loss: 0.26347893476486206\n",
      "Epoch 79: train loss: 0.24266445636749268\n",
      "Epoch 79: train loss: 0.25648918747901917\n",
      "Epoch 79: train loss: 0.3195613920688629\n",
      "Epoch 79: train loss: 0.25374916195869446\n",
      "Epoch 79: train loss: 0.15504229068756104\n",
      "Epoch 79: train loss: 0.2691706418991089\n",
      "Epoch 79: train loss: 0.17783573269844055\n",
      "Epoch 80: train loss: 0.19298943877220154\n",
      "Epoch 80: train loss: 0.11069514602422714\n",
      "Epoch 80: train loss: 0.40416309237480164\n",
      "Epoch 80: train loss: 0.15404918789863586\n",
      "Epoch 80: train loss: 0.3021763861179352\n",
      "Epoch 80: train loss: 0.36521172523498535\n",
      "Epoch 80: train loss: 0.09925490617752075\n",
      "Epoch 80: train loss: 0.2643040418624878\n",
      "Epoch 80: train loss: 0.3471624255180359\n",
      "Epoch 80: train loss: 0.1062154471874237\n",
      "Epoch 80: train loss: 0.09803679585456848\n",
      "Epoch 80: train loss: 0.2855912446975708\n",
      "Epoch 80: train loss: 0.3588741421699524\n",
      "Epoch 80: train loss: 0.29629838466644287\n",
      "Epoch 80: train loss: 0.26492583751678467\n",
      "Epoch 80: train loss: 0.16571277379989624\n",
      "Epoch 80: train loss: 0.271517813205719\n",
      "Epoch 80: train loss: 0.4082764983177185\n",
      "Epoch 80: train loss: 0.1764737069606781\n",
      "Epoch 80: train loss: 0.26730626821517944\n",
      "Epoch 80: train loss: 0.25329601764678955\n",
      "Epoch 80: train loss: 0.48327121138572693\n",
      "Epoch 80: train loss: 0.38938355445861816\n",
      "Epoch 80: train loss: 0.3067769408226013\n",
      "Epoch 80: train loss: 0.21433298289775848\n",
      "Epoch 80: train loss: 0.31028708815574646\n",
      "Epoch 80: train loss: 0.12138320505619049\n",
      "Epoch 80: train loss: 0.32611051201820374\n",
      "Epoch 80: train loss: 0.133808895945549\n",
      "Epoch 80: train loss: 0.46466678380966187\n",
      "Epoch 80: train loss: 0.14347821474075317\n",
      "Epoch 80: train loss: 0.14514073729515076\n",
      "Epoch 80: train loss: 0.2913391590118408\n",
      "Epoch 80: train loss: 0.2968890964984894\n",
      "Epoch 80: train loss: 0.11130417883396149\n",
      "Epoch 80: train loss: 0.12690997123718262\n",
      "Epoch 80: train loss: 0.18877731263637543\n",
      "Epoch 80: train loss: 0.37097111344337463\n",
      "Epoch 80: train loss: 0.3822091817855835\n",
      "Epoch 80: train loss: 0.1524951457977295\n",
      "Epoch 80: train loss: 0.4368385374546051\n",
      "Epoch 80: train loss: 0.11494383215904236\n",
      "Epoch 80: train loss: 0.13853004574775696\n",
      "Epoch 80: train loss: 0.18516738712787628\n",
      "Epoch 81: train loss: 0.17736902832984924\n",
      "Epoch 81: train loss: 0.4353204071521759\n",
      "Epoch 81: train loss: 0.24863073229789734\n",
      "Epoch 81: train loss: 0.2165536731481552\n",
      "Epoch 81: train loss: 0.14595341682434082\n",
      "Epoch 81: train loss: 0.20935040712356567\n",
      "Epoch 81: train loss: 0.3706281781196594\n",
      "Epoch 81: train loss: 0.20381972193717957\n",
      "Epoch 81: train loss: 0.1479160487651825\n",
      "Epoch 81: train loss: 0.36004364490509033\n",
      "Epoch 81: train loss: 0.21524327993392944\n",
      "Epoch 81: train loss: 0.3543071150779724\n",
      "Epoch 81: train loss: 0.5551632642745972\n",
      "Epoch 81: train loss: 0.3079858720302582\n",
      "Epoch 81: train loss: 0.1301099956035614\n",
      "Epoch 81: train loss: 0.4446171522140503\n",
      "Epoch 81: train loss: 0.2084587961435318\n",
      "Epoch 81: train loss: 0.23945273458957672\n",
      "Epoch 81: train loss: 0.23909960687160492\n",
      "Epoch 81: train loss: 0.19992801547050476\n",
      "Epoch 81: train loss: 0.48073774576187134\n",
      "Epoch 81: train loss: 0.315784752368927\n",
      "Epoch 81: train loss: 0.14068937301635742\n",
      "Epoch 81: train loss: 0.23375266790390015\n",
      "Epoch 81: train loss: 0.20941299200057983\n",
      "Epoch 81: train loss: 0.23362037539482117\n",
      "Epoch 81: train loss: 0.09121862053871155\n",
      "Epoch 81: train loss: 0.1484154313802719\n",
      "Epoch 81: train loss: 0.22256489098072052\n",
      "Epoch 81: train loss: 0.0981503576040268\n",
      "Epoch 81: train loss: 0.5063413977622986\n",
      "Epoch 81: train loss: 0.3616083562374115\n",
      "Epoch 81: train loss: 0.3906486928462982\n",
      "Epoch 81: train loss: 0.26728981733322144\n",
      "Epoch 81: train loss: 0.4321965277194977\n",
      "Epoch 81: train loss: 0.1500224769115448\n",
      "Epoch 81: train loss: 0.07774388045072556\n",
      "Epoch 81: train loss: 0.14192067086696625\n",
      "Epoch 81: train loss: 0.14083582162857056\n",
      "Epoch 81: train loss: 0.09531046450138092\n",
      "Epoch 81: train loss: 0.23404201865196228\n",
      "Epoch 81: train loss: 0.3201306462287903\n",
      "Epoch 81: train loss: 0.06286705285310745\n",
      "Epoch 81: train loss: 0.26022595167160034\n",
      "Epoch 82: train loss: 0.23434120416641235\n",
      "Epoch 82: train loss: 0.08389090746641159\n",
      "Epoch 82: train loss: 0.190861776471138\n",
      "Epoch 82: train loss: 0.5878709554672241\n",
      "Epoch 82: train loss: 0.1558363288640976\n",
      "Epoch 82: train loss: 0.3841761350631714\n",
      "Epoch 82: train loss: 0.4417319595813751\n",
      "Epoch 82: train loss: 0.19191353023052216\n",
      "Epoch 82: train loss: 0.15091775357723236\n",
      "Epoch 82: train loss: 0.344306617975235\n",
      "Epoch 82: train loss: 0.19698062539100647\n",
      "Epoch 82: train loss: 0.24052296578884125\n",
      "Epoch 82: train loss: 0.2420913130044937\n",
      "Epoch 82: train loss: 0.6524969339370728\n",
      "Epoch 82: train loss: 0.2506615221500397\n",
      "Epoch 82: train loss: 0.20852415263652802\n",
      "Epoch 82: train loss: 0.3405916392803192\n",
      "Epoch 82: train loss: 0.22449661791324615\n",
      "Epoch 82: train loss: 0.3182718753814697\n",
      "Epoch 82: train loss: 0.1596767008304596\n",
      "Epoch 82: train loss: 0.3000274896621704\n",
      "Epoch 82: train loss: 0.2984975576400757\n",
      "Epoch 82: train loss: 0.1473088413476944\n",
      "Epoch 82: train loss: 0.36334821581840515\n",
      "Epoch 82: train loss: 0.16779778897762299\n",
      "Epoch 82: train loss: 0.23024006187915802\n",
      "Epoch 82: train loss: 0.17527636885643005\n",
      "Epoch 82: train loss: 0.1467563807964325\n",
      "Epoch 82: train loss: 0.31389090418815613\n",
      "Epoch 82: train loss: 0.0794907733798027\n",
      "Epoch 82: train loss: 0.17688177525997162\n",
      "Epoch 82: train loss: 0.1604287475347519\n",
      "Epoch 82: train loss: 0.26345527172088623\n",
      "Epoch 82: train loss: 0.25729942321777344\n",
      "Epoch 82: train loss: 0.3647063970565796\n",
      "Epoch 82: train loss: 0.362954705953598\n",
      "Epoch 82: train loss: 0.40152859687805176\n",
      "Epoch 82: train loss: 0.3270438313484192\n",
      "Epoch 82: train loss: 0.045824188739061356\n",
      "Epoch 82: train loss: 0.22990281879901886\n",
      "Epoch 82: train loss: 0.266487181186676\n",
      "Epoch 82: train loss: 0.2534414231777191\n",
      "Epoch 82: train loss: 0.11318428069353104\n",
      "Epoch 82: train loss: 0.11929575353860855\n",
      "Epoch 83: train loss: 0.25195878744125366\n",
      "Epoch 83: train loss: 0.34277740120887756\n",
      "Epoch 83: train loss: 0.454485148191452\n",
      "Epoch 83: train loss: 0.13697101175785065\n",
      "Epoch 83: train loss: 0.29351502656936646\n",
      "Epoch 83: train loss: 0.29025915265083313\n",
      "Epoch 83: train loss: 0.24623355269432068\n",
      "Epoch 83: train loss: 0.13467006385326385\n",
      "Epoch 83: train loss: 0.12407202273607254\n",
      "Epoch 83: train loss: 0.318008691072464\n",
      "Epoch 83: train loss: 0.28183308243751526\n",
      "Epoch 83: train loss: 0.20007598400115967\n",
      "Epoch 83: train loss: 0.30089256167411804\n",
      "Epoch 83: train loss: 0.367993026971817\n",
      "Epoch 83: train loss: 0.22149799764156342\n",
      "Epoch 83: train loss: 0.5872879028320312\n",
      "Epoch 83: train loss: 0.3170994222164154\n",
      "Epoch 83: train loss: 0.20289134979248047\n",
      "Epoch 83: train loss: 0.15793733298778534\n",
      "Epoch 83: train loss: 0.2985904812812805\n",
      "Epoch 83: train loss: 0.1590871661901474\n",
      "Epoch 83: train loss: 0.16454267501831055\n",
      "Epoch 83: train loss: 0.4522903263568878\n",
      "Epoch 83: train loss: 0.18981198966503143\n",
      "Epoch 83: train loss: 0.20016898214817047\n",
      "Epoch 83: train loss: 0.1473248451948166\n",
      "Epoch 83: train loss: 0.23245371878147125\n",
      "Epoch 83: train loss: 0.35565292835235596\n",
      "Epoch 83: train loss: 0.2188970446586609\n",
      "Epoch 83: train loss: 0.2627243399620056\n",
      "Epoch 83: train loss: 0.12617193162441254\n",
      "Epoch 83: train loss: 0.2491125613451004\n",
      "Epoch 83: train loss: 0.4017057716846466\n",
      "Epoch 83: train loss: 0.131936714053154\n",
      "Epoch 83: train loss: 0.37707772850990295\n",
      "Epoch 83: train loss: 0.18666116893291473\n",
      "Epoch 83: train loss: 0.07318848371505737\n",
      "Epoch 83: train loss: 0.25181886553764343\n",
      "Epoch 83: train loss: 0.26920679211616516\n",
      "Epoch 83: train loss: 0.14228135347366333\n",
      "Epoch 83: train loss: 0.3413395583629608\n",
      "Epoch 83: train loss: 0.25913479924201965\n",
      "Epoch 83: train loss: 0.16061964631080627\n",
      "Epoch 83: train loss: 0.14856596291065216\n",
      "Epoch 84: train loss: 0.06935122609138489\n",
      "Epoch 84: train loss: 0.167323499917984\n",
      "Epoch 84: train loss: 0.3790988028049469\n",
      "Epoch 84: train loss: 0.3637126088142395\n",
      "Epoch 84: train loss: 0.32443612813949585\n",
      "Epoch 84: train loss: 0.24134905636310577\n",
      "Epoch 84: train loss: 0.26145654916763306\n",
      "Epoch 84: train loss: 0.22468842566013336\n",
      "Epoch 84: train loss: 0.299485981464386\n",
      "Epoch 84: train loss: 0.3537267744541168\n",
      "Epoch 84: train loss: 0.2484627068042755\n",
      "Epoch 84: train loss: 0.21045951545238495\n",
      "Epoch 84: train loss: 0.2786623239517212\n",
      "Epoch 84: train loss: 0.20345835387706757\n",
      "Epoch 84: train loss: 0.263200581073761\n",
      "Epoch 84: train loss: 0.2222847044467926\n",
      "Epoch 84: train loss: 0.20439115166664124\n",
      "Epoch 84: train loss: 0.367834210395813\n",
      "Epoch 84: train loss: 0.19217728078365326\n",
      "Epoch 84: train loss: 0.26622822880744934\n",
      "Epoch 84: train loss: 0.2639550566673279\n",
      "Epoch 84: train loss: 0.2903963625431061\n",
      "Epoch 84: train loss: 0.4047618806362152\n",
      "Epoch 84: train loss: 0.0729159340262413\n",
      "Epoch 84: train loss: 0.3171467185020447\n",
      "Epoch 84: train loss: 0.10846731811761856\n",
      "Epoch 84: train loss: 0.21893709897994995\n",
      "Epoch 84: train loss: 0.46323055028915405\n",
      "Epoch 84: train loss: 0.20850053429603577\n",
      "Epoch 84: train loss: 0.1457606852054596\n",
      "Epoch 84: train loss: 0.051897112280130386\n",
      "Epoch 84: train loss: 0.06440533697605133\n",
      "Epoch 84: train loss: 0.24806512892246246\n",
      "Epoch 84: train loss: 0.1990879774093628\n",
      "Epoch 84: train loss: 0.3370596170425415\n",
      "Epoch 84: train loss: 0.33784401416778564\n",
      "Epoch 84: train loss: 0.17614181339740753\n",
      "Epoch 84: train loss: 0.3302813768386841\n",
      "Epoch 84: train loss: 0.26984429359436035\n",
      "Epoch 84: train loss: 0.2865876257419586\n",
      "Epoch 84: train loss: 0.27394717931747437\n",
      "Epoch 84: train loss: 0.21945001184940338\n",
      "Epoch 84: train loss: 0.31496313214302063\n",
      "Epoch 84: train loss: 0.24673056602478027\n",
      "Epoch 85: train loss: 0.10169919580221176\n",
      "Epoch 85: train loss: 0.10294880717992783\n",
      "Epoch 85: train loss: 0.4671567976474762\n",
      "Epoch 85: train loss: 0.23039236664772034\n",
      "Epoch 85: train loss: 0.17704729735851288\n",
      "Epoch 85: train loss: 0.2337505966424942\n",
      "Epoch 85: train loss: 0.17286056280136108\n",
      "Epoch 85: train loss: 0.33157116174697876\n",
      "Epoch 85: train loss: 0.3627507984638214\n",
      "Epoch 85: train loss: 0.22180674970149994\n",
      "Epoch 85: train loss: 0.23401731252670288\n",
      "Epoch 85: train loss: 0.3588217496871948\n",
      "Epoch 85: train loss: 0.18276117742061615\n",
      "Epoch 85: train loss: 0.2055659294128418\n",
      "Epoch 85: train loss: 0.2536851465702057\n",
      "Epoch 85: train loss: 0.11437308043241501\n",
      "Epoch 85: train loss: 0.3380384147167206\n",
      "Epoch 85: train loss: 0.3059776723384857\n",
      "Epoch 85: train loss: 0.26727113127708435\n",
      "Epoch 85: train loss: 0.19112089276313782\n",
      "Epoch 85: train loss: 0.2930458188056946\n",
      "Epoch 85: train loss: 0.15440785884857178\n",
      "Epoch 85: train loss: 0.24442824721336365\n",
      "Epoch 85: train loss: 0.11758781969547272\n",
      "Epoch 85: train loss: 0.22226093709468842\n",
      "Epoch 85: train loss: 0.2998679578304291\n",
      "Epoch 85: train loss: 0.28491783142089844\n",
      "Epoch 85: train loss: 0.33317744731903076\n",
      "Epoch 85: train loss: 0.21583299338817596\n",
      "Epoch 85: train loss: 0.11055975407361984\n",
      "Epoch 85: train loss: 0.2589433789253235\n",
      "Epoch 85: train loss: 0.17700457572937012\n",
      "Epoch 85: train loss: 0.1267155259847641\n",
      "Epoch 85: train loss: 0.1109093651175499\n",
      "Epoch 85: train loss: 0.2963818609714508\n",
      "Epoch 85: train loss: 0.3940916657447815\n",
      "Epoch 85: train loss: 0.328084260225296\n",
      "Epoch 85: train loss: 0.3989446461200714\n",
      "Epoch 85: train loss: 0.30627113580703735\n",
      "Epoch 85: train loss: 0.27140146493911743\n",
      "Epoch 85: train loss: 0.32543644309043884\n",
      "Epoch 85: train loss: 0.22190424799919128\n",
      "Epoch 85: train loss: 0.40435391664505005\n",
      "Epoch 85: train loss: 0.2551979422569275\n",
      "Epoch 86: train loss: 0.15992961823940277\n",
      "Epoch 86: train loss: 0.4305708706378937\n",
      "Epoch 86: train loss: 0.33349937200546265\n",
      "Epoch 86: train loss: 0.3500058054924011\n",
      "Epoch 86: train loss: 0.14068768918514252\n",
      "Epoch 86: train loss: 0.36704522371292114\n",
      "Epoch 86: train loss: 0.2897602319717407\n",
      "Epoch 86: train loss: 0.21859179437160492\n",
      "Epoch 86: train loss: 0.21987175941467285\n",
      "Epoch 86: train loss: 0.15941983461380005\n",
      "Epoch 86: train loss: 0.40369415283203125\n",
      "Epoch 86: train loss: 0.19949547946453094\n",
      "Epoch 86: train loss: 0.19648300111293793\n",
      "Epoch 86: train loss: 0.3282705247402191\n",
      "Epoch 86: train loss: 0.07594343274831772\n",
      "Epoch 86: train loss: 0.3393726944923401\n",
      "Epoch 86: train loss: 0.26125872135162354\n",
      "Epoch 86: train loss: 0.17154251039028168\n",
      "Epoch 86: train loss: 0.29794785380363464\n",
      "Epoch 86: train loss: 0.3760433793067932\n",
      "Epoch 86: train loss: 0.21717071533203125\n",
      "Epoch 86: train loss: 0.12210816890001297\n",
      "Epoch 86: train loss: 0.2543809115886688\n",
      "Epoch 86: train loss: 0.14917147159576416\n",
      "Epoch 86: train loss: 0.20001006126403809\n",
      "Epoch 86: train loss: 0.11927144974470139\n",
      "Epoch 86: train loss: 0.2651247978210449\n",
      "Epoch 86: train loss: 0.3234505355358124\n",
      "Epoch 86: train loss: 0.1919453889131546\n",
      "Epoch 86: train loss: 0.09965956956148148\n",
      "Epoch 86: train loss: 0.32119235396385193\n",
      "Epoch 86: train loss: 0.21802985668182373\n",
      "Epoch 86: train loss: 0.222469300031662\n",
      "Epoch 86: train loss: 0.43606844544410706\n",
      "Epoch 86: train loss: 0.09531469643115997\n",
      "Epoch 86: train loss: 0.31638839840888977\n",
      "Epoch 86: train loss: 0.35873910784721375\n",
      "Epoch 86: train loss: 0.2237369567155838\n",
      "Epoch 86: train loss: 0.10920816659927368\n",
      "Epoch 86: train loss: 0.2007540464401245\n",
      "Epoch 86: train loss: 0.3030174970626831\n",
      "Epoch 86: train loss: 0.24427950382232666\n",
      "Epoch 86: train loss: 0.46078383922576904\n",
      "Epoch 86: train loss: 0.17048503458499908\n",
      "Epoch 87: train loss: 0.2717178165912628\n",
      "Epoch 87: train loss: 0.1256987303495407\n",
      "Epoch 87: train loss: 0.18718647956848145\n",
      "Epoch 87: train loss: 0.1858237087726593\n",
      "Epoch 87: train loss: 0.07859092950820923\n",
      "Epoch 87: train loss: 0.2529289126396179\n",
      "Epoch 87: train loss: 0.3415479362010956\n",
      "Epoch 87: train loss: 0.20201978087425232\n",
      "Epoch 87: train loss: 0.13315431773662567\n",
      "Epoch 87: train loss: 0.2842743396759033\n",
      "Epoch 87: train loss: 0.3118869960308075\n",
      "Epoch 87: train loss: 0.49572813510894775\n",
      "Epoch 87: train loss: 0.2580126225948334\n",
      "Epoch 87: train loss: 0.2104114443063736\n",
      "Epoch 87: train loss: 0.1834147423505783\n",
      "Epoch 87: train loss: 0.14784100651741028\n",
      "Epoch 87: train loss: 0.24900612235069275\n",
      "Epoch 87: train loss: 0.31383025646209717\n",
      "Epoch 87: train loss: 0.194890558719635\n",
      "Epoch 87: train loss: 0.1824459284543991\n",
      "Epoch 87: train loss: 0.22898854315280914\n",
      "Epoch 87: train loss: 0.18085522949695587\n",
      "Epoch 87: train loss: 0.27810707688331604\n",
      "Epoch 87: train loss: 0.25913190841674805\n",
      "Epoch 87: train loss: 0.27693021297454834\n",
      "Epoch 87: train loss: 0.22346043586730957\n",
      "Epoch 87: train loss: 0.1907418966293335\n",
      "Epoch 87: train loss: 0.21909929811954498\n",
      "Epoch 87: train loss: 0.30150818824768066\n",
      "Epoch 87: train loss: 0.5416804552078247\n",
      "Epoch 87: train loss: 0.2995472252368927\n",
      "Epoch 87: train loss: 0.2686220407485962\n",
      "Epoch 87: train loss: 0.31991681456565857\n",
      "Epoch 87: train loss: 0.17245939373970032\n",
      "Epoch 87: train loss: 0.1153944879770279\n",
      "Epoch 87: train loss: 0.09971573203802109\n",
      "Epoch 87: train loss: 0.3232446312904358\n",
      "Epoch 87: train loss: 0.28577572107315063\n",
      "Epoch 87: train loss: 0.4258313775062561\n",
      "Epoch 87: train loss: 0.2089250683784485\n",
      "Epoch 87: train loss: 0.29706358909606934\n",
      "Epoch 87: train loss: 0.3228321075439453\n",
      "Epoch 87: train loss: 0.41169777512550354\n",
      "Epoch 87: train loss: 0.13721482455730438\n",
      "Epoch 88: train loss: 0.11253105103969574\n",
      "Epoch 88: train loss: 0.28063130378723145\n",
      "Epoch 88: train loss: 0.42631369829177856\n",
      "Epoch 88: train loss: 0.16504661738872528\n",
      "Epoch 88: train loss: 0.21674469113349915\n",
      "Epoch 88: train loss: 0.11344818025827408\n",
      "Epoch 88: train loss: 0.4276104271411896\n",
      "Epoch 88: train loss: 0.20994247496128082\n",
      "Epoch 88: train loss: 0.5604298114776611\n",
      "Epoch 88: train loss: 0.2591453790664673\n",
      "Epoch 88: train loss: 0.05642127990722656\n",
      "Epoch 88: train loss: 0.25472307205200195\n",
      "Epoch 88: train loss: 0.31454306840896606\n",
      "Epoch 88: train loss: 0.3141091763973236\n",
      "Epoch 88: train loss: 0.1038648933172226\n",
      "Epoch 88: train loss: 0.2663656771183014\n",
      "Epoch 88: train loss: 0.29688167572021484\n",
      "Epoch 88: train loss: 0.20941686630249023\n",
      "Epoch 88: train loss: 0.2283528745174408\n",
      "Epoch 88: train loss: 0.36435356736183167\n",
      "Epoch 88: train loss: 0.07354471832513809\n",
      "Epoch 88: train loss: 0.29892465472221375\n",
      "Epoch 88: train loss: 0.17945630848407745\n",
      "Epoch 88: train loss: 0.20054098963737488\n",
      "Epoch 88: train loss: 0.14750537276268005\n",
      "Epoch 88: train loss: 0.5353450179100037\n",
      "Epoch 88: train loss: 0.1863728016614914\n",
      "Epoch 88: train loss: 0.24751640856266022\n",
      "Epoch 88: train loss: 0.2343699187040329\n",
      "Epoch 88: train loss: 0.37821534276008606\n",
      "Epoch 88: train loss: 0.3054604232311249\n",
      "Epoch 88: train loss: 0.13417594134807587\n",
      "Epoch 88: train loss: 0.19128049910068512\n",
      "Epoch 88: train loss: 0.2739377021789551\n",
      "Epoch 88: train loss: 0.2606073021888733\n",
      "Epoch 88: train loss: 0.11051557958126068\n",
      "Epoch 88: train loss: 0.29008597135543823\n",
      "Epoch 88: train loss: 0.11071976274251938\n",
      "Epoch 88: train loss: 0.21382202208042145\n",
      "Epoch 88: train loss: 0.22517907619476318\n",
      "Epoch 88: train loss: 0.175801083445549\n",
      "Epoch 88: train loss: 0.19832265377044678\n",
      "Epoch 88: train loss: 0.474919855594635\n",
      "Epoch 88: train loss: 0.32819223403930664\n",
      "Epoch 89: train loss: 0.1287490576505661\n",
      "Epoch 89: train loss: 0.23249302804470062\n",
      "Epoch 89: train loss: 0.16520245373249054\n",
      "Epoch 89: train loss: 0.3888479769229889\n",
      "Epoch 89: train loss: 0.12930484116077423\n",
      "Epoch 89: train loss: 0.18114478886127472\n",
      "Epoch 89: train loss: 0.14017823338508606\n",
      "Epoch 89: train loss: 0.39145737886428833\n",
      "Epoch 89: train loss: 0.10642758011817932\n",
      "Epoch 89: train loss: 0.32575494050979614\n",
      "Epoch 89: train loss: 0.19090577960014343\n",
      "Epoch 89: train loss: 0.15049846470355988\n",
      "Epoch 89: train loss: 0.31530967354774475\n",
      "Epoch 89: train loss: 0.18467998504638672\n",
      "Epoch 89: train loss: 0.08823900669813156\n",
      "Epoch 89: train loss: 0.10251007974147797\n",
      "Epoch 89: train loss: 0.33949732780456543\n",
      "Epoch 89: train loss: 0.5309000611305237\n",
      "Epoch 89: train loss: 0.23180687427520752\n",
      "Epoch 89: train loss: 0.14445169270038605\n",
      "Epoch 89: train loss: 0.1237940788269043\n",
      "Epoch 89: train loss: 0.4018171727657318\n",
      "Epoch 89: train loss: 0.08591800928115845\n",
      "Epoch 89: train loss: 0.4421048164367676\n",
      "Epoch 89: train loss: 0.415630042552948\n",
      "Epoch 89: train loss: 0.08543756604194641\n",
      "Epoch 89: train loss: 0.29535144567489624\n",
      "Epoch 89: train loss: 0.2509215474128723\n",
      "Epoch 89: train loss: 0.1762491762638092\n",
      "Epoch 89: train loss: 0.20991528034210205\n",
      "Epoch 89: train loss: 0.43917012214660645\n",
      "Epoch 89: train loss: 0.1349237859249115\n",
      "Epoch 89: train loss: 0.39563825726509094\n",
      "Epoch 89: train loss: 0.17359544336795807\n",
      "Epoch 89: train loss: 0.15332956612110138\n",
      "Epoch 89: train loss: 0.24030978977680206\n",
      "Epoch 89: train loss: 0.3083955943584442\n",
      "Epoch 89: train loss: 0.3213127553462982\n",
      "Epoch 89: train loss: 0.30084455013275146\n",
      "Epoch 89: train loss: 0.4435446262359619\n",
      "Epoch 89: train loss: 0.32908663153648376\n",
      "Epoch 89: train loss: 0.12377119064331055\n",
      "Epoch 89: train loss: 0.3798924386501312\n",
      "Epoch 89: train loss: 0.20839236676692963\n",
      "Epoch 90: train loss: 0.2255057543516159\n",
      "Epoch 90: train loss: 0.1712522953748703\n",
      "Epoch 90: train loss: 0.3252687156200409\n",
      "Epoch 90: train loss: 0.24458856880664825\n",
      "Epoch 90: train loss: 0.14378772675991058\n",
      "Epoch 90: train loss: 0.24964147806167603\n",
      "Epoch 90: train loss: 0.4587469696998596\n",
      "Epoch 90: train loss: 0.23650117218494415\n",
      "Epoch 90: train loss: 0.2820971608161926\n",
      "Epoch 90: train loss: 0.10974898934364319\n",
      "Epoch 90: train loss: 0.22753363847732544\n",
      "Epoch 90: train loss: 0.18205498158931732\n",
      "Epoch 90: train loss: 0.13599996268749237\n",
      "Epoch 90: train loss: 0.1578836888074875\n",
      "Epoch 90: train loss: 0.27061107754707336\n",
      "Epoch 90: train loss: 0.05954606831073761\n",
      "Epoch 90: train loss: 0.5166261792182922\n",
      "Epoch 90: train loss: 0.22546358406543732\n",
      "Epoch 90: train loss: 0.3875124454498291\n",
      "Epoch 90: train loss: 0.49455609917640686\n",
      "Epoch 90: train loss: 0.12366843223571777\n",
      "Epoch 90: train loss: 0.1798117607831955\n",
      "Epoch 90: train loss: 0.295725554227829\n",
      "Epoch 90: train loss: 0.2071114182472229\n",
      "Epoch 90: train loss: 0.24600178003311157\n",
      "Epoch 90: train loss: 0.3068440854549408\n",
      "Epoch 90: train loss: 0.20661166310310364\n",
      "Epoch 90: train loss: 0.17783716320991516\n",
      "Epoch 90: train loss: 0.3086561858654022\n",
      "Epoch 90: train loss: 0.2932525873184204\n",
      "Epoch 90: train loss: 0.22237937152385712\n",
      "Epoch 90: train loss: 0.2681940197944641\n",
      "Epoch 90: train loss: 0.2317071557044983\n",
      "Epoch 90: train loss: 0.11207063496112823\n",
      "Epoch 90: train loss: 0.4082983732223511\n",
      "Epoch 90: train loss: 0.15482410788536072\n",
      "Epoch 90: train loss: 0.439786821603775\n",
      "Epoch 90: train loss: 0.33604347705841064\n",
      "Epoch 90: train loss: 0.12733638286590576\n",
      "Epoch 90: train loss: 0.2891532778739929\n",
      "Epoch 90: train loss: 0.11946417391300201\n",
      "Epoch 90: train loss: 0.3230763375759125\n",
      "Epoch 90: train loss: 0.2209288477897644\n",
      "Epoch 90: train loss: 0.3894483745098114\n",
      "Epoch 91: train loss: 0.2784769833087921\n",
      "Epoch 91: train loss: 0.22035273909568787\n",
      "Epoch 91: train loss: 0.2339206486940384\n",
      "Epoch 91: train loss: 0.26229310035705566\n",
      "Epoch 91: train loss: 0.2611893117427826\n",
      "Epoch 91: train loss: 0.3765297532081604\n",
      "Epoch 91: train loss: 0.3175066411495209\n",
      "Epoch 91: train loss: 0.2892937660217285\n",
      "Epoch 91: train loss: 0.5384030938148499\n",
      "Epoch 91: train loss: 0.207604318857193\n",
      "Epoch 91: train loss: 0.1464843451976776\n",
      "Epoch 91: train loss: 0.16061191260814667\n",
      "Epoch 91: train loss: 0.2168184518814087\n",
      "Epoch 91: train loss: 0.33402517437934875\n",
      "Epoch 91: train loss: 0.39749252796173096\n",
      "Epoch 91: train loss: 0.11040286719799042\n",
      "Epoch 91: train loss: 0.16789793968200684\n",
      "Epoch 91: train loss: 0.11182983219623566\n",
      "Epoch 91: train loss: 0.2086910903453827\n",
      "Epoch 91: train loss: 0.2853788435459137\n",
      "Epoch 91: train loss: 0.30713480710983276\n",
      "Epoch 91: train loss: 0.2549612820148468\n",
      "Epoch 91: train loss: 0.36491701006889343\n",
      "Epoch 91: train loss: 0.38219568133354187\n",
      "Epoch 91: train loss: 0.249452605843544\n",
      "Epoch 91: train loss: 0.17601081728935242\n",
      "Epoch 91: train loss: 0.42653703689575195\n",
      "Epoch 91: train loss: 0.24729453027248383\n",
      "Epoch 91: train loss: 0.16747023165225983\n",
      "Epoch 91: train loss: 0.1999908834695816\n",
      "Epoch 91: train loss: 0.2611253559589386\n",
      "Epoch 91: train loss: 0.1757187843322754\n",
      "Epoch 91: train loss: 0.17303699254989624\n",
      "Epoch 91: train loss: 0.2636507451534271\n",
      "Epoch 91: train loss: 0.3041505217552185\n",
      "Epoch 91: train loss: 0.28445836901664734\n",
      "Epoch 91: train loss: 0.22568689286708832\n",
      "Epoch 91: train loss: 0.19616694748401642\n",
      "Epoch 91: train loss: 0.3099626302719116\n",
      "Epoch 91: train loss: 0.31988289952278137\n",
      "Epoch 91: train loss: 0.16006919741630554\n",
      "Epoch 91: train loss: 0.157014399766922\n",
      "Epoch 91: train loss: 0.13939274847507477\n",
      "Epoch 91: train loss: 0.07945284992456436\n",
      "Epoch 92: train loss: 0.2534852921962738\n",
      "Epoch 92: train loss: 0.19292721152305603\n",
      "Epoch 92: train loss: 0.2283269762992859\n",
      "Epoch 92: train loss: 0.4373584985733032\n",
      "Epoch 92: train loss: 0.2036849856376648\n",
      "Epoch 92: train loss: 0.20587040483951569\n",
      "Epoch 92: train loss: 0.25499826669692993\n",
      "Epoch 92: train loss: 0.1585443615913391\n",
      "Epoch 92: train loss: 0.2889898121356964\n",
      "Epoch 92: train loss: 0.24822334945201874\n",
      "Epoch 92: train loss: 0.19310803711414337\n",
      "Epoch 92: train loss: 0.4970731735229492\n",
      "Epoch 92: train loss: 0.27757346630096436\n",
      "Epoch 92: train loss: 0.23652274906635284\n",
      "Epoch 92: train loss: 0.18730096518993378\n",
      "Epoch 92: train loss: 0.0950240045785904\n",
      "Epoch 92: train loss: 0.23680616915225983\n",
      "Epoch 92: train loss: 0.18977591395378113\n",
      "Epoch 92: train loss: 0.21254855394363403\n",
      "Epoch 92: train loss: 0.2426043152809143\n",
      "Epoch 92: train loss: 0.4055647850036621\n",
      "Epoch 92: train loss: 0.16620087623596191\n",
      "Epoch 92: train loss: 0.34661665558815\n",
      "Epoch 92: train loss: 0.3040757477283478\n",
      "Epoch 92: train loss: 0.2003665715456009\n",
      "Epoch 92: train loss: 0.38474780321121216\n",
      "Epoch 92: train loss: 0.2589671015739441\n",
      "Epoch 92: train loss: 0.20100070536136627\n",
      "Epoch 92: train loss: 0.21012148261070251\n",
      "Epoch 92: train loss: 0.27384400367736816\n",
      "Epoch 92: train loss: 0.11663568019866943\n",
      "Epoch 92: train loss: 0.22705741226673126\n",
      "Epoch 92: train loss: 0.2973320484161377\n",
      "Epoch 92: train loss: 0.22483351826667786\n",
      "Epoch 92: train loss: 0.31611838936805725\n",
      "Epoch 92: train loss: 0.17794957756996155\n",
      "Epoch 92: train loss: 0.353142112493515\n",
      "Epoch 92: train loss: 0.3719055950641632\n",
      "Epoch 92: train loss: 0.2623550593852997\n",
      "Epoch 92: train loss: 0.17257826030254364\n",
      "Epoch 92: train loss: 0.12410570681095123\n",
      "Epoch 92: train loss: 0.16550105810165405\n",
      "Epoch 92: train loss: 0.23177145421504974\n",
      "Epoch 92: train loss: 0.38671398162841797\n",
      "Epoch 93: train loss: 0.5152390003204346\n",
      "Epoch 93: train loss: 0.32535046339035034\n",
      "Epoch 93: train loss: 0.46164312958717346\n",
      "Epoch 93: train loss: 0.26898422837257385\n",
      "Epoch 93: train loss: 0.2441205382347107\n",
      "Epoch 93: train loss: 0.21045857667922974\n",
      "Epoch 93: train loss: 0.2251545637845993\n",
      "Epoch 93: train loss: 0.48556995391845703\n",
      "Epoch 93: train loss: 0.17487718164920807\n",
      "Epoch 93: train loss: 0.2605881094932556\n",
      "Epoch 93: train loss: 0.24600297212600708\n",
      "Epoch 93: train loss: 0.21770362555980682\n",
      "Epoch 93: train loss: 0.15282976627349854\n",
      "Epoch 93: train loss: 0.29185599088668823\n",
      "Epoch 93: train loss: 0.1500253677368164\n",
      "Epoch 93: train loss: 0.1558937430381775\n",
      "Epoch 93: train loss: 0.13282115757465363\n",
      "Epoch 93: train loss: 0.20906925201416016\n",
      "Epoch 93: train loss: 0.15914194285869598\n",
      "Epoch 93: train loss: 0.3042713701725006\n",
      "Epoch 93: train loss: 0.1992325335741043\n",
      "Epoch 93: train loss: 0.3066854476928711\n",
      "Epoch 93: train loss: 0.2471201866865158\n",
      "Epoch 93: train loss: 0.16548140347003937\n",
      "Epoch 93: train loss: 0.2856311798095703\n",
      "Epoch 93: train loss: 0.5002113580703735\n",
      "Epoch 93: train loss: 0.14298008382320404\n",
      "Epoch 93: train loss: 0.1839490383863449\n",
      "Epoch 93: train loss: 0.14749972522258759\n",
      "Epoch 93: train loss: 0.3866030275821686\n",
      "Epoch 93: train loss: 0.24972479045391083\n",
      "Epoch 93: train loss: 0.309906929731369\n",
      "Epoch 93: train loss: 0.3927476108074188\n",
      "Epoch 93: train loss: 0.14210370182991028\n",
      "Epoch 93: train loss: 0.18893708288669586\n",
      "Epoch 93: train loss: 0.34843018651008606\n",
      "Epoch 93: train loss: 0.2228100746870041\n",
      "Epoch 93: train loss: 0.12425543367862701\n",
      "Epoch 93: train loss: 0.25058436393737793\n",
      "Epoch 93: train loss: 0.3246820867061615\n",
      "Epoch 93: train loss: 0.28188565373420715\n",
      "Epoch 93: train loss: 0.17139630019664764\n",
      "Epoch 93: train loss: 0.16151875257492065\n",
      "Epoch 93: train loss: 0.2258208692073822\n",
      "Epoch 94: train loss: 0.43025830388069153\n",
      "Epoch 94: train loss: 0.14779745042324066\n",
      "Epoch 94: train loss: 0.11058083921670914\n",
      "Epoch 94: train loss: 0.25987446308135986\n",
      "Epoch 94: train loss: 0.128705233335495\n",
      "Epoch 94: train loss: 0.3222048878669739\n",
      "Epoch 94: train loss: 0.14420895278453827\n",
      "Epoch 94: train loss: 0.3186921775341034\n",
      "Epoch 94: train loss: 0.18617451190948486\n",
      "Epoch 94: train loss: 0.1389884352684021\n",
      "Epoch 94: train loss: 0.39633065462112427\n",
      "Epoch 94: train loss: 0.2309158891439438\n",
      "Epoch 94: train loss: 0.26125192642211914\n",
      "Epoch 94: train loss: 0.2886185050010681\n",
      "Epoch 94: train loss: 0.28665316104888916\n",
      "Epoch 94: train loss: 0.11984395980834961\n",
      "Epoch 94: train loss: 0.4031081199645996\n",
      "Epoch 94: train loss: 0.3987654149532318\n",
      "Epoch 94: train loss: 0.07583671063184738\n",
      "Epoch 94: train loss: 0.3366374969482422\n",
      "Epoch 94: train loss: 0.422929048538208\n",
      "Epoch 94: train loss: 0.14300866425037384\n",
      "Epoch 94: train loss: 0.21359796822071075\n",
      "Epoch 94: train loss: 0.25664210319519043\n",
      "Epoch 94: train loss: 0.1413870006799698\n",
      "Epoch 94: train loss: 0.3840472102165222\n",
      "Epoch 94: train loss: 0.2904514670372009\n",
      "Epoch 94: train loss: 0.29541075229644775\n",
      "Epoch 94: train loss: 0.23897603154182434\n",
      "Epoch 94: train loss: 0.19971273839473724\n",
      "Epoch 94: train loss: 0.19885706901550293\n",
      "Epoch 94: train loss: 0.40286514163017273\n",
      "Epoch 94: train loss: 0.17545917630195618\n",
      "Epoch 94: train loss: 0.33436229825019836\n",
      "Epoch 94: train loss: 0.20738883316516876\n",
      "Epoch 94: train loss: 0.10683215409517288\n",
      "Epoch 94: train loss: 0.08245322853326797\n",
      "Epoch 94: train loss: 0.3735728859901428\n",
      "Epoch 94: train loss: 0.24572554230690002\n",
      "Epoch 94: train loss: 0.5805320739746094\n",
      "Epoch 94: train loss: 0.23078447580337524\n",
      "Epoch 94: train loss: 0.13235975801944733\n",
      "Epoch 94: train loss: 0.20890267193317413\n",
      "Epoch 94: train loss: 0.18774625658988953\n",
      "Epoch 95: train loss: 0.27995339035987854\n",
      "Epoch 95: train loss: 0.21999557316303253\n",
      "Epoch 95: train loss: 0.30366817116737366\n",
      "Epoch 95: train loss: 0.2878035604953766\n",
      "Epoch 95: train loss: 0.11447162181138992\n",
      "Epoch 95: train loss: 0.4831588566303253\n",
      "Epoch 95: train loss: 0.15405181050300598\n",
      "Epoch 95: train loss: 0.2865235507488251\n",
      "Epoch 95: train loss: 0.3540482521057129\n",
      "Epoch 95: train loss: 0.29041588306427\n",
      "Epoch 95: train loss: 0.28042030334472656\n",
      "Epoch 95: train loss: 0.23835039138793945\n",
      "Epoch 95: train loss: 0.18742714822292328\n",
      "Epoch 95: train loss: 0.34644028544425964\n",
      "Epoch 95: train loss: 0.29098761081695557\n",
      "Epoch 95: train loss: 0.29168933629989624\n",
      "Epoch 95: train loss: 0.1275196224451065\n",
      "Epoch 95: train loss: 0.276054322719574\n",
      "Epoch 95: train loss: 0.14227966964244843\n",
      "Epoch 95: train loss: 0.1504812091588974\n",
      "Epoch 95: train loss: 0.22796528041362762\n",
      "Epoch 95: train loss: 0.2686254382133484\n",
      "Epoch 95: train loss: 0.2711619734764099\n",
      "Epoch 95: train loss: 0.35314682126045227\n",
      "Epoch 95: train loss: 0.31762030720710754\n",
      "Epoch 95: train loss: 0.21543487906455994\n",
      "Epoch 95: train loss: 0.18034528195858002\n",
      "Epoch 95: train loss: 0.3196737468242645\n",
      "Epoch 95: train loss: 0.09136553853750229\n",
      "Epoch 95: train loss: 0.1965751349925995\n",
      "Epoch 95: train loss: 0.18261486291885376\n",
      "Epoch 95: train loss: 0.25066331028938293\n",
      "Epoch 95: train loss: 0.28348153829574585\n",
      "Epoch 95: train loss: 0.2413945198059082\n",
      "Epoch 95: train loss: 0.32269352674484253\n",
      "Epoch 95: train loss: 0.27147814631462097\n",
      "Epoch 95: train loss: 0.3537188768386841\n",
      "Epoch 95: train loss: 0.13802219927310944\n",
      "Epoch 95: train loss: 0.09271050989627838\n",
      "Epoch 95: train loss: 0.33532440662384033\n",
      "Epoch 95: train loss: 0.1701422482728958\n",
      "Epoch 95: train loss: 0.0825255811214447\n",
      "Epoch 95: train loss: 0.3355605900287628\n",
      "Epoch 95: train loss: 0.3402116894721985\n",
      "Epoch 96: train loss: 0.20959040522575378\n",
      "Epoch 96: train loss: 0.19513927400112152\n",
      "Epoch 96: train loss: 0.2304619550704956\n",
      "Epoch 96: train loss: 0.24438008666038513\n",
      "Epoch 96: train loss: 0.27129825949668884\n",
      "Epoch 96: train loss: 0.31301331520080566\n",
      "Epoch 96: train loss: 0.2624486982822418\n",
      "Epoch 96: train loss: 0.1926717311143875\n",
      "Epoch 96: train loss: 0.2791835069656372\n",
      "Epoch 96: train loss: 0.138298898935318\n",
      "Epoch 96: train loss: 0.18214745819568634\n",
      "Epoch 96: train loss: 0.43474504351615906\n",
      "Epoch 96: train loss: 0.29095137119293213\n",
      "Epoch 96: train loss: 0.18135449290275574\n",
      "Epoch 96: train loss: 0.22726555168628693\n",
      "Epoch 96: train loss: 0.28618955612182617\n",
      "Epoch 96: train loss: 0.3095487952232361\n",
      "Epoch 96: train loss: 0.16211764514446259\n",
      "Epoch 96: train loss: 0.3152943253517151\n",
      "Epoch 96: train loss: 0.22489523887634277\n",
      "Epoch 96: train loss: 0.19026300311088562\n",
      "Epoch 96: train loss: 0.2584971487522125\n",
      "Epoch 96: train loss: 0.38906294107437134\n",
      "Epoch 96: train loss: 0.21471846103668213\n",
      "Epoch 96: train loss: 0.14252325892448425\n",
      "Epoch 96: train loss: 0.45533478260040283\n",
      "Epoch 96: train loss: 0.1199619248509407\n",
      "Epoch 96: train loss: 0.48671525716781616\n",
      "Epoch 96: train loss: 0.3186494708061218\n",
      "Epoch 96: train loss: 0.1186627522110939\n",
      "Epoch 96: train loss: 0.17408648133277893\n",
      "Epoch 96: train loss: 0.3151337504386902\n",
      "Epoch 96: train loss: 0.19391939043998718\n",
      "Epoch 96: train loss: 0.07462228834629059\n",
      "Epoch 96: train loss: 0.20812176167964935\n",
      "Epoch 96: train loss: 0.19560714066028595\n",
      "Epoch 96: train loss: 0.0818081870675087\n",
      "Epoch 96: train loss: 0.2321065366268158\n",
      "Epoch 96: train loss: 0.7723104357719421\n",
      "Epoch 96: train loss: 0.1182846799492836\n",
      "Epoch 96: train loss: 0.2021874636411667\n",
      "Epoch 96: train loss: 0.30969199538230896\n",
      "Epoch 96: train loss: 0.3280119299888611\n",
      "Epoch 96: train loss: 0.10597782582044601\n",
      "Epoch 97: train loss: 0.2817099392414093\n",
      "Epoch 97: train loss: 0.21540474891662598\n",
      "Epoch 97: train loss: 0.25606656074523926\n",
      "Epoch 97: train loss: 0.15378199517726898\n",
      "Epoch 97: train loss: 0.30553755164146423\n",
      "Epoch 97: train loss: 0.08211144059896469\n",
      "Epoch 97: train loss: 0.29057586193084717\n",
      "Epoch 97: train loss: 0.32922661304473877\n",
      "Epoch 97: train loss: 0.3210963010787964\n",
      "Epoch 97: train loss: 0.3279939293861389\n",
      "Epoch 97: train loss: 0.20109474658966064\n",
      "Epoch 97: train loss: 0.22233963012695312\n",
      "Epoch 97: train loss: 0.30940645933151245\n",
      "Epoch 97: train loss: 0.31409046053886414\n",
      "Epoch 97: train loss: 0.1711060106754303\n",
      "Epoch 97: train loss: 0.20197036862373352\n",
      "Epoch 97: train loss: 0.13589923083782196\n",
      "Epoch 97: train loss: 0.2934246063232422\n",
      "Epoch 97: train loss: 0.23888325691223145\n",
      "Epoch 97: train loss: 0.166488379240036\n",
      "Epoch 97: train loss: 0.2869962155818939\n",
      "Epoch 97: train loss: 0.17421722412109375\n",
      "Epoch 97: train loss: 0.30894502997398376\n",
      "Epoch 97: train loss: 0.3248711824417114\n",
      "Epoch 97: train loss: 0.32900211215019226\n",
      "Epoch 97: train loss: 0.30895888805389404\n",
      "Epoch 97: train loss: 0.24403990805149078\n",
      "Epoch 97: train loss: 0.29312124848365784\n",
      "Epoch 97: train loss: 0.44866839051246643\n",
      "Epoch 97: train loss: 0.4278561770915985\n",
      "Epoch 97: train loss: 0.2885599732398987\n",
      "Epoch 97: train loss: 0.1809878796339035\n",
      "Epoch 97: train loss: 0.2213939130306244\n",
      "Epoch 97: train loss: 0.14270900189876556\n",
      "Epoch 97: train loss: 0.25165051221847534\n",
      "Epoch 97: train loss: 0.17274023592472076\n",
      "Epoch 97: train loss: 0.24794691801071167\n",
      "Epoch 97: train loss: 0.15684984624385834\n",
      "Epoch 97: train loss: 0.21116532385349274\n",
      "Epoch 97: train loss: 0.19662117958068848\n",
      "Epoch 97: train loss: 0.3987498879432678\n",
      "Epoch 97: train loss: 0.36573565006256104\n",
      "Epoch 97: train loss: 0.07277140766382217\n",
      "Epoch 97: train loss: 0.07162365317344666\n",
      "Epoch 98: train loss: 0.3200344741344452\n",
      "Epoch 98: train loss: 0.16515901684761047\n",
      "Epoch 98: train loss: 0.26383262872695923\n",
      "Epoch 98: train loss: 0.08093907684087753\n",
      "Epoch 98: train loss: 0.2851371467113495\n",
      "Epoch 98: train loss: 0.2459796667098999\n",
      "Epoch 98: train loss: 0.26521623134613037\n",
      "Epoch 98: train loss: 0.10899519175291061\n",
      "Epoch 98: train loss: 0.303022176027298\n",
      "Epoch 98: train loss: 0.31539344787597656\n",
      "Epoch 98: train loss: 0.38544315099716187\n",
      "Epoch 98: train loss: 0.13910581171512604\n",
      "Epoch 98: train loss: 0.26466408371925354\n",
      "Epoch 98: train loss: 0.19580453634262085\n",
      "Epoch 98: train loss: 0.0795738473534584\n",
      "Epoch 98: train loss: 0.25775444507598877\n",
      "Epoch 98: train loss: 0.09800444543361664\n",
      "Epoch 98: train loss: 0.26802656054496765\n",
      "Epoch 98: train loss: 0.37670010328292847\n",
      "Epoch 98: train loss: 0.35388630628585815\n",
      "Epoch 98: train loss: 0.13291846215724945\n",
      "Epoch 98: train loss: 0.41278064250946045\n",
      "Epoch 98: train loss: 0.3095017075538635\n",
      "Epoch 98: train loss: 0.38802072405815125\n",
      "Epoch 98: train loss: 0.330723375082016\n",
      "Epoch 98: train loss: 0.20814546942710876\n",
      "Epoch 98: train loss: 0.37760695815086365\n",
      "Epoch 98: train loss: 0.3734859824180603\n",
      "Epoch 98: train loss: 0.20008186995983124\n",
      "Epoch 98: train loss: 0.2131318897008896\n",
      "Epoch 98: train loss: 0.3957662582397461\n",
      "Epoch 98: train loss: 0.2705982029438019\n",
      "Epoch 98: train loss: 0.1946558654308319\n",
      "Epoch 98: train loss: 0.22618542611598969\n",
      "Epoch 98: train loss: 0.16619986295700073\n",
      "Epoch 98: train loss: 0.21484531462192535\n",
      "Epoch 98: train loss: 0.18395967781543732\n",
      "Epoch 98: train loss: 0.22487863898277283\n",
      "Epoch 98: train loss: 0.07950858026742935\n",
      "Epoch 98: train loss: 0.20106835663318634\n",
      "Epoch 98: train loss: 0.054493315517902374\n",
      "Epoch 98: train loss: 0.21012014150619507\n",
      "Epoch 98: train loss: 0.5056864023208618\n",
      "Epoch 98: train loss: 0.35274675488471985\n",
      "Epoch 99: train loss: 0.37651270627975464\n",
      "Epoch 99: train loss: 0.3177744448184967\n",
      "Epoch 99: train loss: 0.3515663146972656\n",
      "Epoch 99: train loss: 0.2826772630214691\n",
      "Epoch 99: train loss: 0.3650508224964142\n",
      "Epoch 99: train loss: 0.35600754618644714\n",
      "Epoch 99: train loss: 0.22009018063545227\n",
      "Epoch 99: train loss: 0.11752676963806152\n",
      "Epoch 99: train loss: 0.3276006877422333\n",
      "Epoch 99: train loss: 0.33909642696380615\n",
      "Epoch 99: train loss: 0.24092547595500946\n",
      "Epoch 99: train loss: 0.3010210692882538\n",
      "Epoch 99: train loss: 0.2931986153125763\n",
      "Epoch 99: train loss: 0.2912067472934723\n",
      "Epoch 99: train loss: 0.18374192714691162\n",
      "Epoch 99: train loss: 0.2997684180736542\n",
      "Epoch 99: train loss: 0.19798365235328674\n",
      "Epoch 99: train loss: 0.3502092659473419\n",
      "Epoch 99: train loss: 0.30840936303138733\n",
      "Epoch 99: train loss: 0.18138714134693146\n",
      "Epoch 99: train loss: 0.2044549137353897\n",
      "Epoch 99: train loss: 0.127386674284935\n",
      "Epoch 99: train loss: 0.33775976300239563\n",
      "Epoch 99: train loss: 0.22280241549015045\n",
      "Epoch 99: train loss: 0.17339767515659332\n",
      "Epoch 99: train loss: 0.16615906357765198\n",
      "Epoch 99: train loss: 0.06542462855577469\n",
      "Epoch 99: train loss: 0.14808931946754456\n",
      "Epoch 99: train loss: 0.25861087441444397\n",
      "Epoch 99: train loss: 0.16580556333065033\n",
      "Epoch 99: train loss: 0.10014767944812775\n",
      "Epoch 99: train loss: 0.09143847227096558\n",
      "Epoch 99: train loss: 0.24860867857933044\n",
      "Epoch 99: train loss: 0.2974727153778076\n",
      "Epoch 99: train loss: 0.2233819216489792\n",
      "Epoch 99: train loss: 0.136187344789505\n",
      "Epoch 99: train loss: 0.6080655455589294\n",
      "Epoch 99: train loss: 0.13128924369812012\n",
      "Epoch 99: train loss: 0.26601558923721313\n",
      "Epoch 99: train loss: 0.22401653230190277\n",
      "Epoch 99: train loss: 0.39213302731513977\n",
      "Epoch 99: train loss: 0.3583538234233856\n",
      "Epoch 99: train loss: 0.19648022949695587\n",
      "Epoch 99: train loss: 0.3104936182498932\n"
     ]
    }
   ],
   "source": [
    "# model using Sequential()\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(10, 5),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(5, 1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "epochs = 100\n",
    "l2lamb = 0.0001\n",
    "for ep in range(epochs):\n",
    "    for (Xb, yb, _) in traindl:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y1_pred = model(torch.tensor(Xb).float())\n",
    "        loss_pred = loss_fn(y1_pred, torch.tensor(yb).float())\n",
    "        l2_norm = sum(val.pow(2.0).sum() for val in model.parameters())\n",
    "        loss = loss_pred + l2lamb * l2_norm\n",
    "        print('Epoch {}: train loss: {}'.format(ep, loss.item()))\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "#torch.save(model, \"./dice_demo_same_setting_as_sklearn.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "model = torch.load(\"./dice_demo_same_setting_as_sklearn.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      " bad credit (0)      0.812     0.194     0.313       134\n",
      "good credit (1)      0.921     0.995     0.957      1271\n",
      "\n",
      "       accuracy                          0.919      1405\n",
      "      macro avg      0.867     0.595     0.635      1405\n",
      "   weighted avg      0.911     0.919     0.895      1405\n",
      "\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      " bad credit (0)      0.750     0.176     0.286        34\n",
      "good credit (1)      0.919     0.994     0.955       318\n",
      "\n",
      "       accuracy                          0.915       352\n",
      "      macro avg      0.834     0.585     0.620       352\n",
      "   weighted avg      0.902     0.915     0.890       352\n",
      "\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      " bad credit (0)      0.849     0.201     0.325       309\n",
      "good credit (1)      0.928     0.997     0.961      3206\n",
      "\n",
      "       accuracy                          0.927      3515\n",
      "      macro avg      0.889     0.599     0.643      3515\n",
      "   weighted avg      0.921     0.927     0.905      3515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resres = model(torch.tensor(X1_train.values).float()).detach().numpy().flatten().round()\n",
    "print('\\n', classification_report(y1_train, resres, target_names=[f'bad credit (0)', f'good credit (1)'], digits=3))\n",
    "resres = model(torch.tensor(X1_test.values).float()).detach().numpy().flatten().round()\n",
    "print('\\n', classification_report(y1_test, resres, target_names=[f'bad credit (0)', f'good credit (1)'], digits=3))\n",
    "resres = model(torch.tensor(X.values).float()).detach().numpy().flatten().round()\n",
    "print('\\n', classification_report(y, resres, target_names=[f'bad credit (0)', f'good credit (1)'], digits=3))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# wrapper class of pytorch models\n",
    "class CLF(object):\n",
    "    def __init__(self, model, hidden_layer_sizes=5, n_features_in=10):\n",
    "        self.model = copy.deepcopy(model)\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.n_features_in_ = n_features_in\n",
    "        self.coefs_, self.intercepts_ = self.get_params()\n",
    "\n",
    "    # take in [n_samples, n_features] array, return a numpy arr\n",
    "    def predict(self, x):\n",
    "        yhats = None\n",
    "        try:\n",
    "            yhats = self.model(torch.tensor(x).float()).detach().numpy().flatten().round().astype(np.int64)\n",
    "        except:\n",
    "            yhats = self.model(torch.tensor(x.values).float()).detach().numpy().flatten().round().astype(np.int64)\n",
    "        return yhats\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        self.model.eval()\n",
    "        yhats = None\n",
    "        try:\n",
    "            yhats = self.model(torch.tensor(x).float()).detach().numpy().flatten().astype(np.float64)\n",
    "        except:\n",
    "            yhats = self.model(torch.tensor(x.values).float()).detach().numpy().flatten().astype(np.float64)\n",
    "        yreturn = []\n",
    "        for this_proba in yhats:\n",
    "            yreturn.append([1 - this_proba, this_proba])\n",
    "        return np.array(yreturn)\n",
    "\n",
    "    def partial_fit(self, Xdf, ydf, targets=None):\n",
    "        trainds1 = DemoDataset(Xdf, ydf)\n",
    "        params = {'batch_size': 8,\n",
    "                  'shuffle': True}\n",
    "        traindl1 = DataLoader(trainds1, **params)\n",
    "        #if isinstance(Xdf, pd.core.frame.DataFrame):\n",
    "        #    Xt = Xdf.values\n",
    "        #else:\n",
    "        #    Xt = Xdf\n",
    "        #if isinstance(ydf, pd.core.frame.DataFrame):\n",
    "        #    yt = ydf.values\n",
    "        #else:\n",
    "        #    yt = ydf\n",
    "        loss_fn = torch.nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.02)\n",
    "        epochs = 1\n",
    "        l2lamb = 0.0001\n",
    "        for ep in range(epochs):\n",
    "            for (Xb, yb, _) in traindl1:\n",
    "                self.model.train()\n",
    "                optimizer.zero_grad()\n",
    "                yp = self.model(torch.tensor(Xb).float())\n",
    "                loss_pred = loss_fn(yp, torch.tensor(yb).float())\n",
    "                #print('Epoch {}: train loss: {}'.format(ep, loss.item()))\n",
    "                l2_norm = sum(val.pow(2.0).sum() for val in self.model.parameters())\n",
    "                loss = loss_pred + l2lamb * l2_norm\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        # update params\n",
    "        self.coefs_, self.intercepts_ = self.get_params()\n",
    "\n",
    "    def get_params(self):\n",
    "        w1 = None\n",
    "        w2 = None\n",
    "        b1 = None\n",
    "        b2 = None\n",
    "        for i, item in enumerate(self.model.parameters()):\n",
    "            if i == 0:\n",
    "                w1 = item.data.detach().numpy()\n",
    "            if i == 1:\n",
    "                b1 = item.data.detach().numpy()\n",
    "            if i == 2:\n",
    "                w2 = item.data.detach().numpy()\n",
    "            if i == 3:\n",
    "                b2 = item.data.detach().numpy()\n",
    "        w1 = w1.transpose()\n",
    "        w2 = w2.transpose()\n",
    "        b1 = b1\n",
    "        return [w1, w2], [b1, b2]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "clf = CLF(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x16567b82b48>]"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gaps = np.arange(0.02, 1.01, 0.02)\n",
    "deltas = []\n",
    "clfs = []\n",
    "for a, i in enumerate(gaps):\n",
    "    clfs.append(copy.deepcopy(clf))\n",
    "    util_exp = UtilExp(clfs[a], X1, y1, X2, y2, columns, ordinal_features, discrete_features, continuous_features,\n",
    "                       feat_var_map,\n",
    "                       num_test_instances=1000, gap=i)\n",
    "    deltas.append(util_exp.delta_min)\n",
    "plt.plot(gaps, deltas)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.040017545\n"
     ]
    }
   ],
   "source": [
    "util_exppp = UtilExp(clf, X1, y1, X2, y2, columns, ordinal_features, discrete_features, continuous_features, feat_var_map,\n",
    "                   num_test_instances=1000, gap=0.007)\n",
    "print(util_exppp.delta_min)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02 0.09324026\n",
      "0.04 0.13901126\n",
      "0.06 0.16879225\n",
      "0.08 0.14681673\n",
      "0.1 0.1916436\n",
      "0.12000000000000001 0.20660746\n",
      "0.13999999999999999 0.29378095\n",
      "0.16 0.27556035\n",
      "0.18 0.24417102\n",
      "0.19999999999999998 0.261091\n",
      "0.22 0.309106\n",
      "0.24 0.3085249\n",
      "0.26 0.33368182\n",
      "0.28 0.30332696\n",
      "0.30000000000000004 0.41295728\n",
      "0.32 0.34723717\n",
      "0.34 0.38288307\n",
      "0.36000000000000004 0.48531213\n",
      "0.38 0.4220926\n",
      "0.4 0.40223384\n",
      "0.42000000000000004 0.44970298\n",
      "0.44 0.449638\n",
      "0.46 0.547352\n",
      "0.48000000000000004 0.5043716\n",
      "0.5 0.54281163\n",
      "0.52 0.5723149\n",
      "0.54 0.5201705\n",
      "0.56 0.5285491\n",
      "0.5800000000000001 0.5506539\n",
      "0.6 0.52101856\n",
      "0.62 0.63625085\n",
      "0.64 0.575032\n",
      "0.66 0.55152047\n",
      "0.68 0.68470496\n",
      "0.7000000000000001 0.66278183\n",
      "0.7200000000000001 0.66090405\n",
      "0.74 0.5897924\n",
      "0.76 0.70318544\n",
      "0.78 0.70691407\n",
      "0.8 0.63102055\n",
      "0.8200000000000001 0.73904794\n",
      "0.8400000000000001 0.70384526\n",
      "0.86 0.7091412\n",
      "0.88 0.7575629\n",
      "0.9 0.70423675\n",
      "0.92 0.82391816\n",
      "0.9400000000000001 0.74561155\n",
      "0.9600000000000001 0.75195575\n",
      "0.98 0.78915244\n",
      "1.0 0.8041189\n"
     ]
    }
   ],
   "source": [
    "for i, gap in enumerate(gaps):\n",
    "    print(gap, deltas[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "util_exp = UtilExp(clf, X1, y1, X2, y2, columns, ordinal_features, discrete_features, continuous_features, feat_var_map,\n",
    "                   num_test_instances=1000, gap=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61073226\n",
      "0.19641781\n"
     ]
    }
   ],
   "source": [
    "print(util_exp.delta_max)\n",
    "print(util_exp.delta_min)\n",
    "#util_exp.delta_min = 0.19641781 # gap=0.1\n",
    "#util_exp.build_inns()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2023-06-14\n",
      "percentage of sound model changes: 0.025\n",
      "1\n",
      "[19]\n"
     ]
    }
   ],
   "source": [
    "valids = util_exp.verify_soundness()\n",
    "print(len(valids))\n",
    "print(valids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# DiCE imports\n",
    "import dice_ml\n",
    "from dice_ml import Dice"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "d = dice_ml.Data(dataframe=df1, continuous_features=X.columns.to_list(), outcome_name=\"BAD\")\n",
    "m = dice_ml.Model(model=model, backend=\"PYT\")\n",
    "exp = Dice(d, m)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14512472 0.08599266 0.05864832 0.02439024 0.8        0.1\n",
      " 0.1887657  0.         0.26153846 0.32839981]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.14512472, 0.08599266, 0.05864832, 0.02439024, 0.8, 0.1,\n",
    "              0.1887657, 0., 0., 0.32839981])\n",
    "# test_instances[19], sound"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root: MAD for feature DEROG is 0, so replacing it with 1.0 to avoid error.\n",
      "WARNING:root: MAD for feature DELINQ is 0, so replacing it with 1.0 to avoid error.\n",
      "WARNING:root: MAD for feature NINQ is 0, so replacing it with 1.0 to avoid error.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diverse Counterfactuals found! total time taken: 00 min 07 sec\n",
      "Query instance (original outcome : 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "       LOAN   MORTDUE     VALUE  YOJ  DEROG  DELINQ     CLAGE  NINQ      CLNO  \\\n0  0.145125  0.085993  0.058648  0.0    0.8     0.1  0.188766   0.0  0.261539   \n\n   DEBTINC    BAD  \n0   0.3284  0.723  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LOAN</th>\n      <th>MORTDUE</th>\n      <th>VALUE</th>\n      <th>YOJ</th>\n      <th>DEROG</th>\n      <th>DELINQ</th>\n      <th>CLAGE</th>\n      <th>NINQ</th>\n      <th>CLNO</th>\n      <th>DEBTINC</th>\n      <th>BAD</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.145125</td>\n      <td>0.085993</td>\n      <td>0.058648</td>\n      <td>0.0</td>\n      <td>0.8</td>\n      <td>0.1</td>\n      <td>0.188766</td>\n      <td>0.0</td>\n      <td>0.261539</td>\n      <td>0.3284</td>\n      <td>0.723</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diverse Counterfactual set (new outcome: 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "       LOAN   MORTDUE     VALUE  YOJ  DEROG  DELINQ     CLAGE  NINQ      CLNO  \\\n0  0.148453  0.264276  0.244112  0.0    0.0     0.1  0.030954   0.0  0.015385   \n1  0.057039  0.353074  0.060257  0.0    0.0     0.0  0.376738   0.0  0.266972   \n2  0.172336  0.084500  0.101791  0.0    0.4     0.0  0.156164   0.0  0.424058   \n3  0.172336  0.088707  0.037781  0.8    0.0     0.0  0.376738   0.0  0.236708   \n4  0.114611  0.511079  0.000000  0.0    0.5     0.2  0.188633   0.0  0.269811   \n5  0.142129  0.000000  0.000000  0.2    0.7     0.1  0.223560   0.0  0.015385   \n6  0.145315  0.000000  0.058835  0.0    0.0     0.1  0.101929   0.0  0.858074   \n7  0.145302  0.085431  0.059314  0.0    0.1     0.0  0.312782   0.0  0.260222   \n8  0.123532  0.150387  0.187199  0.0    0.3     0.1  0.376738   0.0  1.000000   \n9  0.144973  0.086464  0.436959  0.4    0.9     0.1  0.187505   0.0  0.260487   \n\n    DEBTINC  BAD  \n0  0.330495    0  \n1  0.054665    0  \n2  0.015194    0  \n3  0.328443    0  \n4  0.393520    0  \n5  0.185091    0  \n6  0.377527    0  \n7  0.633233    0  \n8  0.324383    0  \n9  0.329384    0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LOAN</th>\n      <th>MORTDUE</th>\n      <th>VALUE</th>\n      <th>YOJ</th>\n      <th>DEROG</th>\n      <th>DELINQ</th>\n      <th>CLAGE</th>\n      <th>NINQ</th>\n      <th>CLNO</th>\n      <th>DEBTINC</th>\n      <th>BAD</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.148453</td>\n      <td>0.264276</td>\n      <td>0.244112</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.1</td>\n      <td>0.030954</td>\n      <td>0.0</td>\n      <td>0.015385</td>\n      <td>0.330495</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.057039</td>\n      <td>0.353074</td>\n      <td>0.060257</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.376738</td>\n      <td>0.0</td>\n      <td>0.266972</td>\n      <td>0.054665</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.172336</td>\n      <td>0.084500</td>\n      <td>0.101791</td>\n      <td>0.0</td>\n      <td>0.4</td>\n      <td>0.0</td>\n      <td>0.156164</td>\n      <td>0.0</td>\n      <td>0.424058</td>\n      <td>0.015194</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.172336</td>\n      <td>0.088707</td>\n      <td>0.037781</td>\n      <td>0.8</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.376738</td>\n      <td>0.0</td>\n      <td>0.236708</td>\n      <td>0.328443</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.114611</td>\n      <td>0.511079</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>0.2</td>\n      <td>0.188633</td>\n      <td>0.0</td>\n      <td>0.269811</td>\n      <td>0.393520</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.142129</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.2</td>\n      <td>0.7</td>\n      <td>0.1</td>\n      <td>0.223560</td>\n      <td>0.0</td>\n      <td>0.015385</td>\n      <td>0.185091</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.145315</td>\n      <td>0.000000</td>\n      <td>0.058835</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.1</td>\n      <td>0.101929</td>\n      <td>0.0</td>\n      <td>0.858074</td>\n      <td>0.377527</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.145302</td>\n      <td>0.085431</td>\n      <td>0.059314</td>\n      <td>0.0</td>\n      <td>0.1</td>\n      <td>0.0</td>\n      <td>0.312782</td>\n      <td>0.0</td>\n      <td>0.260222</td>\n      <td>0.633233</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.123532</td>\n      <td>0.150387</td>\n      <td>0.187199</td>\n      <td>0.0</td>\n      <td>0.3</td>\n      <td>0.1</td>\n      <td>0.376738</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n      <td>0.324383</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.144973</td>\n      <td>0.086464</td>\n      <td>0.436959</td>\n      <td>0.4</td>\n      <td>0.9</td>\n      <td>0.1</td>\n      <td>0.187505</td>\n      <td>0.0</td>\n      <td>0.260487</td>\n      <td>0.329384</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#x = util_exp.test_instances[19]\n",
    "cf = exp.generate_counterfactuals(pd.DataFrame(x.reshape(1, -1), columns=X1.columns), total_CFs=10,\n",
    "                                  desired_class=1, proximity_weight=0.05, diversity_weight=1)\n",
    "cf.visualize_as_dataframe(show_only_changes=False)\n",
    "cfs = cf.cf_examples_list[0].final_cfs_df\n",
    "cfs = cfs.drop(columns=[\"BAD\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "#cf.cf_examples_list[0].final_cfs_df.to_csv(path_or_buf='dicedemores10percent.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 0 0 0 0 1 0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(cfs))\n",
    "print(clf.predict(x.reshape(1, -1)))\n",
    "cfs_valid = np.array(\n",
    "    [cfs.values[0], cfs.values[1], cfs.values[2], cfs.values[3], cfs.values[8]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(cfs_valid))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, -2.0826012444857565)\n",
      "(1, 5.246532413160037)\n",
      "(1, 0.5915440560559933)\n",
      "(1, 0.5662771894267387)\n",
      "(0, -1.925906143927269)\n"
     ]
    }
   ],
   "source": [
    "for item in cfs_valid:\n",
    "    print(util_exp.is_robust_raw(x, item))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for item in cfs_valid:\n",
    "    print(util_exp.is_robust(x, item))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "cfs_robust = np.array([cfs.values[1], cfs.values[2], cfs.values[3]])\n",
    "cfs_non_robust = np.array([cfs.values[0], cfs.values[8]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "    LOAN  MORTDUE  VALUE  YOJ  DEROG  DELINQ  CLAGE  NINQ  CLNO  DEBTINC  \\\n0  14500    38986  49970    1      8       1    221     0    17       48   \n\n   REPAID  \n0       0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LOAN</th>\n      <th>MORTDUE</th>\n      <th>VALUE</th>\n      <th>YOJ</th>\n      <th>DEROG</th>\n      <th>DELINQ</th>\n      <th>CLAGE</th>\n      <th>NINQ</th>\n      <th>CLNO</th>\n      <th>DEBTINC</th>\n      <th>REPAID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14500</td>\n      <td>38986</td>\n      <td>49970</td>\n      <td>1</td>\n      <td>8</td>\n      <td>1</td>\n      <td>221</td>\n      <td>0</td>\n      <td>17</td>\n      <td>48</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns_good = copy.deepcopy(continuous_features)\n",
    "columns_good.append(\"REPAID\")\n",
    "xdf = pd.DataFrame(data=(\n",
    "    np.round(np.concatenate((inverse_min_max_scale(x), np.array(clf.predict(x.reshape(1, -1)))))).reshape(1,\n",
    "                                                                                                          -1)).astype(\n",
    "    int), columns=columns_good)\n",
    "display(xdf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "    LOAN  MORTDUE  VALUE  YOJ  DEROG  DELINQ  CLAGE  NINQ  CLNO  DEBTINC  \\\n0   6731   144306  50761    0      0       0    440     0    17        9   \n1  16900    38397  71175    0      4       0    183     0    28        3   \n2  16900    40056  39714   33      0       0    440     0    15       48   \n\n   REPAID  \n0       1  \n1       1  \n2       1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LOAN</th>\n      <th>MORTDUE</th>\n      <th>VALUE</th>\n      <th>YOJ</th>\n      <th>DEROG</th>\n      <th>DELINQ</th>\n      <th>CLAGE</th>\n      <th>NINQ</th>\n      <th>CLNO</th>\n      <th>DEBTINC</th>\n      <th>REPAID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6731</td>\n      <td>144306</td>\n      <td>50761</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>440</td>\n      <td>0</td>\n      <td>17</td>\n      <td>9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>16900</td>\n      <td>38397</td>\n      <td>71175</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>183</td>\n      <td>0</td>\n      <td>28</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>16900</td>\n      <td>40056</td>\n      <td>39714</td>\n      <td>33</td>\n      <td>0</td>\n      <td>0</td>\n      <td>440</td>\n      <td>0</td>\n      <td>15</td>\n      <td>48</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfdfrdata = []\n",
    "for cf in cfs_robust:\n",
    "    cfdfrdata.append(np.round(np.concatenate((inverse_min_max_scale(cf), np.array(clf.predict(cf.reshape(1, -1)))))))\n",
    "cfdfrdata = np.array(cfdfrdata).astype(int)\n",
    "cfdfr = pd.DataFrame(data=cfdfrdata, columns=columns_good)\n",
    "display(cfdfr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "    LOAN  MORTDUE   VALUE  YOJ  DEROG  DELINQ  CLAGE  NINQ  CLNO  DEBTINC  \\\n0  14794   109290  141126    0      0       1     37     0     1       48   \n1   6731   144306   50761    0      0       0    440     0    17        9   \n2  16900    38397   71175    0      4       0    183     0    28        3   \n3  16900    40056   39714   33      0       0    440     0    15       48   \n4  12595    64379  113153    0      3       1    440     0    65       47   \n\n   REPAID  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LOAN</th>\n      <th>MORTDUE</th>\n      <th>VALUE</th>\n      <th>YOJ</th>\n      <th>DEROG</th>\n      <th>DELINQ</th>\n      <th>CLAGE</th>\n      <th>NINQ</th>\n      <th>CLNO</th>\n      <th>DEBTINC</th>\n      <th>REPAID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14794</td>\n      <td>109290</td>\n      <td>141126</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>37</td>\n      <td>0</td>\n      <td>1</td>\n      <td>48</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6731</td>\n      <td>144306</td>\n      <td>50761</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>440</td>\n      <td>0</td>\n      <td>17</td>\n      <td>9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>16900</td>\n      <td>38397</td>\n      <td>71175</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>183</td>\n      <td>0</td>\n      <td>28</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>16900</td>\n      <td>40056</td>\n      <td>39714</td>\n      <td>33</td>\n      <td>0</td>\n      <td>0</td>\n      <td>440</td>\n      <td>0</td>\n      <td>15</td>\n      <td>48</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>12595</td>\n      <td>64379</td>\n      <td>113153</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>440</td>\n      <td>0</td>\n      <td>65</td>\n      <td>47</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfdfdata = []\n",
    "for cf in cfs_valid:\n",
    "    cfdfdata.append(np.round(np.concatenate((inverse_min_max_scale(cf), np.array(clf.predict(cf.reshape(1, -1)))))))\n",
    "cfdfdata = np.array(cfdfdata).astype(int)\n",
    "cfdf = pd.DataFrame(data=cfdfdata, columns=columns_good)\n",
    "display(cfdf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.01\n",
      "0.02\n",
      "0.03\n",
      "0.0\n",
      "0.01\n",
      "0.02\n",
      "0.03\n",
      "0.04\n",
      "0.05\n",
      "0.06\n",
      "0.07\n",
      "0.08\n",
      "0.09\n"
     ]
    }
   ],
   "source": [
    "for nrcf in cfs_non_robust:\n",
    "    test_deltas = np.arange(0.0, 0.19, 0.01)\n",
    "    for delta in test_deltas:\n",
    "        nodes = build_inn_nodes(clf, util_exp.num_layers)\n",
    "        weights, biases = build_inn_weights_biases(clf, util_exp.num_layers, delta, nodes)\n",
    "        this_inn = Inn(util_exp.num_layers, delta, nodes, weights, biases)\n",
    "        y_prime = 1\n",
    "        this_solver = OptSolver(util_exp.dataset, this_inn, y_prime, x, mode=1, M=10000, x_prime=nrcf)\n",
    "        if this_solver.compute_inn_bounds()[0] == 1:\n",
    "            print(delta)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]WARNING:root: MAD for feature DEROG is 0, so replacing it with 1.0 to avoid error.\n",
      "WARNING:root: MAD for feature DELINQ is 0, so replacing it with 1.0 to avoid error.\n",
      "WARNING:root: MAD for feature NINQ is 0, so replacing it with 1.0 to avoid error.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.49s/it]\n"
     ]
    }
   ],
   "source": [
    "rob = []\n",
    "for i in tqdm(valids):\n",
    "    #if i == 10 or i == 30 or i == 36 or i == 40:\n",
    "    #    continue\n",
    "    x = util_exp.test_instances[i]\n",
    "    with HiddenPrints():\n",
    "        cf = exp.generate_counterfactuals(pd.DataFrame(x.reshape(1, -1), columns=X1.columns), total_CFs=10,\n",
    "                                          desired_class=1, proximity_weight=0.05, diversity_weight=1)\n",
    "    #cf.visualize_as_dataframe(show_only_changes=False)\n",
    "    cfs = cf.cf_examples_list[0].final_cfs_df.values\n",
    "    for item in cfs:\n",
    "        if util_exp.is_robust(x, item):\n",
    "            rob.append(i)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 19, 19]\n"
     ]
    }
   ],
   "source": [
    "print(rob)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}